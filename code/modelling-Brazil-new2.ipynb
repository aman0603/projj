{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0599c0cb",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/GAUTAMMANU/projminor/blob/main/code/modelling-Brazil-new2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7audVqxFBDWh",
   "metadata": {
    "id": "7audVqxFBDWh"
   },
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "AK_rMS2tfBMt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AK_rMS2tfBMt",
    "outputId": "64c0392d-987f-47d4-afc6-41ba7fe59475"
   },
   "outputs": [],
   "source": [
    "# !git clone https://github.com/GAUTAMMANU/projminor.git\n",
    "# %cd projminor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "UdxUs05bahYS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "id": "UdxUs05bahYS",
    "outputId": "e6a47034-b1c8-404f-a48a-ebfbf9592a21",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\amanp\\\\Desktop\\\\MINOR\\\\projj\\\\code'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %cd code\n",
    "# insert your desired path to work on\n",
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "# os.chdir(join('..','data'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8tZCHvverUb_",
   "metadata": {
    "id": "8tZCHvverUb_"
   },
   "source": [
    "Run the following section one time per session. This cell links the code folder to the python exectution path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "hftunTO1rHKZ",
   "metadata": {
    "id": "hftunTO1rHKZ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33R-Be_BrUuK",
   "metadata": {
    "id": "33R-Be_BrUuK"
   },
   "source": [
    "The following cell allows Jupyter Notebooks to detect changes in external code and to automatically update it without restarting the runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "12d77b30-7bf9-4302-9def-c4f4ea4a5963",
    "outputId": "576120a9-dc33-42f0-ca64-cb70cce06751",
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "K2C_kf2otfQF",
   "metadata": {
    "id": "K2C_kf2otfQF"
   },
   "source": [
    "Plots settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "LENlVPDfteud",
   "metadata": {
    "id": "LENlVPDfteud",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family':'Arial', 'size':'15', 'weight':'normal'}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f16e241-e507-4edd-a900-5594f835b4e6",
   "metadata": {
    "id": "3f16e241-e507-4edd-a900-5594f835b4e6"
   },
   "source": [
    "Set folder structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73e8d95b-6ce2-4ce0-a419-4628ceea6e2c",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73e8d95b-6ce2-4ce0-a419-4628ceea6e2c",
    "outputId": "ed20a27e-4bbe-430e-be23-3cd3f52cf7d9",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "project_path\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
   "metadata": {
    "id": "8d3d1eed-54e9-4f76-b8f8-42587caaabaa",
    "tags": []
   },
   "source": [
    "# **AI4Dengue forecasting**\n",
    "![](https://drive.google.com/uc?export=view&id=1J5Bt5Cks-e2IV-dEJLHJkuwXFJNFAZgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
   "metadata": {
    "id": "272e0945-3bd3-4660-87bc-ebc9438535fc",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from config import DEP_NAMES, GROUPED_VARS, DATA_REDUCER_SETTINGS, DATA_PROCESSING_SETTINGS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Mmz-HTHisxH3",
   "metadata": {
    "id": "Mmz-HTHisxH3",
    "tags": []
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d",
   "metadata": {
    "id": "717e2eae-97ff-475f-bfd5-78e8dcee981d"
   },
   "source": [
    "## Load the dataframe\n",
    "**This dataframe comprises all the variables (climatic, epidemiological etc.) acquired for each Department during a defined number of years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "8e5208ca-5c08-4b12-b4bf-37a49f338d25",
    "outputId": "e26c4b4c-2265-4738-fd98-fbd191372f2c",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                2004-02-01\n",
       "Year                      2004\n",
       "Month                        2\n",
       "CD_UF                       12\n",
       "area_km2            164173.431\n",
       "                       ...    \n",
       "rdpc_def_vulner         119.68\n",
       "t_analf_18m              17.79\n",
       "t_formal_18m             46.45\n",
       "t_fundc_ocup18m           55.2\n",
       "t_medioc_ocup18m         39.61\n",
       "Name: 1000, Length: 62, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "dataframe.head()\n",
    "dataframe.iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "KAIRkgshwFwP",
   "metadata": {
    "id": "KAIRkgshwFwP",
    "tags": []
   },
   "source": [
    "**Load CNN results as columns to dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "Thwg_xPWwJfs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 423
    },
    "id": "Thwg_xPWwJfs",
    "outputId": "216640b5-4247-4fd4-b002-d0cb5ea0487d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>53</td>\n",
       "      <td>29.022461</td>\n",
       "      <td>16.795465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>53</td>\n",
       "      <td>20.277210</td>\n",
       "      <td>5.658783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>53</td>\n",
       "      <td>7.219064</td>\n",
       "      <td>16.862005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>53</td>\n",
       "      <td>17.866333</td>\n",
       "      <td>28.619926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>53</td>\n",
       "      <td>13.846931</td>\n",
       "      <td>8.825871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CD_UF    CNN_all   CNN_0-19\n",
       "0        11   1.000000   1.000000\n",
       "1        11   1.000000   1.000000\n",
       "2        11   1.000000   1.000000\n",
       "3        11   1.000000   1.000000\n",
       "4        11  32.159859  17.186546\n",
       "...     ...        ...        ...\n",
       "6151     53  29.022461  16.795465\n",
       "6152     53  20.277210   5.658783\n",
       "6153     53   7.219064  16.862005\n",
       "6154     53  17.866333  28.619926\n",
       "6155     53  13.846931   8.825871\n",
       "\n",
       "[6156 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = pd.read_csv(join('saved_models', \"cnn_dataframe.csv\")).drop('Unnamed: 0', axis=1)\n",
    "cnn['CD_UF'] = cnn['CD_UF'].astype(np.int64)\n",
    "\n",
    "assert dataframe.shape[0] == cnn.shape[0]\n",
    "assert all(dataframe['CD_UF'].unique() == cnn['CD_UF'].unique())\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "p6I4_itCcnn7",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "p6I4_itCcnn7",
    "outputId": "7fd12674-248f-4b87-9919-733a599b21b1",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>pea10a14</th>\n",
       "      <th>pea15a17</th>\n",
       "      <th>pea18m</th>\n",
       "      <th>t_eletrica</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.362744</td>\n",
       "      <td>282.150351</td>\n",
       "      <td>42.202163</td>\n",
       "      <td>304.210083</td>\n",
       "      <td>287.271135</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.317748</td>\n",
       "      <td>281.820936</td>\n",
       "      <td>34.023500</td>\n",
       "      <td>307.566780</td>\n",
       "      <td>290.719267</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>286.196146</td>\n",
       "      <td>45.486547</td>\n",
       "      <td>307.716003</td>\n",
       "      <td>291.720099</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.235493</td>\n",
       "      <td>290.445969</td>\n",
       "      <td>64.916154</td>\n",
       "      <td>306.706715</td>\n",
       "      <td>291.496597</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.314533</td>\n",
       "      <td>291.019027</td>\n",
       "      <td>69.799133</td>\n",
       "      <td>303.716225</td>\n",
       "      <td>291.647991</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0     2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1     2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2     2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3     2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4     2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "...          ...   ...    ...    ...         ...       ...   \n",
       "6151  2019-08-01  2019      8     53    5760.784  0.362744   \n",
       "6152  2019-09-01  2019      9     53    5760.784  0.317748   \n",
       "6153  2019-10-01  2019     10     53    5760.784  0.271795   \n",
       "6154  2019-11-01  2019     11     53    5760.784  0.235493   \n",
       "6155  2019-12-01  2019     12     53    5760.784  0.314533   \n",
       "\n",
       "      dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                    295.674980   88.460308            303.987216   \n",
       "1                    295.944060   88.856948            304.738755   \n",
       "2                    296.092747   89.305463            304.620829   \n",
       "3                    296.186143   88.590375            304.168669   \n",
       "4                    295.562972   86.939606            303.903043   \n",
       "...                         ...         ...                   ...   \n",
       "6151                 282.150351   42.202163            304.210083   \n",
       "6152                 281.820936   34.023500            307.566780   \n",
       "6153                 286.196146   45.486547            307.716003   \n",
       "6154                 290.445969   64.916154            306.706715   \n",
       "6155                 291.019027   69.799133            303.716225   \n",
       "\n",
       "      min_temperature_2m_d  ...  pea10a14  pea15a17   pea18m  t_eletrica  \\\n",
       "0               294.155015  ...     18698     34904   723839       97.26   \n",
       "1               294.332566  ...     18698     34904   723839       97.26   \n",
       "2               294.304126  ...     18698     34904   723839       97.26   \n",
       "3               293.921815  ...     18698     34904   723839       97.26   \n",
       "4               293.395959  ...     18698     34904   723839       97.26   \n",
       "...                    ...  ...       ...       ...      ...         ...   \n",
       "6151            287.271135  ...     10706     36652  1361053       99.91   \n",
       "6152            290.719267  ...     10706     36652  1361053       99.91   \n",
       "6153            291.720099  ...     10706     36652  1361053       99.91   \n",
       "6154            291.496597  ...     10706     36652  1361053       99.91   \n",
       "6155            291.647991  ...     10706     36652  1361053       99.91   \n",
       "\n",
       "      t_densidadem2  rdpc_def_vulner  t_analf_18m  t_formal_18m  \\\n",
       "0             27.15           144.93         9.42         51.72   \n",
       "1             27.15           144.93         9.42         51.72   \n",
       "2             27.15           144.93         9.42         51.72   \n",
       "3             27.15           144.93         9.42         51.72   \n",
       "4             27.15           144.93         9.42         51.72   \n",
       "...             ...              ...          ...           ...   \n",
       "6151          23.48           171.62         3.66         71.62   \n",
       "6152          23.48           171.62         3.66         71.62   \n",
       "6153          23.48           171.62         3.66         71.62   \n",
       "6154          23.48           171.62         3.66         71.62   \n",
       "6155          23.48           171.62         3.66         71.62   \n",
       "\n",
       "      t_fundc_ocup18m  t_medioc_ocup18m  \n",
       "0               53.83             36.93  \n",
       "1               53.83             36.93  \n",
       "2               53.83             36.93  \n",
       "3               53.83             36.93  \n",
       "4               53.83             36.93  \n",
       "...               ...               ...  \n",
       "6151            76.39             61.00  \n",
       "6152            76.39             61.00  \n",
       "6153            76.39             61.00  \n",
       "6154            76.39             61.00  \n",
       "6155            76.39             61.00  \n",
       "\n",
       "[6156 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sort_values(['CD_UF', 'Date'], inplace=True, ignore_index=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "FDTXJ5u_2jLd",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 617
    },
    "id": "FDTXJ5u_2jLd",
    "outputId": "01973fcf-62e0-42a1-f0ec-a556daaec241",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>pea18m</th>\n",
       "      <th>t_eletrica</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.362744</td>\n",
       "      <td>282.150351</td>\n",
       "      <td>42.202163</td>\n",
       "      <td>304.210083</td>\n",
       "      <td>287.271135</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>29.022461</td>\n",
       "      <td>16.795465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.317748</td>\n",
       "      <td>281.820936</td>\n",
       "      <td>34.023500</td>\n",
       "      <td>307.566780</td>\n",
       "      <td>290.719267</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>20.277210</td>\n",
       "      <td>5.658783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>286.196146</td>\n",
       "      <td>45.486547</td>\n",
       "      <td>307.716003</td>\n",
       "      <td>291.720099</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>7.219064</td>\n",
       "      <td>16.862005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.235493</td>\n",
       "      <td>290.445969</td>\n",
       "      <td>64.916154</td>\n",
       "      <td>306.706715</td>\n",
       "      <td>291.496597</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>17.866333</td>\n",
       "      <td>28.619926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.314533</td>\n",
       "      <td>291.019027</td>\n",
       "      <td>69.799133</td>\n",
       "      <td>303.716225</td>\n",
       "      <td>291.647991</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>13.846931</td>\n",
       "      <td>8.825871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0     2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1     2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2     2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3     2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4     2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "...          ...   ...    ...    ...         ...       ...   \n",
       "6151  2019-08-01  2019      8     53    5760.784  0.362744   \n",
       "6152  2019-09-01  2019      9     53    5760.784  0.317748   \n",
       "6153  2019-10-01  2019     10     53    5760.784  0.271795   \n",
       "6154  2019-11-01  2019     11     53    5760.784  0.235493   \n",
       "6155  2019-12-01  2019     12     53    5760.784  0.314533   \n",
       "\n",
       "      dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                    295.674980   88.460308            303.987216   \n",
       "1                    295.944060   88.856948            304.738755   \n",
       "2                    296.092747   89.305463            304.620829   \n",
       "3                    296.186143   88.590375            304.168669   \n",
       "4                    295.562972   86.939606            303.903043   \n",
       "...                         ...         ...                   ...   \n",
       "6151                 282.150351   42.202163            304.210083   \n",
       "6152                 281.820936   34.023500            307.566780   \n",
       "6153                 286.196146   45.486547            307.716003   \n",
       "6154                 290.445969   64.916154            306.706715   \n",
       "6155                 291.019027   69.799133            303.716225   \n",
       "\n",
       "      min_temperature_2m_d  ...   pea18m  t_eletrica  t_densidadem2  \\\n",
       "0               294.155015  ...   723839       97.26          27.15   \n",
       "1               294.332566  ...   723839       97.26          27.15   \n",
       "2               294.304126  ...   723839       97.26          27.15   \n",
       "3               293.921815  ...   723839       97.26          27.15   \n",
       "4               293.395959  ...   723839       97.26          27.15   \n",
       "...                    ...  ...      ...         ...            ...   \n",
       "6151            287.271135  ...  1361053       99.91          23.48   \n",
       "6152            290.719267  ...  1361053       99.91          23.48   \n",
       "6153            291.720099  ...  1361053       99.91          23.48   \n",
       "6154            291.496597  ...  1361053       99.91          23.48   \n",
       "6155            291.647991  ...  1361053       99.91          23.48   \n",
       "\n",
       "      rdpc_def_vulner  t_analf_18m  t_formal_18m  t_fundc_ocup18m  \\\n",
       "0              144.93         9.42         51.72            53.83   \n",
       "1              144.93         9.42         51.72            53.83   \n",
       "2              144.93         9.42         51.72            53.83   \n",
       "3              144.93         9.42         51.72            53.83   \n",
       "4              144.93         9.42         51.72            53.83   \n",
       "...               ...          ...           ...              ...   \n",
       "6151           171.62         3.66         71.62            76.39   \n",
       "6152           171.62         3.66         71.62            76.39   \n",
       "6153           171.62         3.66         71.62            76.39   \n",
       "6154           171.62         3.66         71.62            76.39   \n",
       "6155           171.62         3.66         71.62            76.39   \n",
       "\n",
       "      t_medioc_ocup18m    CNN_all   CNN_0-19  \n",
       "0                36.93   1.000000   1.000000  \n",
       "1                36.93   1.000000   1.000000  \n",
       "2                36.93   1.000000   1.000000  \n",
       "3                36.93   1.000000   1.000000  \n",
       "4                36.93  32.159859  17.186546  \n",
       "...                ...        ...        ...  \n",
       "6151             61.00  29.022461  16.795465  \n",
       "6152             61.00  20.277210   5.658783  \n",
       "6153             61.00   7.219064  16.862005  \n",
       "6154             61.00  17.866333  28.619926  \n",
       "6155             61.00  13.846931   8.825871  \n",
       "\n",
       "[6156 rows x 64 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.concat([dataframe, cnn[['CNN_all', 'CNN_0-19']]], axis=1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886",
   "metadata": {
    "id": "a4233a8a-bd01-4edd-96a9-6f8df9733886"
   },
   "source": [
    "**'Clean' the dataset (e.g. remove NaN values)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "70b042e6-c358-4688-b897-e395e312ddc1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 342
    },
    "id": "70b042e6-c358-4688-b897-e395e312ddc1",
    "outputId": "0ae75fc7-24ec-4c8f-ba32-6d418ebf7b34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "      <th>rate_total</th>\n",
       "      <th>rate_019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.754490</td>\n",
       "      <td>29.124122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.601025</td>\n",
       "      <td>11.718582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.072645</td>\n",
       "      <td>6.376287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.120298</td>\n",
       "      <td>3.791306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "      <td>6.976406</td>\n",
       "      <td>4.652966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0  2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1  2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2  2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3  2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4  2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "\n",
       "   dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                 295.674980   88.460308            303.987216   \n",
       "1                 295.944060   88.856948            304.738755   \n",
       "2                 296.092747   89.305463            304.620829   \n",
       "3                 296.186143   88.590375            304.168669   \n",
       "4                 295.562972   86.939606            303.903043   \n",
       "\n",
       "   min_temperature_2m_d  ...  t_densidadem2  rdpc_def_vulner  t_analf_18m  \\\n",
       "0            294.155015  ...          27.15           144.93         9.42   \n",
       "1            294.332566  ...          27.15           144.93         9.42   \n",
       "2            294.304126  ...          27.15           144.93         9.42   \n",
       "3            293.921815  ...          27.15           144.93         9.42   \n",
       "4            293.395959  ...          27.15           144.93         9.42   \n",
       "\n",
       "   t_formal_18m  t_fundc_ocup18m  t_medioc_ocup18m    CNN_all   CNN_0-19  \\\n",
       "0         51.72            53.83             36.93   1.000000   1.000000   \n",
       "1         51.72            53.83             36.93   1.000000   1.000000   \n",
       "2         51.72            53.83             36.93   1.000000   1.000000   \n",
       "3         51.72            53.83             36.93   1.000000   1.000000   \n",
       "4         51.72            53.83             36.93  32.159859  17.186546   \n",
       "\n",
       "   rate_total   rate_019  \n",
       "0   42.754490  29.124122  \n",
       "1   17.601025  11.718582  \n",
       "2   11.072645   6.376287  \n",
       "3    5.120298   3.791306  \n",
       "4    6.976406   4.652966  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = utils.clean(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "AgWy_v6LJkhM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AgWy_v6LJkhM",
    "outputId": "36f06ba2-5af0-49ef-81a5-fb9054865bb4",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6156 entries, 0 to 6155\n",
      "Data columns (total 61 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Date                       6156 non-null   object \n",
      " 1   Year                       6156 non-null   int64  \n",
      " 2   Month                      6156 non-null   int64  \n",
      " 3   CD_UF                      6156 non-null   int64  \n",
      " 4   area_km2                   6156 non-null   float64\n",
      " 5   NDVI_d                     6156 non-null   float64\n",
      " 6   dewpoint_temperature_2m_d  6156 non-null   float64\n",
      " 7   humidity_d                 6156 non-null   float64\n",
      " 8   max_temperature_2m_d       6156 non-null   float64\n",
      " 9   min_temperature_2m_d       6156 non-null   float64\n",
      " 10  surface_pressure_d         6156 non-null   float64\n",
      " 11  temperature_2m_d           6156 non-null   float64\n",
      " 12  total_precipitation_d      6156 non-null   float64\n",
      " 13  u_component_of_wind_10m_d  6156 non-null   float64\n",
      " 14  v_component_of_wind_10m_d  6156 non-null   float64\n",
      " 15  max_elevation_d            6156 non-null   float64\n",
      " 16  mean_elevation_d           6156 non-null   float64\n",
      " 17  min_elevation_d            6156 non-null   float64\n",
      " 18  stdDev_elevation_d         6156 non-null   float64\n",
      " 19  variance_elevation_d       6156 non-null   float64\n",
      " 20  PopTotal_Urban_UF          6156 non-null   int64  \n",
      " 21  PopTotal_Rural_UF          6156 non-null   int64  \n",
      " 22  cases0_19                  6156 non-null   int64  \n",
      " 23  cases20_99                 6156 non-null   int64  \n",
      " 24  Forest_Cover_Percent       6156 non-null   float64\n",
      " 25  Urban_Cover_Percent        6156 non-null   float64\n",
      " 26  ivs                        6156 non-null   float64\n",
      " 27  ivs_infraestrutura_urbana  6156 non-null   float64\n",
      " 28  ivs_capital_humano         6156 non-null   float64\n",
      " 29  ivs_renda_e_trabalho       6156 non-null   float64\n",
      " 30  t_sem_agua_esgoto          6156 non-null   float64\n",
      " 31  t_sem_lixo                 6156 non-null   float64\n",
      " 32  t_vulner_mais1h            6156 non-null   float64\n",
      " 33  t_analf_15m                6156 non-null   float64\n",
      " 34  t_cdom_fundin              6156 non-null   float64\n",
      " 35  t_p15a24_nada              6156 non-null   float64\n",
      " 36  t_vulner                   6156 non-null   float64\n",
      " 37  t_desocup18m               6156 non-null   float64\n",
      " 38  t_p18m_fundin_informal     6156 non-null   float64\n",
      " 39  idhm                       6156 non-null   float64\n",
      " 40  idhm_long                  6156 non-null   float64\n",
      " 41  idhm_educ                  6156 non-null   float64\n",
      " 42  idhm_renda                 6156 non-null   float64\n",
      " 43  idhm_educ_sub_esc          6156 non-null   float64\n",
      " 44  t_pop18m_fundc             6156 non-null   float64\n",
      " 45  idhm_educ_sub_freq         6156 non-null   float64\n",
      " 46  renda_per_capita           6156 non-null   float64\n",
      " 47  pea10a14                   6156 non-null   int64  \n",
      " 48  pea15a17                   6156 non-null   int64  \n",
      " 49  pea18m                     6156 non-null   int64  \n",
      " 50  t_eletrica                 6156 non-null   float64\n",
      " 51  t_densidadem2              6156 non-null   float64\n",
      " 52  rdpc_def_vulner            6156 non-null   float64\n",
      " 53  t_analf_18m                6156 non-null   float64\n",
      " 54  t_formal_18m               6156 non-null   float64\n",
      " 55  t_fundc_ocup18m            6156 non-null   float64\n",
      " 56  t_medioc_ocup18m           6156 non-null   float64\n",
      " 57  CNN_all                    6156 non-null   float64\n",
      " 58  CNN_0-19                   6156 non-null   float64\n",
      " 59  rate_total                 6156 non-null   float64\n",
      " 60  rate_019                   6156 non-null   float64\n",
      "dtypes: float64(50), int64(10), object(1)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "eba94bc2-fc95-47c7-830c-d211584d9b97",
   "metadata": {
    "id": "eba94bc2-fc95-47c7-830c-d211584d9b97"
   },
   "outputs": [],
   "source": [
    "dataframe.to_csv('out.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca",
   "metadata": {
    "id": "ea3ca977-edc7-4d92-9ddb-47c7bd6fbaca"
   },
   "source": [
    "## Apply Data Reduction\n",
    "**Data reduction is applied to three macro groups in order to reduce the number of variables on which the AI framework will be trained. The variables belonging to each group are set with the *PCAgroups* dictionary. The groups are:**\n",
    "1. ***CLIMATIC VARIABLES***,\n",
    "2. ***GEO VARIABLES***,\n",
    "3. ***SOCIO VARIABLES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e8606a60-7d0f-4b60-9050-6a456be00f58",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e8606a60-7d0f-4b60-9050-6a456be00f58",
    "outputId": "ed9785c4-8981-4c35-9d22-49c3c04eb4ef",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m PCA Excluded Variables \u001b[0m\n",
      "----- 1 t_fundc_ocup18m\n",
      "----- 2 t_medioc_ocup18m\n",
      "----- 3 PopTotal_Urban_UF\n",
      "----- 4 PopTotal_Rural_UF\n",
      "----- 5 total_precipitation_d\n",
      "----- 6 surface_pressure_d\n",
      "----- 7 area_km2\n",
      "----- 8 humidity_d\n",
      "----- 9 temperature_2m_d\n",
      "----- 10 min_temperature_2m_d\n",
      "----- 11 CNN_all\n",
      "----- 12 CNN_0-19\n",
      "\u001b[1m Climatic variables \u001b[0m\n",
      "----- 1 dewpoint_temperature_2m_d\n",
      "----- 2 max_temperature_2m_d\n",
      "----- 3 u_component_of_wind_10m_d\n",
      "----- 4 v_component_of_wind_10m_d\n",
      "\u001b[1m Geo variables \u001b[0m\n",
      "----- 1 NDVI_d\n",
      "----- 2 max_elevation_d\n",
      "----- 3 mean_elevation_d\n",
      "----- 4 min_elevation_d\n",
      "----- 5 stdDev_elevation_d\n",
      "----- 6 variance_elevation_d\n",
      "----- 7 Forest_Cover_Percent\n",
      "----- 8 Urban_Cover_Percent\n",
      "\u001b[1m Socio variables \u001b[0m\n",
      "----- 1 Urban_Cover_Percent\n",
      "----- 2 ivs\n",
      "----- 3 ivs_infraestrutura_urbana\n",
      "----- 4 ivs_capital_humano\n",
      "----- 5 ivs_renda_e_trabalho\n",
      "----- 6 t_sem_agua_esgoto\n",
      "----- 7 t_sem_lixo\n",
      "----- 8 t_vulner_mais1h\n",
      "----- 9 t_analf_15m\n",
      "----- 10 t_cdom_fundin\n",
      "----- 11 t_p15a24_nada\n",
      "----- 12 t_vulner\n",
      "----- 13 t_desocup18m\n",
      "----- 14 t_p18m_fundin_informal\n",
      "----- 15 idhm\n",
      "----- 16 idhm_long\n",
      "----- 17 idhm_educ\n",
      "----- 18 idhm_renda\n",
      "----- 19 idhm_educ_sub_esc\n",
      "----- 20 t_pop18m_fundc\n",
      "----- 21 idhm_educ_sub_freq\n",
      "----- 22 renda_per_capita\n",
      "----- 23 pea10a14\n",
      "----- 24 pea15a17\n",
      "----- 25 pea18m\n",
      "----- 26 t_eletrica\n",
      "----- 27 t_densidadem2\n",
      "----- 28 rdpc_def_vulner\n",
      "----- 29 t_analf_18m\n",
      "----- 30 t_formal_18m\n",
      "\u001b[1m Additional variables \u001b[0m\n",
      "----- 1 Month\n",
      "----- 2 cases20_99\n",
      "----- 3 cases0_19\n",
      "\u001b[1m Dengue variables \u001b[0m\n",
      "----- 1 rate_total\n",
      "----- 2 rate_019\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m PCA Excluded Variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['EXCLUDED'])\n",
    "\n",
    "print('\\033[1m Climatic variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['CLIMATIC VARIABLES'])\n",
    "\n",
    "print('\\033[1m Geo variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['GEO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Socio variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['SOCIO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Additional variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['AUXILIAR'])\n",
    "\n",
    "print('\\033[1m Dengue variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['DENGUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c",
   "metadata": {
    "id": "b7733c78-84dd-4f71-8b6d-1788c4ea218c"
   },
   "source": [
    "**We selected two types of data reduction methods: PCA (Principal Component Analysis) and PLS (Principal Least Square). The second one is the default solution because it reduces the input data by considering also a second variable that in our case is the Dengue Incidence Rates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00",
   "metadata": {
    "id": "ed07eec8-4360-4a04-8a1a-9f47e8aadf00"
   },
   "outputs": [],
   "source": [
    "from data_reduction import pca_reducer, pls_reducer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493bea11-18e6-49ff-b3db-d749f0812ab9",
   "metadata": {
    "id": "493bea11-18e6-49ff-b3db-d749f0812ab9"
   },
   "source": [
    "**Extract climatic, geophysical and socio-economic variables from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd",
   "metadata": {
    "id": "5b5eef13-eb3e-407c-a44a-7c0d98b476fd"
   },
   "outputs": [],
   "source": [
    "X_climatic = dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values\n",
    "X_geo = dataframe[GROUPED_VARS['GEO VARIABLES']].values\n",
    "X_socio = dataframe[GROUPED_VARS['SOCIO VARIABLES']].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eAhKA1MLspS8",
   "metadata": {
    "id": "eAhKA1MLspS8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5",
   "metadata": {
    "id": "16023ac4-fc4a-42f5-aaf1-ef6612e443e5"
   },
   "source": [
    "**Extract Dengue variables from the dataframe, apply a root scaling and normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "816a51e4-c585-4fc6-9620-3227c699164e",
   "metadata": {
    "id": "816a51e4-c585-4fc6-9620-3227c699164e"
   },
   "outputs": [],
   "source": [
    "y_dengue = dataframe[GROUPED_VARS['DENGUE']].values\n",
    "scaler = MinMaxScaler()\n",
    "y_dengue = scaler.fit_transform(y_dengue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b0401b9-59af-4f29-868b-c37052808de9",
   "metadata": {
    "id": "3b0401b9-59af-4f29-868b-c37052808de9"
   },
   "source": [
    "**Apply data reduction technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ce6b0059-cf6d-469d-ade5-85c86bb5ad36",
    "outputId": "4290e542-87b2-4b29-80f5-743df9405399"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6156, 10)\n"
     ]
    }
   ],
   "source": [
    "if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':\n",
    "    climatic_vars_reduced = pls_reducer(\n",
    "        X_climatic,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "\n",
    "    geo_vars_reduced = pls_reducer(\n",
    "        X_geo,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "\n",
    "    socio_vars_reduced = pls_reducer(\n",
    "        X_socio,\n",
    "        y_dengue,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "    print(socio_vars_reduced.shape    )\n",
    "elif DATA_REDUCER_SETTINGS['TYPE'] == 'PCA':\n",
    "    climatic_vars_reduced = pca_reducer(\n",
    "        X_climatic,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "\n",
    "    geo_vars_reduced = pca_reducer(\n",
    "        X_geo,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "\n",
    "    socio_vars_reduced = pca_reducer(\n",
    "        X_socio,\n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "else:\n",
    "    print('No data reduction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ae33ce-0228-4303-a796-ce8a7c50869a",
   "metadata": {
    "id": "52ae33ce-0228-4303-a796-ce8a7c50869a"
   },
   "source": [
    "## Order reduced data in a new dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979c063e-4ae5-4371-98ac-def71760765b",
   "metadata": {
    "id": "979c063e-4ae5-4371-98ac-def71760765b"
   },
   "source": [
    "**Normalize remaining variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "zYOskxhcmUDF",
   "metadata": {
    "id": "zYOskxhcmUDF"
   },
   "outputs": [],
   "source": [
    "x_excluded = dataframe[GROUPED_VARS['EXCLUDED']].values\n",
    "x_excluded = MinMaxScaler().fit_transform(x_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63",
   "metadata": {
    "id": "b543c83f-80b6-4d4a-9ff6-7188a7ce7b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.00000000e+00 2.57485926e-03 2.91741472e-03]\n",
      " [9.09090909e-02 1.06812799e-03 1.17387101e-03]\n",
      " [1.81818182e-01 7.01765249e-04 6.38723933e-04]\n",
      " ...\n",
      " [8.18181818e-01 1.58929189e-03 3.95318326e-03]\n",
      " [9.09090909e-01 1.60477200e-03 3.48708742e-03]\n",
      " [1.00000000e+00 2.13625598e-03 2.93467753e-03]]\n"
     ]
    }
   ],
   "source": [
    "X_auxiliar = dataframe[GROUPED_VARS['AUXILIAR']].values\n",
    "X_auxiliar = MinMaxScaler().fit_transform(X_auxiliar)\n",
    "print(X_auxiliar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c3da58-db5b-456f-a119-0830ad336ced",
   "metadata": {
    "id": "39c3da58-db5b-456f-a119-0830ad336ced"
   },
   "source": [
    "**Create a new database with the reduced, the auxiliar and Dengue variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1df37d0b-af3d-4ad6-b17d-5f9f6317f6ad",
    "outputId": "00c35692-749b-49af-fa4c-0aadf0b215b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'dep_id', 't_fundc_ocup18m', 't_medioc_ocup18m',\n",
       "       'PopTotal_Urban_UF', 'PopTotal_Rural_UF', 'total_precipitation_d',\n",
       "       'surface_pressure_d', 'area_km2', 'humidity_d', 'temperature_2m_d',\n",
       "       'min_temperature_2m_d', 'CNN_all', 'CNN_0-19', 'Month', 'cases20_99',\n",
       "       'cases0_19', 'RandEffects1', 'RandEffects2', 'RandEffects3',\n",
       "       'PCA0-Climatic', 'PCA1-Climatic', 'PCA2-Climatic', 'PCA3-Climatic',\n",
       "       'PCA0-Geo', 'PCA1-Geo', 'PCA2-Geo', 'PCA3-Geo', 'PCA4-Geo', 'PCA5-Geo',\n",
       "       'PCA0-Socio', 'PCA1-Socio', 'PCA2-Socio', 'PCA3-Socio', 'PCA4-Socio',\n",
       "       'PCA5-Socio', 'DengRate_all', 'DengRate_019'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent = {'Year':dataframe.Year.values, 'dep_id':dataframe.CD_UF.values, 't_fundc_ocup18m':x_excluded[:, 0], 't_medioc_ocup18m':x_excluded[:, 1],\n",
    "               'PopTotal_Urban_UF':x_excluded[:, 2], 'PopTotal_Rural_UF':x_excluded[:, 3], 'total_precipitation_d':x_excluded[:, 4],\n",
    "               'surface_pressure_d':x_excluded[:, 5], 'area_km2':x_excluded[:, 6], 'humidity_d':x_excluded[:, 7], 'temperature_2m_d':x_excluded[:, 8],\n",
    "               'min_temperature_2m_d':x_excluded[:, 9], 'CNN_all':x_excluded[:, 10], 'CNN_0-19':x_excluded[:, 11]}\n",
    "\n",
    "auxiliar    = {'Month': X_auxiliar[:, 0],\n",
    "               'cases20_99': X_auxiliar[:, 1], 'cases0_19': X_auxiliar[:, 2],\n",
    "               'RandEffects1':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],\n",
    "               'RandEffects2':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],\n",
    "               'RandEffects3':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0]}\n",
    "\n",
    "climatic    = {'PCA0-Climatic':climatic_vars_reduced[:,0], 'PCA1-Climatic':climatic_vars_reduced[:,1], 'PCA2-Climatic':climatic_vars_reduced[:,2],\n",
    "               'PCA3-Climatic':climatic_vars_reduced[:,3]}\n",
    "\n",
    "geo         = {'PCA0-Geo':geo_vars_reduced[:,0], 'PCA1-Geo':geo_vars_reduced[:,1], 'PCA2-Geo':geo_vars_reduced[:,2],\n",
    "               'PCA3-Geo':geo_vars_reduced[:,3], 'PCA4-Geo':geo_vars_reduced[:,4], 'PCA5-Geo':geo_vars_reduced[:,5]}\n",
    "\n",
    "socio       = {'PCA0-Socio':socio_vars_reduced[:,0], 'PCA1-Socio':socio_vars_reduced[:,1], 'PCA2-Socio':socio_vars_reduced[:,2],\n",
    "               'PCA3-Socio':socio_vars_reduced[:,3], 'PCA4-Socio':socio_vars_reduced[:,4], 'PCA5-Socio':socio_vars_reduced[:,5]}\n",
    "\n",
    "dengue      = {'DengRate_all': y_dengue[:,0], 'DengRate_019': y_dengue[:,1]}\n",
    "\n",
    "columns     = {**independent, **auxiliar, **climatic, **geo, **socio, **dengue}\n",
    "\n",
    "reduced_dataframe = pd.DataFrame(columns)\n",
    "reduced_dataframe.head()\n",
    "reduced_dataframe.columns\n",
    "\n",
    "# reduced_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
   "metadata": {
    "id": "462125c4-fbb2-4fe7-8cfd-3e023fd5356c"
   },
   "source": [
    "## Create training and validation data\n",
    "**First of all, the dataframe is divided in two sub-dataframes (training and validation) by using the variable *Year***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdc0fd5-42b6-4649-bca4-5256c879e3b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fb37c08d-4df8-4843-8a40-54877433d299",
   "metadata": {
    "id": "fb37c08d-4df8-4843-8a40-54877433d299"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1296, 38)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe   = reduced_dataframe[reduced_dataframe.Year <= 2016]\n",
    "validation_dataframe = reduced_dataframe[reduced_dataframe.Year >= 2016]\n",
    "training_dataframe.shape\n",
    "validation_dataframe.shape\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558",
   "metadata": {
    "id": "c096a133-8ecc-479c-a82b-7c4b5ebf0558"
   },
   "source": [
    "**Then the dataset handler is initialized. This object will handle all the operations needed to create, reshape and augment the training and validation dataset to fit the requirements of each Deep Learning or Machine Learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd",
   "metadata": {
    "id": "180107d5-4541-4cdc-a6df-6ce7e272bddd"
   },
   "outputs": [],
   "source": [
    "from datasetHandler import datasetHandler\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cdb279-8d02-4211-bc0b-b1c48237192a",
   "metadata": {
    "id": "f9cdb279-8d02-4211-bc0b-b1c48237192a"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8605b584-8866-4792-acdb-17dfcd613d0d",
   "metadata": {
    "id": "8605b584-8866-4792-acdb-17dfcd613d0d"
   },
   "source": [
    "**Get training and validation vectors from dataframes.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f79b39bd-1397-4376-bd84-06fa3ffadd77",
    "outputId": "1690fec7-a234-4059-e934-55c03f1d4277"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training shape (4860, 12, 38)\n",
      "Y Training shape (4860, 12, 38)\n",
      "Processing departments 26 of 27\t\t\n",
      "X Validation shape (972, 12, 38)\n",
      "Y Validation shape (972, 2)\n",
      "Processing departments 26 of 27\t\t\n",
      "\n",
      "X Training shape (4860, 12, 38)\n",
      "Y Training shape (4860, 2)\n",
      "X Validation shape (972, 12, 38)\n",
      "Y Validation shape (972, 2)\n"
     ]
    }
   ],
   "source": [
    "# x_train, y_train, x_val, y_val = dataset_handler.get_data(DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION'])\n",
    "\n",
    "x_train, y_train, x_val, y_val, train_indices, val_indices = dataset_handler.get_data(\n",
    "    DATA_PROCESSING_SETTINGS['T LEARNING'], DATA_PROCESSING_SETTINGS['T PREDICTION']\n",
    ")\n",
    "\n",
    "print('\\n\\nX Training shape', x_train.shape)\n",
    "print('Y Training shape', y_train.shape)\n",
    "print('X Validation shape', x_val.shape)\n",
    "print('Y Validation shape', y_val.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164e07a3-8236-491c-9c10-6c270cc1d83e",
   "metadata": {
    "id": "164e07a3-8236-491c-9c10-6c270cc1d83e"
   },
   "source": [
    "**Apply data augmention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2936103f-fe82-45d0-9ffa-e404d61b96a6",
    "outputId": "f9be1e1a-fd8f-4d7b-d0cc-967b73d356b5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training shape (14580, 12, 38)\n",
      "Y Training shape (14580, 2)\n",
      "X Validation shape (2916, 12, 38)\n",
      "Y Validation shape (2916, 2)\n"
     ]
    }
   ],
   "source": [
    "x_train_a, y_train_a, x_val_a, y_val_a = dataset_handler.augment(x_train, y_train, x_val, y_val, DATA_PROCESSING_SETTINGS['AUGMENTATION'])\n",
    "print('X Training shape', x_train_a.shape)\n",
    "print('Y Training shape', y_train_a.shape)\n",
    "print('X Validation shape', x_val_a.shape)\n",
    "print('Y Validation shape', y_val_a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd86e91c-b575-420f-ad54-21cd6c5dbd61",
   "metadata": {
    "id": "bd86e91c-b575-420f-ad54-21cd6c5dbd61"
   },
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ed9d2282-179e-4d2a-9263-6abc048eb563",
   "metadata": {
    "id": "ed9d2282-179e-4d2a-9263-6abc048eb563"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "60f4f502-72c1-4a2f-bf65-1fa1601c4703",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models, regularizers, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "\n",
    "LSTM_SETTINGS = {\n",
    "    'EPOCHS': 200,\n",
    "    'LEARNING RATE': 0.0001,\n",
    "    'BATCH SIZE': 16,\n",
    "    'OPTIMZER': 'rmsprop', #'adam',\n",
    "    'LOSS':'mae',\n",
    "    'EVALUATION METRIC':['mse'],\n",
    "    'EARLY STOPPING': 12\n",
    "}\n",
    "\n",
    "def build_tcn_model_v2(input_shape, output_units):\n",
    "    def residual_block(x, filters, dilation_rate):\n",
    "        shortcut = x  # Residual connection\n",
    "        x = layers.Conv1D(filters, kernel_size=3, padding='causal', \n",
    "                          activation='relu', dilation_rate=dilation_rate,\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        x = layers.Conv1D(filters, kernel_size=3, padding='causal', \n",
    "                          activation='relu', dilation_rate=dilation_rate,\n",
    "                          kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(0.3)(x)\n",
    "        \n",
    "        # Add residual connection\n",
    "        x = layers.add([x, shortcut])\n",
    "        x = layers.ReLU()(x)\n",
    "        return x\n",
    "\n",
    "    inputs = Input(shape=input_shape)\n",
    "    x = layers.Conv1D(filters=64, kernel_size=3, padding='causal', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Add residual blocks with increasing dilation rates\n",
    "    for dilation_rate in [1, 2, 4, 8]:\n",
    "        x = residual_block(x, filters=64, dilation_rate=dilation_rate)\n",
    "    \n",
    "    x = layers.Flatten()(x)\n",
    "    outputs = layers.Dense(units=output_units, activation='linear')(x)  # For regression\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='Huber', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "class ImprovedTCNNet:\n",
    "    def __init__(self, shape, output_units=2):\n",
    "        self.shape = shape\n",
    "        self.epochs = LSTM_SETTINGS['EPOCHS']  # Max epochs\n",
    "        self.batch_size = LSTM_SETTINGS['BATCH SIZE']\n",
    "        self.lr = LSTM_SETTINGS['LEARNING RATE']\n",
    "        self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']\n",
    "        \n",
    "        self.model = build_tcn_model_v2(self.shape, output_units)\n",
    "    \n",
    "    def load(self, model_path):\n",
    "            \"\"\"Load a saved model from the specified path.\"\"\"\n",
    "            self.model = tf.keras.models.load_model(model_path)\n",
    "            print(f\"Model loaded successfully from {model_path}\")\n",
    "        \n",
    "    def train(self, training, validation, output_path):\n",
    "        # Early stopping and learning rate scheduler\n",
    "        es = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=self.early_stopping_rounds,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Train the model\n",
    "        history = self.model.fit(\n",
    "            x=training[0],\n",
    "            y=training[1],\n",
    "            validation_data=(validation[0], validation[1]),\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[es, lr_scheduler],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Plot the training history\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Improved Model Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the model\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        self.model.save(os.path.join(output_path, 'TCN-new-' + today + '.keras'))\n",
    "        return history\n",
    "\n",
    "# Use the improved model\n",
    "trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)\n",
    "tcn = ImprovedTCNNet(trainingT[0].shape[1:])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "3af3340f-9ac7-400c-997a-7bd1d7b8ac2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, average_precision_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score, roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "13bf7f3c-a48e-4f46-bf86-a950f6ca126f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for saved models...\n",
      "Model loaded successfully from C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\saved_models\\Brazil\\TCN-new-18-11-2024-23-46-08.keras\n",
      "Loading model from: C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\saved_models\\Brazil\\TCN-new-18-11-2024-23-46-08.keras\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step\n",
      "Results saved to C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\metrics\\Brazil\\TCN_new_model_26-11-2024-08-29-01.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "# Define paths\n",
    "output_path = os.path.join(config['output'], \"Brazil\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define training flag\n",
    "TRAINING = False  # Set to False to skip training and load the model if it exists\n",
    "\n",
    "def calculate_nrmse(true_values, predicted_values):\n",
    "    return root_mean_squared_error(true_values, predicted_values) / (true_values.max() - true_values.min())\n",
    "\n",
    "def calculate_mae(true_values, predicted_values):\n",
    "    return mean_absolute_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_mse(true_values, predicted_values):\n",
    "    return mean_squared_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_rmse(mse):\n",
    "    return mse ** 0.5\n",
    "\n",
    "def calculate_mape(true_values, predicted_values):\n",
    "    return mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_r2(true_values, predicted_values):\n",
    "    return r2_score(true_values, predicted_values)\n",
    "\n",
    "# Discretize continuous values into binary for confusion matrix\n",
    "def discretize_to_binary(values, threshold=0.5):\n",
    "    return np.where(values > threshold, 1, 0)\n",
    "\n",
    "# Function to plot confusion matrix\n",
    "def plot_confusion_matrix(true_values, predicted_values, department_name, metric_type=\"All\"):\n",
    "    cm = confusion_matrix(true_values, predicted_values)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'Confusion Matrix: {department_name} - {metric_type}')\n",
    "    plt.show()\n",
    "\n",
    "# Check if training is required\n",
    "if TRAINING:\n",
    "    print(\"Training the model...\")\n",
    "\n",
    "    # history = tcn.train(trainingT, validationT, output_path=join(config['output'], \"Brazil\"))\n",
    "    # Train the model with augmented data\n",
    "    history = tcn.train(\n",
    "        training=trainingT,\n",
    "        validation=validationT,\n",
    "        output_path=join(config['output'], \"Brazil\")\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "    # Check if a saved model exists\n",
    "    print(\"Checking for saved models...\")\n",
    "    tcn_models = glob(os.path.join(output_path, \"TCN-new-18*.keras\"))\n",
    "    if not tcn_models:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "        exit()\n",
    "    else:\n",
    "        tcn.load(tcn_models[-1])\n",
    "        print(f\"Loading model from: {tcn_models[-1]}\")\n",
    "    \n",
    "    trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "    \n",
    "    # # Make predictions\n",
    "    # y_val_indices=validation_dataframe.index.to_list()\n",
    "    # y_val_indices_df = pd.DataFrame(y_val_indices,columns = ['actual_index'])\n",
    "\n",
    "    # y_train_indices=training_dataframe.index.to_list()\n",
    "    # y_train_indices_df = pd.DataFrame(y_train_indices,columns = ['actual_index'])\n",
    "\n",
    "    # Create index mappings\n",
    "    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "\n",
    "\n",
    "    # Make predictions on training and validation sets\n",
    "    preds_tra = tcn.model.predict(trainT[0])\n",
    "    preds_tra[preds_tra < 0] = 0\n",
    "    preds_val = tcn.model.predict(valT[0])\n",
    "    preds_val[preds_val < 0] = 0\n",
    "\n",
    "    results = []\n",
    "    preds_val_original = scaler.inverse_transform(preds_val)\n",
    "    y_val_original = scaler.inverse_transform(valT[1])\n",
    "    \n",
    "    # Inverse transform predictions and ground truth values for training data\n",
    "    preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "    y_train_original = scaler.inverse_transform(trainT[1])\n",
    "    \n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        # Filter rows corresponding to the current department for both training and validation\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "    \n",
    "        if department_rows_val.empty or department_rows_train.empty:\n",
    "            continue\n",
    "    \n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "    \n",
    "        # Matching indices for validation data\n",
    "        # matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "        # Matching indices for validation data\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # Matching indices for training data\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "    \n",
    "        if matching_indices_val.empty or matching_indices_train.empty:\n",
    "            continue\n",
    "    \n",
    "        # Extract original scale true and predicted values for validation data\n",
    "        true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "        true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "    \n",
    "        # Extract original scale true and predicted values for training data\n",
    "        true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "        true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "    \n",
    "          # Calculate metrics for DengRate_019 (Validation and Training)\n",
    "        # Calculate metrics for DengRate_all (Validation and Training)\n",
    "        mse_dengrate_all_val = mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mse_dengrate_all_train = mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "        rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "        \n",
    "        mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        # Calculate metrics for DengRate_019 (Validation and Training)\n",
    "        mse_dengrate_019_val = mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mse_dengrate_019_train = mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "        rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "        \n",
    "        mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Discretize predictions for confusion matrix\n",
    "        true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "        predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "\n",
    "        true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "        predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "            'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "            'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "            'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "            'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "            'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "            'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "            'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "            'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "            'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "            'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "            'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "            'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "            'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "            'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "            'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "            'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "            'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "        })\n",
    "\n",
    "\n",
    "\n",
    "    # print(results)\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    #Save results\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_path = os.path.join(config['metrics'], \"Brazil\", f'TCN_new_model_{today}.csv')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63580720-d275-4301-830a-48f09a6a02cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5fab282-9e7d-415b-a93b-d0c2a770c51f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "06ccb288-0dbe-4783-9eb7-304066ba1a8c",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "bd46b4c5-4580-4d61-a72d-bef9b79c8346",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, BatchNormalization, Input, Flatten\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n",
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "2b566bb2-a8e3-49b7-b6a8-d463fe4477df",
   "metadata": {
    "id": "2b566bb2-a8e3-49b7-b6a8-d463fe4477df"
   },
   "outputs": [],
   "source": [
    "trainingL, validationL = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "f650bad7-fdcd-4598-b245-c4ca333df2a0",
   "metadata": {
    "id": "f650bad7-fdcd-4598-b245-c4ca333df2a0"
   },
   "outputs": [],
   "source": [
    "LSTM_SETTINGS = {\n",
    "    'EPOCHS': 200,\n",
    "    'LEARNING RATE': 0.0001,\n",
    "    'BATCH SIZE': 16,\n",
    "    'OPTIMZER': 'rmsprop', #'adam',\n",
    "    'LOSS':'mae',\n",
    "    'EVALUATION METRIC':['mse'],\n",
    "    'EARLY STOPPING': 12\n",
    "}\n",
    "\n",
    "def custom_load_model(path):\n",
    "    return load_model(\n",
    "        path,\n",
    "        custom_objects={\n",
    "            'mae': MeanAbsoluteError(),\n",
    "            'mse': MeanSquaredError(),\n",
    "        },\n",
    "        compile=True\n",
    "    )\n",
    "    \n",
    "class LSTMNet:\n",
    "    def __init__(self, shape):\n",
    "        self.shape = shape\n",
    "        self.epochs = LSTM_SETTINGS['EPOCHS']\n",
    "        self.lr = LSTM_SETTINGS['LEARNING RATE']\n",
    "        self.batch_size = LSTM_SETTINGS['BATCH SIZE']\n",
    "        self.loss = LSTM_SETTINGS['LOSS']\n",
    "        self.eval_metric = LSTM_SETTINGS['EVALUATION METRIC']\n",
    "        self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']\n",
    "        \n",
    "        # Use .get() to avoid KeyError if 'OPTIMIZER' is missing\n",
    "        self.optimizer = LSTM_SETTINGS.get('OPTIMIZER', 'adam')\n",
    "        if self.optimizer == 'adam':\n",
    "            self.optimizer = Adam(learning_rate=self.lr)\n",
    "        elif self.optimizer == 'rmsprop':\n",
    "            self.optimizer = RMSprop(learning_rate=self.lr)\n",
    "        else:\n",
    "            self.optimizer = Adam(learning_rate=self.lr)\n",
    "\n",
    "        self.model = self.__build()\n",
    "\n",
    "    def __build(self):\n",
    "        model = Sequential()\n",
    "        model.add(Input(shape=self.shape))\n",
    "        model.add(LSTM(60, return_sequences=True, dropout=0.5))\n",
    "        model.add(LSTM(20, dropout=0.5))\n",
    "        model.add(Dense(2))\n",
    "        model.compile(loss=self.loss, metrics=self.eval_metric, optimizer=self.optimizer)\n",
    "        return model\n",
    "\n",
    "    # def load(self, path):\n",
    "    #     self.model = load_model(path)\n",
    "    def load(self, path):\n",
    "        self.model = custom_load_model(path)\n",
    "\n",
    "    def train(self, training, validation, output_path):\n",
    "        es = EarlyStopping(\n",
    "            monitor='val_loss', \n",
    "            min_delta=0, \n",
    "            patience=self.early_stopping_rounds, \n",
    "            verbose=0, \n",
    "            mode='auto', \n",
    "            baseline=None, \n",
    "            restore_best_weights=True\n",
    "        )\n",
    "\n",
    "        history = self.model.fit(\n",
    "            x=training[0],\n",
    "            y=training[1],\n",
    "            validation_data=(validation[0], validation[1]),\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[es],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "            # Plot the training history\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Model Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        self.model.save(os.path.join(output_path, 'LSTM-' + today + '.h5'))\n",
    "        return history\n",
    "\n",
    "lstm = LSTMNet(trainingL[0].shape[1:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7f04d0f6-0f47-4b78-abb4-c5ce27c311f9",
   "metadata": {
    "id": "7f04d0f6-0f47-4b78-abb4-c5ce27c311f9"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\saved_models\\Brazil\\LSTM-16-11-2024-19-44-07.h5\n",
      "\u001b[1m152/152\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 8ms/step\n",
      "\u001b[1m31/31\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step\n",
      "Results saved to C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\metrics\\Brazil\\LSTM_Model_26-11-2024-08-30-12.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    lstm.train(trainingL, validationL, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent lstm model\n",
    "    lstm_model = glob(join(config['output'], \"Brazil\", \"LSTM-16*\"))\n",
    "    if not lstm_model:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "        exit()\n",
    "    else:\n",
    "        lstm.load(lstm_model[-1])\n",
    "        print(f\"Loading model from: {lstm_model[-1]}\")\n",
    "\n",
    "    trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "    \n",
    "    preds_tra = lstm.model.predict(trainL[0])\n",
    "    preds_tra[preds_tra < 0] = 0\n",
    "    preds_val = lstm.model.predict(valL[0])\n",
    "    preds_val[preds_val < 0] = 0\n",
    "    # print(\"preds_tra: \\n\",preds_tra.shape,\"\\n\")\n",
    "    # print(\"preds_val: \\n\",preds_val.shape,\"\\n\")\n",
    "    \n",
    "    #     # Align predictions with department indices\n",
    "    # y_val_indices = validation_dataframe.index.to_list()\n",
    "    # y_val_indices_df = pd.DataFrame(y_val_indices, columns=['actual_index'])\n",
    "    \n",
    "    # y_train_indices = training_dataframe.index.to_list()\n",
    "    # y_train_indices_df = pd.DataFrame(y_train_indices, columns=['actual_index'])\n",
    "\n",
    "    # Create index mappings\n",
    "    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "\n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    preds_val_original = scaler.inverse_transform(preds_val)\n",
    "    y_val_original = scaler.inverse_transform(valL[1])\n",
    "    \n",
    "    # Inverse transform predictions and ground truth values for training data\n",
    "    preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "    y_train_original = scaler.inverse_transform(trainL[1])\n",
    "    \n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        # Filter rows corresponding to the current department for both training and validation\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "    \n",
    "        if department_rows_val.empty or department_rows_train.empty:\n",
    "            continue\n",
    "    \n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "    \n",
    "        # # Matching indices for validation data\n",
    "        # matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "        # Matching indices for validation data\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # Matching indices for training data\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "    \n",
    "        if matching_indices_val.empty or matching_indices_train.empty:\n",
    "            continue\n",
    "    \n",
    "        # Extract original scale true and predicted values for validation data\n",
    "        true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "        true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "    \n",
    "        # Extract original scale true and predicted values for training data\n",
    "        true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "        true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "    \n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)\n",
    "        # Calculate metrics for DengRate_all (Validation and Training)\n",
    "        mse_dengrate_all_val = mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mse_dengrate_all_train = mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "        rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "        \n",
    "        mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        # Calculate metrics for DengRate_019 (Validation and Training)\n",
    "        mse_dengrate_019_val = mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mse_dengrate_019_train = mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "        rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "        \n",
    "        mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Discretize predictions for confusion matrix\n",
    "        true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "        predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "\n",
    "        true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "        predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "            'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "            'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "            'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "            'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "            'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "            'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "            'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "            'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "            'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "            'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "            'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "            'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "            'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "            'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "            'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "            'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "            'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "        })\n",
    "\n",
    "    # print(results)\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    #Save results\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_path = os.path.join(config['metrics'], \"Brazil\", f'LSTM_Model_{today}.csv')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610b340b-326a-4d0c-9a03-f6c75ccbe41e",
   "metadata": {
    "id": "610b340b-326a-4d0c-9a03-f6c75ccbe41e"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30eae8c8-8b03-4153-ae2a-9f3b63a00a7c",
   "metadata": {
    "id": "30eae8c8-8b03-4153-ae2a-9f3b63a00a7c"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5m9RpOuos8Qe",
   "metadata": {
    "id": "5m9RpOuos8Qe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pc3JyQ_ds8w-",
   "metadata": {
    "id": "pc3JyQ_ds8w-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
   "metadata": {
    "id": "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "tags": []
   },
   "source": [
    "# CatBoost\n",
    "**Prepare dataset to fit CatBoost requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "bbfe84c9-27cb-4292-aae1-eaf987d3e6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad",
   "metadata": {
    "id": "4223e056-f0e6-4f7d-a64a-6e2312c423ad",
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainingC, validationC = dataset_handler.prepare_data_CatBoost(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd",
   "metadata": {
    "id": "c7656808-1dc4-4ee2-b4b9-dafd208302bd"
   },
   "source": [
    "**Initilize the CatBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6",
   "metadata": {
    "id": "d9ec7e36-3097-4027-9689-fce5aa650ea6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from models import CatBoostNet\n",
    "cat_boost = CatBoostNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "365d2393-baca-492a-9b49-54d7b71f57ca",
   "metadata": {
    "id": "365d2393-baca-492a-9b49-54d7b71f57ca",
    "outputId": "230a9fd7-28c1-4b27-d384-21a743506dc5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'iterations': 27000,\n",
       " 'learning_rate': 0.001,\n",
       " 'loss_function': 'MultiRMSE',\n",
       " 'random_seed': 42,\n",
       " 'verbose': True,\n",
       " 'eval_metric': 'MultiRMSE',\n",
       " 'task_type': 'CPU',\n",
       " 'max_depth': 6,\n",
       " 'early_stopping_rounds': 300}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_boost.model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263",
   "metadata": {
    "id": "25bb2420-a30c-44cd-bc0c-cc4dde58a263"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "OpXZZV2VU0va",
   "metadata": {
    "id": "OpXZZV2VU0va",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_scores(gt, preds):\n",
    "    # Compute Normalized-RMSE and R2 metrics\n",
    "    rmse = root_mean_squared_error(gt, preds)\n",
    "    nrmse = rmse/(gt.max() - gt.min())\n",
    "    r2 = r2_score(gt, preds)\n",
    "    return nrmse, r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "64091893-834e-4605-8bea-c75b72172b4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\saved_models\\Brazil\\CATBOOST-06-11-2024-19-24-20\n",
      "Results saved to C:\\Users\\amanp\\Desktop\\MINOR\\projj\\code\\metrics\\Brazil\\catboost_normal26-11-2024-08-30-30.csv\n"
     ]
    }
   ],
   "source": [
    "TRAINING = False\n",
    "# nb_deps = 27\n",
    "\n",
    "if TRAINING:\n",
    "    # cat_boost.train(trainingC, validationC, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "    snapshot_path = os.path.join(config['output'], \"Brazil\", \"snapshot.bkp\")\n",
    "    # Run training with snapshot support\n",
    "    cat_boost.train(\n",
    "        trainingC,\n",
    "        validationC,\n",
    "        output_path=os.path.join(config['output'], \"Brazil\", f\"CATBOOST-normal-{today}\"),\n",
    "        snapshot_file=snapshot_path,\n",
    "        snapshot_interval=300  # Snapshot every 300 seconds\n",
    "    )\n",
    "\n",
    "\n",
    "else:\n",
    "    # get most recent catboost model\n",
    "    cat_boost_model = glob(join(config['output'], \"Brazil\", \"CATBOOST-06*\"))\n",
    "    if not cat_boost_model:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "        exit()\n",
    "    else:\n",
    "        cat_boost.load(cat_boost_model[-1])\n",
    "        print(f\"Loading model from: {cat_boost_model[-1]}\")\n",
    "\n",
    "    trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "    \n",
    "    preds_tra = cat_boost.model.predict(trainC[0])\n",
    "    preds_tra[preds_tra < 0] = 0\n",
    "    preds_val = cat_boost.model.predict(valC[0])\n",
    "    preds_val[preds_val < 0] = 0\n",
    "    # print(\"preds_tra: \\n\",preds_tra.shape,\"\\n\")\n",
    "    # print(\"preds_val: \\n\",preds_val.shape,\"\\n\")\n",
    "    \n",
    "        # Align predictions with department indices\n",
    "    # y_val_indices = validation_dataframe.index.to_list()\n",
    "    # y_val_indices_df = pd.DataFrame(y_val_indices, columns=['actual_index'])\n",
    "    \n",
    "    # y_train_indices = training_dataframe.index.to_list()\n",
    "    # y_train_indices_df = pd.DataFrame(y_train_indices, columns=['actual_index'])\n",
    "\n",
    "        # Create index mappings\n",
    "    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "    \n",
    "    # Initialize results list\n",
    "    results = []\n",
    "    preds_val_original = scaler.inverse_transform(preds_val)\n",
    "    y_val_original = scaler.inverse_transform(valL[1])\n",
    "    \n",
    "    # Inverse transform predictions and ground truth values for training data\n",
    "    preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "    y_train_original = scaler.inverse_transform(trainL[1])\n",
    "    \n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        # Filter rows corresponding to the current department for both training and validation\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "    \n",
    "        if department_rows_val.empty or department_rows_train.empty:\n",
    "            continue\n",
    "    \n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "    \n",
    "        # Matching indices for validation data\n",
    "        # matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "        # Matching indices for validation data\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        # Matching indices for training data\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "    \n",
    "        if matching_indices_val.empty or matching_indices_train.empty:\n",
    "            continue\n",
    "    \n",
    "        # Extract original scale true and predicted values for validation data\n",
    "        true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "        true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "    \n",
    "        # Extract original scale true and predicted values for training data\n",
    "        true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "        true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "    \n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)\n",
    "        # Calculate metrics for DengRate_all (Validation and Training)\n",
    "        mse_dengrate_all_val = mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mse_dengrate_all_train = mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "        rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "        \n",
    "        mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        \n",
    "        r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        # Calculate metrics for DengRate_019 (Validation and Training)\n",
    "        mse_dengrate_019_val = mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mse_dengrate_019_train = mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "        rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "        \n",
    "        mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "        \n",
    "        r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Discretize predictions for confusion matrix\n",
    "        true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "        predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "\n",
    "        true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "        predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "        # Store results\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "            'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "            'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "            'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "            'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "            'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "            'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "            'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "            'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "            'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "            'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "            'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "            'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "            'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "            'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "            'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "            'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "            'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "        })\n",
    "\n",
    "\n",
    "    # print(results)\n",
    "    results_df = pd.DataFrame(results)\n",
    "\n",
    "    #Save results\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_path = os.path.join(config['metrics'], \"Brazil\", f'catboost_normal{today}.csv')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
   "metadata": {
    "id": "d87ad018-27ff-45fc-8f3d-f2bb92bb2c82",
    "outputId": "1f846f6d-5da9-4308-b6eb-df65c8c2a593",
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "Le8KHGbJ2MuH",
   "metadata": {
    "id": "Le8KHGbJ2MuH"
   },
   "source": [
    "Write file with performance metrics for baseline model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "oJpCRde21sFh",
   "metadata": {
    "id": "oJpCRde21sFh",
    "jupyter": {
     "source_hidden": true
    },
    "outputId": "4ccba1df-bcc5-4f6b-b740-abde3d1fe009",
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nb_deps' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m res \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     17\u001b[0m res\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDepartment,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[43mnb_deps\u001b[49m):\n\u001b[0;32m     20\u001b[0m     idxt \u001b[38;5;241m=\u001b[39m preds_tra\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mnb_deps\n\u001b[0;32m     21\u001b[0m     idxv \u001b[38;5;241m=\u001b[39m preds_val\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mnb_deps\n",
      "\u001b[1;31mNameError\u001b[0m: name 'nb_deps' is not defined"
     ]
    }
   ],
   "source": [
    "cat_boost_model = glob(join(config['output'], \"Brazil\", \"catboost*\"))[0]\n",
    "cat_boost.load(cat_boost_model)\n",
    "\n",
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "preds_tra = cat_boost.model.predict(trainC[0])\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = cat_boost.model.predict(valC[0])\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "res = []\n",
    "res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "\n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    if baseAll_df.empty or base019_df.empty:\n",
    "        # print(f\"No baseline data for state_index {i}; skipping this plot.\")\n",
    "        plt.close(fig)\n",
    "        continue\n",
    "    print('--------------------- TRAINING scores ---------------------')\n",
    "    rmse1, r2 = compute_scores(gtt[:,0], baseAll_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group total): ', rmse1)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse2, r2 = compute_scores(gtt[:,1], base019_df['pred.mean'].iloc[t1])\n",
    "    print('N-RMSE (group 0-19): ', rmse2)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    print('-------------------- VALIDATION scores ---------------------')\n",
    "    rmse3, r2 = compute_scores(gtv[:,0], baseAll_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group total): ', rmse3)\n",
    "    print('R2   (group total): ', r2)\n",
    "\n",
    "    rmse4, r2 = compute_scores(gtv[:,1], base019_df['pred.mean'].iloc[t2])\n",
    "    print('N-RMSE (group 0-19): ', rmse4)\n",
    "    print('R2   (group 0-19): ', r2)\n",
    "\n",
    "    res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "\n",
    "    print('\\n##########################################################################################################')\n",
    "\n",
    "# # Save results\n",
    "# today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "# with open(join(config['metrics'], \"Brazil\", 'baseline-'+today+'.csv'),'w') as f:\n",
    "#     print('Writing performance metrics to file...')\n",
    "#     for item in res:\n",
    "#         f.write(\"%s\\n\" % item)\n",
    "\n",
    "# Save results\n",
    "today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "output_path = os.path.join(config['metrics'], \"Brazil\", f'baseline-{today}.csv')\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "with open(output_path, 'w', encoding='utf-8') as f:\n",
    "    print('Writing performance metrics to file...')\n",
    "    for item in res:\n",
    "        f.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79c5394f-1835-4a46-9fb1-645bfdd9f6e0",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# other stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EG-TYA2Ispib",
   "metadata": {
    "id": "EG-TYA2Ispib",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c80-UoybcjRp",
   "metadata": {
    "id": "c80-UoybcjRp",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Baseline\n",
    "**Upload results from Baseline Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bcad38f-0236-4f42-a998-31a44182f530",
   "metadata": {
    "id": "3bcad38f-0236-4f42-a998-31a44182f530"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Define output path for saving baseline files\n",
    "output_path = join(config['baseline'], \"Brazil\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Group by state, year, and month\n",
    "grouped = df.groupby(['CD_UF', 'Year', 'Month'], as_index=False)\n",
    "\n",
    "# Define baseline model function\n",
    "def create_baseline(df, target_col, population_col, filename):\n",
    "    # Initialize baseline predictions DataFrame\n",
    "    baseline_df = pd.DataFrame()\n",
    "\n",
    "    # Iterate over states and months to generate predictions\n",
    "    for _, group in grouped:\n",
    "        # Simple baseline: mean cases of the target column for the group\n",
    "        mean_cases = group[target_col].mean()\n",
    "\n",
    "        # Prepare result DataFrame\n",
    "        result = group[['Date', 'Year', 'Month', 'CD_UF', population_col]]\n",
    "        result['state_index'] = group['CD_UF']  # Using CD_UF as state index\n",
    "        result['pred.mean'] = mean_cases\n",
    "        result['pred.uci'] = mean_cases * 1.1  # +10% upper confidence interval\n",
    "        result['pred.lci'] = mean_cases * 0.9  # -10% lower confidence interval\n",
    "\n",
    "        baseline_df = pd.concat([baseline_df, result], axis=0)\n",
    "\n",
    "    # Scale predictions to cases per 100,000 individuals in the population\n",
    "    cols = ['pred.uci', 'pred.mean', 'pred.lci']\n",
    "    baseline_df[cols] = baseline_df[cols].div(baseline_df[population_col], axis=0).multiply(100000.0, axis=0)\n",
    "\n",
    "    # Save to CSV\n",
    "    baseline_df.to_csv(join(output_path, filename), index=False)\n",
    "    print(f\"Baseline file saved as: {filename}\")\n",
    "\n",
    "# Generate the two baseline files\n",
    "create_baseline(df, target_col='cases_total', population_col='PopTotal_UF', filename=\"Baseline_fitted_predicted_AllCases.csv\")\n",
    "create_baseline(df, target_col='cases0_19', population_col='Pop0_19_UF', filename=\"Baseline_fitted_predicted_0_19cases.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9423c72-12ef-4c68-86f8-f0d20d9bd227",
   "metadata": {
    "id": "a9423c72-12ef-4c68-86f8-f0d20d9bd227"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# Define output path for saving baseline files\n",
    "output_path = join(config['baseline'], \"Brazil\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Group by state, year, and month\n",
    "grouped = df.groupby(['CD_UF', 'Year', 'Month'], as_index=False)\n",
    "\n",
    "# Define linear regression-based model function\n",
    "def create_baseline(df, target_col, population_col, filename):\n",
    "    baseline_df = pd.DataFrame()\n",
    "\n",
    "    for _, group in grouped:\n",
    "        # Check if we have enough data points for regression\n",
    "        if len(group) < 2:\n",
    "            continue  # Skip groups with insufficient data for regression\n",
    "\n",
    "        # Prepare features (X) and target (y)\n",
    "        X = np.arange(len(group)).reshape(-1, 1)  # time-based index as feature\n",
    "        y = group[target_col].values\n",
    "\n",
    "        # Fit linear regression model\n",
    "        model = LinearRegression()\n",
    "        model.fit(X, y)\n",
    "\n",
    "        # Make predictions\n",
    "        predictions = model.predict(X)\n",
    "\n",
    "        print(predictions)\n",
    "\n",
    "    #     # Prepare result DataFrame\n",
    "    #     result = group[['Date', 'Year', 'Month', 'CD_UF', population_col]].copy()\n",
    "    #     result['state_index'] = group['CD_UF']\n",
    "    #     result['pred.mean'] = predictions\n",
    "    #     result['pred.uci'] = predictions * 1.1  # +10% for UCI\n",
    "    #     result['pred.lci'] = predictions * 0.9  # -10% for LCI\n",
    "\n",
    "\n",
    "    #     baseline_df = pd.concat([baseline_df, result], axis=0)\n",
    "\n",
    "    # # Scale predictions to cases per 100,000 individuals in the population\n",
    "    # cols = ['pred.uci', 'pred.mean', 'pred.lci']\n",
    "    # baseline_df[cols] = baseline_df[cols].div(baseline_df[population_col], axis=0).multiply(100000.0, axis=0)\n",
    "\n",
    "    # # Save to CSV\n",
    "    # baseline_df.to_csv(join(output_path, filename), index=False)\n",
    "    # print(f\"Baseline file saved as: {filename}\")\n",
    "\n",
    "# Generate the baseline files\n",
    "create_baseline(df, target_col='cases_total', population_col='PopTotal_UF', filename=\"Baseline_fitted_predicted_AllCases.csv\")\n",
    "create_baseline(df, target_col='cases0_19', population_col='Pop0_19_UF', filename=\"Baseline_fitted_predicted_0_19cases.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "FXbD73W0cr5L",
   "metadata": {
    "id": "FXbD73W0cr5L",
    "outputId": "89183d2e-e674-4735-938e-c648a4a1e342"
   },
   "outputs": [],
   "source": [
    "baselineAll = pd.read_csv(join(config['baseline'], \"Brazil\", \"Baseline_fitted_predicted_AllCases.csv\"))\n",
    "baselineAll = baselineAll[['Date','Year','Month','CD_UF','PopTotal_UF','state_index','pred.uci','pred.mean','pred.lci']]\n",
    "baselineAll['state_index'] -= 1\n",
    "baselineAll['Date'] = pd.to_datetime(baselineAll['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "# baselineAll.loc[:, cols] = baselineAll.loc[:, cols].div(baselineAll['PopTotal_UF'], axis=0).multiply(100000.0, axis=0)\n",
    "baselineAll.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69d6c628-a19c-47d2-a29f-e82ab40ec6c7",
   "metadata": {
    "id": "69d6c628-a19c-47d2-a29f-e82ab40ec6c7",
    "outputId": "4d3a019e-3f37-4ea0-b367-c9932b683317"
   },
   "outputs": [],
   "source": [
    "baseline019 = pd.read_csv(join(config['baseline'], \"Brazil\", \"Baseline_fitted_predicted_0_19cases.csv\"))\n",
    "baseline019 = baseline019[['Date','Year','Month','CD_UF','Pop0_19_UF','state_index','pred.uci','pred.mean','pred.lci']]\n",
    "baseline019['state_index'] -= 1\n",
    "baseline019['Date'] = pd.to_datetime(baseline019['Date'])\n",
    "\n",
    "cols = ['pred.uci','pred.mean','pred.lci']\n",
    "# baseline019.loc[:, cols] = baseline019.loc[:, cols].div(baseline019['Pop0_19_UF'], axis=0).multiply(100000.0, axis=0)\n",
    "baseline019.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "E3iGL37YB9FU",
   "metadata": {
    "id": "E3iGL37YB9FU",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## SVM\n",
    "**It uses the same data structure of CatBoost, no need to prepare data**\n",
    "\n",
    "**Initilize the SVM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "P0TfzBbeB8yl",
   "metadata": {
    "id": "P0TfzBbeB8yl"
   },
   "outputs": [],
   "source": [
    "from models import SVMNet\n",
    "svm = SVMNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2cb27c-3af5-4135-b4ef-e11b23993b1e",
   "metadata": {
    "id": "5c2cb27c-3af5-4135-b4ef-e11b23993b1e",
    "outputId": "96274375-35d5-4a40-f21d-975d88964c9a"
   },
   "outputs": [],
   "source": [
    "svm.model.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mho-Gh3HDan5",
   "metadata": {
    "id": "mho-Gh3HDan5"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ZXFfioFlC9-9",
   "metadata": {
    "id": "ZXFfioFlC9-9",
    "outputId": "e3238b97-c915-41b2-f309-1d6a3c985cb7"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    svm.train(trainingC, validationC, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent svm model\n",
    "    svm_model = glob(join(config['output'], \"Brazil\", \"svm*\"))\n",
    "    if svm_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        svm.load(svm_model[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = svm.model.predict(trainC[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = svm.model.predict(valC[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainC[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valC[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            if baseAll_df.empty or base019_df.empty:\n",
    "                # print(f\"No baseline data for state_index {i}; skipping this plot.\")\n",
    "                plt.close(fig)\n",
    "                continue\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'SVM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'SVM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # # Save results\n",
    "        # today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        # with open(join(config['metrics'], \"Brazil\", 'svm-'+today+'.csv'),'w') as f:\n",
    "        #     print('Writing performance metrics to file...')\n",
    "        #     for item in res:\n",
    "        #         f.write(\"%s\\n\" % item)\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        output_path = os.path.join(config['metrics'], \"Brazil\", f'svm-{today}.csv')\n",
    "\n",
    "        # Create the directory if it doesn't exist\n",
    "        os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "        with open(output_path, 'w', encoding='utf-8') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(f\"{item}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nhxwEojupvjg",
   "metadata": {
    "id": "nhxwEojupvjg",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## LSTM\n",
    "**Prepare dataset to fit LSTM requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "WxW7x16epvji",
   "metadata": {
    "id": "WxW7x16epvji"
   },
   "outputs": [],
   "source": [
    "trainingL, validationL = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ZMlQADOWpvjl",
   "metadata": {
    "id": "ZMlQADOWpvjl"
   },
   "source": [
    "**Initilize the LSTM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "En62JMKxpvjo",
   "metadata": {
    "id": "En62JMKxpvjo"
   },
   "outputs": [],
   "source": [
    "from models import LSTMNet\n",
    "lstm = LSTMNet(trainingL[0].shape[1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wBlGgF1Npvjq",
   "metadata": {
    "id": "wBlGgF1Npvjq"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "OX-zrXn8pvjs",
   "metadata": {
    "id": "OX-zrXn8pvjs",
    "outputId": "aa37b82a-81c2-427a-c912-eadbb468be8d"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "    lstm.train(trainingL, validationL, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # get most recent lstm model\n",
    "    lstm_model = glob(join(config['output'], \"Brazil\", \"lstm*\"))\n",
    "    if lstm_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        lstm.load(lstm_model[0])\n",
    "\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        preds_tra = lstm.model.predict(trainL[0])\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = lstm.model.predict(valL[0])\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            if baseAll_df.empty or base019_df.empty:\n",
    "                # print(f\"No baseline data for state_index {i}; skipping this plot.\")\n",
    "                plt.close(fig)\n",
    "                continue\n",
    "\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'LSTM prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'LSTM prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'lstm-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oAK1FjNaPrpZ",
   "metadata": {
    "id": "oAK1FjNaPrpZ",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Ensemble\n",
    "**Prepare dataset to fit the Ensemble requirements**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "O0dFRcYMhCDP",
   "metadata": {
    "id": "O0dFRcYMhCDP"
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd115599-acec-403e-94f0-9f708eb0d7f4",
   "metadata": {
    "id": "dd115599-acec-403e-94f0-9f708eb0d7f4"
   },
   "outputs": [],
   "source": [
    "def interpolation(trainL, valL):\n",
    "\n",
    "    def pol_zero(x, a):\n",
    "        return a\n",
    "\n",
    "    def pol_one(x, a, b):\n",
    "        return a + b*x\n",
    "\n",
    "    def pol_two(x, a, b, c):\n",
    "        return a + b*x + c*x*x\n",
    "\n",
    "    train_019_pol0 = []\n",
    "    train_019_pol1 = []\n",
    "    train_019_pol2 = []\n",
    "\n",
    "    train_all_pol0 = []\n",
    "    train_all_pol1 = []\n",
    "    train_all_pol2 = []\n",
    "\n",
    "    val_019_pol0 = []\n",
    "    val_019_pol1 = []\n",
    "    val_019_pol2 = []\n",
    "\n",
    "    val_all_pol0 = []\n",
    "    val_all_pol1 = []\n",
    "    val_all_pol2 = []\n",
    "\n",
    "    for i in tqdm(range(trainL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), trainL[0][i,:,-1])\n",
    "\n",
    "        train_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_019_pol1.append(pol_one(12, *popt_1))\n",
    "        train_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), trainL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1),  trainL[0][i,:,-2])\n",
    "\n",
    "        train_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        train_all_pol1.append(pol_one(12, *popt_1))\n",
    "        train_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    for i in tqdm(range(valL[0].shape[0])):\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-1])\n",
    "\n",
    "        val_019_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_019_pol1.append(pol_one(12, *popt_1))\n",
    "        val_019_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "        popt_0, _ = curve_fit(pol_zero, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_1, _ = curve_fit(pol_one, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "        popt_2, _ = curve_fit(pol_two, np.arange(0,12,1), valL[0][i,:,-2])\n",
    "\n",
    "        val_all_pol0.append(pol_zero(12, *popt_0))\n",
    "        val_all_pol1.append(pol_one(12, *popt_1))\n",
    "        val_all_pol2.append(pol_two(12, *popt_2))\n",
    "\n",
    "    train_019_pol0 = np.array(train_019_pol0)\n",
    "    train_019_pol1 = np.array(train_019_pol1)\n",
    "    train_019_pol2 = np.array(train_019_pol2)\n",
    "\n",
    "    train_all_pol0 = np.array(train_all_pol0)\n",
    "    train_all_pol1 = np.array(train_all_pol1)\n",
    "    train_all_pol2 = np.array(train_all_pol2)\n",
    "\n",
    "    val_019_pol0 = np.array(val_019_pol0)\n",
    "    val_019_pol1 = np.array(val_019_pol1)\n",
    "    val_019_pol2 = np.array(val_019_pol2)\n",
    "\n",
    "    val_all_pol0 = np.array(val_all_pol0)\n",
    "    val_all_pol1 = np.array(val_all_pol1)\n",
    "    val_all_pol2 = np.array(val_all_pol2)\n",
    "\n",
    "\n",
    "    train_pol0 = np.column_stack((train_019_pol0, train_all_pol0))\n",
    "    train_pol1 = np.column_stack((train_019_pol1, train_all_pol1))\n",
    "    train_pol2 = np.column_stack((train_019_pol2, train_all_pol2))\n",
    "\n",
    "    val_pol0 = np.column_stack((val_019_pol0, val_all_pol0))\n",
    "    val_pol1 = np.column_stack((val_019_pol1, val_all_pol1))\n",
    "    val_pol2 = np.column_stack((val_019_pol2, val_all_pol2))\n",
    "\n",
    "    return train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9EhJ44gxPrpb",
   "metadata": {
    "id": "9EhJ44gxPrpb",
    "tags": []
   },
   "source": [
    "**Initilize the Ensemble model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GX4eBGHH0LKS",
   "metadata": {
    "id": "GX4eBGHH0LKS"
   },
   "outputs": [],
   "source": [
    "from models import Ensamble, CatBoostEnsableNet, RandomForestEnsableNet\n",
    "ens = RandomForestEnsableNet(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "waM5x7G4Prpe",
   "metadata": {
    "id": "waM5x7G4Prpe"
   },
   "source": [
    "**Train or Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QeQEC7APXM2w",
   "metadata": {
    "id": "QeQEC7APXM2w",
    "outputId": "0241b547-e0b3-424b-9f6c-a4455b5a3972"
   },
   "outputs": [],
   "source": [
    "# Load the inner models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oKKkq04-Prpf",
   "metadata": {
    "id": "oKKkq04-Prpf",
    "outputId": "8f791db2-476c-496c-f9d9-849092f7d6aa"
   },
   "outputs": [],
   "source": [
    "TRAINING = False\n",
    "\n",
    "if TRAINING:\n",
    "\n",
    "    catBoost_train = cat_boost.model.predict(trainingC[0])\n",
    "    catBoost_train[catBoost_train < 0] = 0\n",
    "    catBoost_val = cat_boost.model.predict(validationC[0])\n",
    "    catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "    svm_train = svm.model.predict(trainingC[0])\n",
    "    svm_train[svm_train < 0] = 0\n",
    "    svm_val = svm.model.predict(validationC[0])\n",
    "    svm_val[svm_val < 0] = 0\n",
    "\n",
    "    lstm_train = lstm.model.predict(trainingL[0])\n",
    "    lstm_train[lstm_train < 0] = 0\n",
    "    lstm_val = lstm.model.predict(validationL[0])\n",
    "    lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "    train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainingL, validationL)\n",
    "\n",
    "    x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "    y_trainE = trainingL[1]\n",
    "    x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "    y_valE = validationL[1]\n",
    "\n",
    "    ens.train(x_trainE, y_trainE, x_valE, y_valE, output_path=join(config['output'], \"Brazil\"))\n",
    "\n",
    "else:\n",
    "    # Load the ensemble model\n",
    "    ens_model = glob(join(config['output'], \"Brazil\", \"rf*\"))\n",
    "    if ens_model == []:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "    else:\n",
    "        ens.load(ens_model[0])\n",
    "\n",
    "        trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "        trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "        catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "        catBoost_train[catBoost_train < 0] = 0\n",
    "        catBoost_val = cat_boost.model.predict(valC[0])\n",
    "        catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "        svm_train = svm.model.predict(trainC[0])\n",
    "        svm_train[svm_train < 0] = 0\n",
    "        svm_val = svm.model.predict(valC[0])\n",
    "        svm_val[svm_val < 0] = 0\n",
    "\n",
    "        lstm_train = lstm.model.predict(trainL[0])\n",
    "        lstm_train[lstm_train < 0] = 0\n",
    "        lstm_val = lstm.model.predict(valL[0])\n",
    "        lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "        train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "        x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis=-1)\n",
    "        y_trainE = trainingL[1]\n",
    "        x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis=-1)\n",
    "        y_valE = validationL[1]\n",
    "\n",
    "        preds_tra = ens.model.predict(x_trainE)\n",
    "        preds_tra[preds_tra < 0] = 0\n",
    "        preds_val = ens.model.predict(x_valE)\n",
    "        preds_val[preds_val < 0] = 0\n",
    "\n",
    "        datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "        dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "        # Total timesteps (nb of months inside this time window) = 228\n",
    "        month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "        ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "        res = []\n",
    "        res.append('Department,NRMSE 0-19 Training,NRMSE All Training,NRMSE 0-19 Validation,NRMSE All Validation')\n",
    "\n",
    "        for i in range(nb_deps):\n",
    "            fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "            idxt = preds_tra.shape[0]//nb_deps\n",
    "            idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "            t1 = ts[12:12+idxt]\n",
    "            t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "            # Reverting Data Norm ---------\n",
    "            pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "            pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "            gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "            gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "            # ------------------------------\n",
    "\n",
    "            baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "            base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "            axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "            axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "            axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "            axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "            axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "\n",
    "            # Baseline model\n",
    "            axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "            axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model Confidence Interval')\n",
    "            axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "            axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "            # Configure visualization\n",
    "            axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "            axes[0].set(ylabel='DIR Total Population')\n",
    "            axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "            axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "            axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "            axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "            for ax in axes:\n",
    "                ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "                ax.grid(True, linewidth=0.5)\n",
    "                ax.set_xlim(ts[0],ts[-1]+5)\n",
    "            axes[0].set_xticklabels([])\n",
    "            axes[1].set_xticklabels(dates)\n",
    "\n",
    "            fig.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "\n",
    "            print('--------------------- TRAINING scores ---------------------')\n",
    "            rmse1, r2 = compute_scores(gtt[:,0], pt[:,0])\n",
    "            print('N-RMSE (group total): ', rmse1)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse2, r2 = compute_scores(gtt[:,1], pt[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse2)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            print('-------------------- VALIDATION scores ---------------------')\n",
    "            rmse3, r2 = compute_scores(gtv[:,0], pv[:,0])\n",
    "            print('N-RMSE (group total): ', rmse3)\n",
    "            print('R2   (group total): ', r2)\n",
    "\n",
    "            rmse4, r2 = compute_scores(gtv[:,1], pv[:,1])\n",
    "            print('N-RMSE (group 0-19): ', rmse4)\n",
    "            print('R2   (group 0-19): ', r2)\n",
    "\n",
    "            res.append('{},{},{},{},{}'.format(DEP_NAMES[i], rmse2, rmse1, rmse4, rmse3))\n",
    "            print('\\n##########################################################################################################')\n",
    "\n",
    "        # Save results\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H:%M:%S\")\n",
    "        with open(join(config['metrics'], \"Brazil\", 'rf_ensemble-'+today+'.csv'),'w') as f:\n",
    "            print('Writing performance metrics to file...')\n",
    "            for item in res:\n",
    "                f.write(\"%s\\n\" % item)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FAj0jXtZy1sc",
   "metadata": {
    "id": "FAj0jXtZy1sc"
   },
   "source": [
    "Compute confidence intervals for the ensemble model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j2Ym-LbAVnvH",
   "metadata": {
    "id": "j2Ym-LbAVnvH",
    "outputId": "d6ed1a77-d23d-4eca-fe26-f809dfe6eb7a"
   },
   "outputs": [],
   "source": [
    "# Load the inner models\n",
    "cat_boost.load(glob(join(config['output'], \"Brazil\", \"catboost*\"))[0])\n",
    "svm.load(glob(join(config['output'], \"Brazil\", \"svm*\"))[0])\n",
    "lstm.load(glob(join(config['output'], \"Brazil\", \"lstm*\"))[0])\n",
    "\n",
    "# Ensemble\n",
    "ens.load(glob(join(config['output'], \"Brazil\", \"rf*\"))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fpt-M0gl7SbK",
   "metadata": {
    "id": "fpt-M0gl7SbK",
    "outputId": "f62f5470-1a8e-4374-9a7b-779b09d61d2c"
   },
   "outputs": [],
   "source": [
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "catBoost_train[catBoost_train < 0] = 0\n",
    "catBoost_val = cat_boost.model.predict(valC[0])\n",
    "catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "svm_train = svm.model.predict(trainC[0])\n",
    "svm_train[svm_train < 0] = 0\n",
    "svm_val = svm.model.predict(valC[0])\n",
    "svm_val[svm_val < 0] = 0\n",
    "\n",
    "lstm_train = lstm.model.predict(trainL[0])\n",
    "lstm_train[lstm_train < 0] = 0\n",
    "lstm_val = lstm.model.predict(valL[0])\n",
    "lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis = -1)\n",
    "y_trainE = trainingL[1]\n",
    "x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis = -1)\n",
    "y_valE = validationL[1]\n",
    "\n",
    "print(x_trainE.shape, y_trainE.shape, x_valE.shape, y_valE.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5F4bagE1W-ys",
   "metadata": {
    "id": "5F4bagE1W-ys"
   },
   "outputs": [],
   "source": [
    "def pred_ints(model, X, percentile=95):\n",
    "    err_down = []\n",
    "    err_up = []\n",
    "    for x in range(len(X)):\n",
    "        preds = []\n",
    "        for pred in model.estimators_:\n",
    "            preds.append(pred.predict(X[x].reshape(1, -1))[0])\n",
    "\n",
    "        preds = np.array(preds)\n",
    "        err_down.append(np.percentile(preds, (100 - percentile) / 2., axis=0))\n",
    "        err_up.append(np.percentile(preds, 100 - (100 - percentile) / 2., axis=0))\n",
    "    return np.array(err_down), np.array(err_up)\n",
    "\n",
    "train_err_down, train_err_up = pred_ints(ens.model, x_trainE, percentile=95)\n",
    "val_err_down, val_err_up = pred_ints(ens.model, x_valE, percentile=95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "IaSxa8z-4QYt",
   "metadata": {
    "id": "IaSxa8z-4QYt"
   },
   "outputs": [],
   "source": [
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(18,9))\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    lstm_pt = scaler.inverse_transform(lstm_train[idxt*i:idxt*(i+1), :])\n",
    "    lstm_pv = scaler.inverse_transform(lstm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    svm_pt = scaler.inverse_transform(svm_train[idxt*i:idxt*(i+1), :])\n",
    "    svm_pv = scaler.inverse_transform(svm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    cb_pt = scaler.inverse_transform(catBoost_train[idxt*i:idxt*(i+1), :])\n",
    "    cb_pv = scaler.inverse_transform(catBoost_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "\n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    ########################\n",
    "    ### Total population ###\n",
    "    ########################\n",
    "    axes[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes[0].plot(t1, pt[:,0], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    #####################\n",
    "    ### Children 0-19 ###\n",
    "    #####################\n",
    "    axes[1].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes[1].plot(t1, pt[:,1], '--', color='red', label = 'Ensemble prediction on Training data')\n",
    "    axes[1].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes[1].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction on Validation data')\n",
    "    axes[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes[1].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes[1].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left')\n",
    "    axes[0].set(ylabel='DIR Total Population')\n",
    "    axes[1].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    axes[1].legend(ncol=3, loc='lower center', bbox_to_anchor=(0.5,-0.5), shadow=True)\n",
    "    axes[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes[1].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    for ax in axes:\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "    axes[0].set_xticklabels([])\n",
    "    axes[1].set_xticklabels(dates)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WA7JBRiVyLxC",
   "metadata": {
    "id": "WA7JBRiVyLxC"
   },
   "source": [
    "**Get time series plots with validation period zoomed.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cKbtZp7hTY-",
   "metadata": {
    "id": "4cKbtZp7hTY-"
   },
   "outputs": [],
   "source": [
    "from matplotlib.patches import Patch\n",
    "from matplotlib.lines import Line2D\n",
    "y_rescaled = False\n",
    "\n",
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "datelist = pd.date_range('01-01-2001', end='01-01-2020', freq='YS')\n",
    "dates = datelist.strftime(\"%Y\").tolist()\n",
    "\n",
    "# Total timesteps (nb of months inside this time window) = 228\n",
    "month_datelist = pd.date_range('01-01-2001', end='31-12-2019', freq='MS')\n",
    "ts = np.arange(0, len(month_datelist), 1)\n",
    "val_dates = pd.date_range('01-01-2017', end='31-12-2019', freq = 'M').strftime(\"%m-%Y\").tolist()\n",
    "\n",
    "data = []\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, (axes1, axes2) = plt.subplots(nrows=2, ncols=2, figsize=(25,9), gridspec_kw={'width_ratios': [7, 3], 'wspace':0.01, 'hspace':0.06})\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    t1 = ts[12:12+idxt]\n",
    "    t2 = ts[(12+idxt):(12+idxt+idxv)]\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    t_lci = scaler.inverse_transform(train_err_down[idxt*i:idxt*(i+1), :])\n",
    "    t_uci = scaler.inverse_transform(train_err_up[idxt*i:idxt*(i+1), :])\n",
    "\n",
    "    v_lci = scaler.inverse_transform(val_err_down[idxv*i:idxv*(i+1), :])\n",
    "    v_uci = scaler.inverse_transform(val_err_up[idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "\n",
    "    baseAll_df = baselineAll[baselineAll.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "    base019_df = baseline019[baseline019.state_index == i][['pred.mean','pred.uci','pred.lci']]\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # TOTAL POPULATION\n",
    "    axes1[0].plot(t1, gtt[:,0], '-', color='orange', label = 'Observed Cases')\n",
    "    axes1[0].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[0].plot(t1, pt[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t1, y1=(pt[:,0]-t_lci[:,0]), y2=(pt[:,0]+t_uci[:,0]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes1[0].plot(t2, pv[:,0], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes1[0].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes1[0].plot(t1, baseAll_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes1[0].fill_between(x=t1, y1=baseAll_df['pred.lci'].iloc[t1], y2=baseAll_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes1[0].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[0].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes1[1].plot(t2, gtv[:,0], '-', color='orange')\n",
    "    axes1[1].plot(t2, pv[:,0], '-', color='red')\n",
    "    axes1[1].fill_between(t2, y1=(pv[:,0]-v_lci[:,0]), y2=(pv[:,0]+v_uci[:,0]), facecolor='red', alpha=.1)\n",
    "    axes1[1].plot(t2, baseAll_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes1[1].fill_between(x=t2, y1=baseAll_df['pred.lci'].iloc[t2], y2=baseAll_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    #________________________________________________________________________\n",
    "    # AGE 0-19\n",
    "    axes2[0].plot(t1, gtt[:,1], '-', color='orange', label = 'Observed Cases')\n",
    "    axes2[0].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[0].plot(t1, pt[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t1, y1=(pt[:,1]-t_lci[:,1]), y2=(pt[:,1]+t_uci[:,1]), facecolor='red', alpha=.1, label='Ensemble CI')\n",
    "    axes2[0].plot(t2, pv[:,1], '-', color='red', label = 'Ensemble prediction')\n",
    "    axes2[0].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "\n",
    "    # Baseline model\n",
    "    axes2[0].plot(t1, base019_df['pred.mean'].iloc[t1], color='forestgreen', label = 'Baseline model prediction')\n",
    "    axes2[0].fill_between(x=t1, y1=base019_df['pred.lci'].iloc[t1], y2=base019_df['pred.uci'].iloc[t1], facecolor='forestgreen', alpha=.1, label='Baseline model CI')\n",
    "    axes2[0].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[0].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Validation Zoom\n",
    "    axes2[1].plot(t2, gtv[:,1], '-', color='orange')\n",
    "    axes2[1].plot(t2, pv[:,1], '-', color='red')\n",
    "    axes2[1].fill_between(t2, y1=(pv[:,1]-v_lci[:,1]), y2=(pv[:,1]+v_uci[:,1]), facecolor='red', alpha=.1)\n",
    "    axes2[1].plot(t2, base019_df['pred.mean'].iloc[t2], color='forestgreen')\n",
    "    axes2[1].fill_between(x=t2, y1=base019_df['pred.lci'].iloc[t2], y2=base019_df['pred.uci'].iloc[t2], facecolor='forestgreen', alpha=.1)\n",
    "\n",
    "    # Configure visualization\n",
    "    axes1[0].set_title(DEP_NAMES[i], fontsize = 18, loc='left', fontweight=\"bold\")\n",
    "    axes1[1].set_title('Validation period', fontsize = 15)\n",
    "    axes1[0].set(ylabel='DIR Total Population')\n",
    "    axes2[0].set(xlabel='Year', ylabel='DIR 0-19')\n",
    "    axes2[1].set(xlabel='Month-Year')\n",
    "    axes1[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "    axes2[0].axvline(t2[0], color='black', linewidth=2, linestyle='--', alpha=0.5)\n",
    "\n",
    "    legend_elements = [Line2D([0], [0], color='orange', lw=2),\n",
    "                       (Line2D([0], [0], color='red', lw=2), Patch(facecolor='red', alpha=.1, lw=2)),\n",
    "                       (Line2D([0], [0], color='forestgreen', lw=2), Patch(facecolor='forestgreen', alpha=.1, lw=2))\n",
    "                    ]\n",
    "    axes2[0].legend(handles=legend_elements, labels=['Observed Cases', 'Ensemble Prediction + CI', 'Baseline Prediction + CI'],\n",
    "                    ncol=3, loc='lower center', bbox_to_anchor=(0.71,-0.55), shadow=True)\n",
    "\n",
    "\n",
    "    for ax in [axes1[0], axes2[0]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(0, len(ts)+1, 12))\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(ts[0],ts[-1]+5)\n",
    "\n",
    "    for ax in [axes1[1], axes2[1]]:\n",
    "        #ax.set_yscale('log')\n",
    "        ax.set_xticks(np.arange(t2[0],t2[0]+len(t2),3))\n",
    "        ax.yaxis.tick_right()\n",
    "        ax.grid(True, linewidth=0.5)\n",
    "        ax.set_xlim(t2[0]-1,)\n",
    "\n",
    "    if y_rescaled:\n",
    "        ymax = max(np.max(gtt[:,0]), np.max(gtv[:,0]), np.max(pt[:,0]), np.max(pv[:,0]))\n",
    "        ymax_019 = max(np.max(gtt[:,1]), np.max(gtv[:,1]), np.max(pt[:,1]), np.max(pv[:,1]))\n",
    "        ymin = min(np.min(gtt[:,0]), np.min(gtv[:,0]), np.min(pt[:,0]), np.min(pv[:,0]))\n",
    "        ymin_019 = min(np.min(gtt[:,1]), np.min(gtv[:,1]), np.min(pt[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        val_ymax = max(np.max(gtv[:,0]), np.max(pv[:,0]))\n",
    "        val_ymax_019 = max(np.max(gtv[:,1]), np.max(pv[:,1]))\n",
    "        val_ymin = min(np.min(gtv[:,0]), np.min(pv[:,0]))\n",
    "        val_ymin_019 = min(np.min(gtv[:,1]), np.min(pv[:,1]))\n",
    "\n",
    "        # 5% bottom margin and 10% top margin\n",
    "        minY = ymin-0.05*(ymax-ymin)\n",
    "        maxY = ymax+0.1*(ymax-ymin)\n",
    "        minY_019 = ymin_019-0.05*(ymax_019-ymin_019)\n",
    "        maxY_019 = ymax_019+0.1*(ymax_019-ymin_019)\n",
    "        val_minY = val_ymin-0.05*(val_ymax-val_ymin)\n",
    "        val_maxY = val_ymax+0.1*(val_ymax-val_ymin)\n",
    "        val_minY_019 = val_ymin_019-0.05*(val_ymax_019-val_ymin_019)\n",
    "        val_maxY_019 = val_ymax_019+0.1*(val_ymax_019-val_ymin_019)\n",
    "\n",
    "        axes1[0].set_ylim((minY, maxY))\n",
    "        axes2[0].set_ylim((minY_019, maxY_019))\n",
    "        axes1[1].set_ylim((val_minY, val_maxY))\n",
    "        axes2[1].set_ylim((val_minY_019, val_maxY_019))\n",
    "\n",
    "    axes1[0].set_xticklabels([])\n",
    "    axes2[0].set_xticklabels(dates)\n",
    "    axes1[1].set_xticklabels([])\n",
    "    axes2[1].set_xticklabels(val_dates[::3], rotation=45)\n",
    "\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "\n",
    "    # print('-------------------- VALIDATION scores ---------------------')\n",
    "    rmseAll, _ = compute_scores(gtv[:,0], pv[:,0])\n",
    "    rmseAll_uci, _ = compute_scores(gtv[:,0], v_uci[:,0])\n",
    "    rmseAll_lci, _ = compute_scores(gtv[:,0], v_lci[:,0])\n",
    "\n",
    "    rmse019, _ = compute_scores(gtv[:,1], pv[:,1])\n",
    "    rmse019_uci, _ = compute_scores(gtv[:,1], v_uci[:,1])\n",
    "    rmse019_lci, _ = compute_scores(gtv[:,1], v_lci[:,1])\n",
    "\n",
    "    data.append([i, DEP_NAMES[i], rmseAll, rmseAll_uci, rmseAll_lci, rmse019, rmse019_uci, rmse019_lci])\n",
    "\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0y2iVphNi2Bo",
   "metadata": {
    "id": "0y2iVphNi2Bo",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Compare Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9o0QM6JIGJaC",
   "metadata": {
    "id": "9o0QM6JIGJaC"
   },
   "source": [
    "## Time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mAjoscoBGJGf",
   "metadata": {
    "id": "mAjoscoBGJGf",
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "trainC, valC = dataset_handler.prepare_data_CatBoost(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "trainL, valL = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "catBoost_train = cat_boost.model.predict(trainC[0])\n",
    "catBoost_train[catBoost_train < 0] = 0\n",
    "catBoost_val = cat_boost.model.predict(valC[0])\n",
    "catBoost_val[catBoost_val < 0] = 0\n",
    "\n",
    "svm_train = svm.model.predict(trainC[0])\n",
    "svm_train[svm_train < 0] = 0\n",
    "svm_val = svm.model.predict(valC[0])\n",
    "svm_val[svm_val < 0] = 0\n",
    "\n",
    "lstm_train = lstm.model.predict(trainL[0])\n",
    "lstm_train[lstm_train < 0] = 0\n",
    "lstm_val = lstm.model.predict(valL[0])\n",
    "lstm_val[lstm_val < 0] = 0\n",
    "\n",
    "train_pol0, train_pol1, train_pol2, val_pol0, val_pol1, val_pol2 = interpolation(trainL, valL)\n",
    "\n",
    "x_trainE = np.concatenate((catBoost_train, svm_train, lstm_train, train_pol0, train_pol1, train_pol2), axis = -1)\n",
    "y_trainE = trainingL[1]\n",
    "x_valE = np.concatenate((catBoost_val, svm_val, lstm_val, val_pol0, val_pol1, val_pol2), axis = -1)\n",
    "y_valE = validationL[1]\n",
    "\n",
    "preds_tra = ens.model.predict(x_trainE)\n",
    "preds_tra[preds_tra < 0] = 0\n",
    "preds_val = ens.model.predict(x_valE)\n",
    "preds_val[preds_val < 0] = 0\n",
    "\n",
    "dates = pd.date_range('01-01-2001', end='31-12-2019', freq = 'Y').strftime(\"%Y\").tolist()\n",
    "val_dates = pd.date_range('01-01-2016', end='31-12-2019', freq = 'M').strftime(\"%m-%Y\").tolist()\n",
    "\n",
    "for i in range(nb_deps):\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(nrows=2, ncols=2, figsize=(30,10), gridspec_kw={'width_ratios': [3, 2]})\n",
    "    t1 = np.arange(0, preds_tra.shape[0]//nb_deps, 1)\n",
    "    t2 = preds_tra.shape[0]//nb_deps + np.arange(0, preds_val.shape[0]//nb_deps, 1)\n",
    "\n",
    "    idxt = preds_tra.shape[0]//nb_deps\n",
    "    idxv = preds_val.shape[0]//nb_deps\n",
    "\n",
    "    # Reverting Data Norm ---------\n",
    "    lstm_pt = scaler.inverse_transform(lstm_train[idxt*i:idxt*(i+1), :])\n",
    "    lstm_pv = scaler.inverse_transform(lstm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    ens_pt = scaler.inverse_transform(preds_tra[idxt*i:idxt*(i+1), :])\n",
    "    ens_pv = scaler.inverse_transform(preds_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    svm_pt = scaler.inverse_transform(svm_train[idxt*i:idxt*(i+1), :])\n",
    "    svm_pv = scaler.inverse_transform(svm_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    cb_pt = scaler.inverse_transform(catBoost_train[idxt*i:idxt*(i+1), :])\n",
    "    cb_pv = scaler.inverse_transform(catBoost_val[idxv*i:idxv*(i+1), :])\n",
    "\n",
    "    gtt = scaler.inverse_transform(trainL[1][idxt*i:idxt*(i+1), :])\n",
    "    gtv = scaler.inverse_transform(valL[1][idxv*i:idxv*(i+1), :])\n",
    "    # ------------------------------\n",
    "\n",
    "    ##############################\n",
    "    ### Cases Total Population ###\n",
    "    ##############################\n",
    "    ax1.plot(t1, gtt[:,0], '-', color='orange', label='Observed Cases')\n",
    "    ax1.plot(t2, gtv[:,0], '-', color='orange')\n",
    "    ax1.set_title('Dengue Incidence Rate (DIR) total for {}'.format(DEP_NAMES[i]), fontsize = 18)\n",
    "\n",
    "    # Ensemble\n",
    "    ax1.plot(t1, ens_pt[:,0], '--', color='darkgrey', label='Ensemble on Training data')\n",
    "    ax1.plot(t2, ens_pv[:,0], '-', color='black', label='Ensemble on Validation data')\n",
    "    # LSTM\n",
    "    ax1.plot(t1, lstm_pt[:,0], '--', color='plum', label='LSTM on Training data')\n",
    "    ax1.plot(t2, lstm_pv[:,0], '-', color='purple', label='LSTM on Validation data')\n",
    "    # SVM\n",
    "    ax1.plot(t1, svm_pt[:,0], '--', color='dodgerblue', label='SVM on Training data')\n",
    "    ax1.plot(t2, svm_pv[:,0], '-', color='blue', label='SVM on Validation data')\n",
    "    # CatBoost\n",
    "    ax1.plot(t1, cb_pt[:,0], '--', color='yellowgreen', label='CatBoost on Training data')\n",
    "    ax1.plot(t2, cb_pv[:,0], '-', color='green', label='CatBoost on Validation data')\n",
    "    ax1.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "    ax1.set_xticklabels(dates)\n",
    "    ax1.set_xlim(t1[0],)\n",
    "    ax1.set(xlabel='Year', ylabel='DIR')\n",
    "    ax1.grid(True, linewidth=0.5)\n",
    "\n",
    "    #################\n",
    "    ax2.plot(t2, gtv[:,0], '-', color='orange', label='Observed Cases')\n",
    "    ax2.set_title('Zoom on the Validation period', fontsize = 18)\n",
    "    ax2.plot(t2, ens_pv[:,0], '-', color='black', label='Ensemble')\n",
    "    ax2.plot(t2, lstm_pv[:,0], '-', color='purple', label='LSTM')\n",
    "    ax2.plot(t2, svm_pv[:,0], '-', color='blue', label='SVM')\n",
    "    ax2.plot(t2, cb_pv[:,0], '-', color='green', label='CatBoost')\n",
    "    ax2.set_xticks(t2)\n",
    "    ax2.set_xticklabels(val_dates, rotation=45)\n",
    "    ax2.set_xlim(t2[0],)\n",
    "    ax2.set(xlabel='Month-Year', ylabel='DIR')\n",
    "    ax2.grid(True, linewidth=0.5)\n",
    "\n",
    "    #############################\n",
    "    ### Cases Population 0-19 ###\n",
    "    #############################\n",
    "    ax3.plot(t1, gtt[:,1], '-', color='orange', label='Observed Cases')\n",
    "    ax3.plot(t2, gtv[:,1], '-', color='orange')\n",
    "    ax3.set_title('Dengue Incidence Rate (DIR) 0-19 for {}'.format(DEP_NAMES[i]), fontsize = 18)\n",
    "\n",
    "    # Ensemble\n",
    "    ax3.plot(t1, ens_pt[:,1], '--', color='darkgrey', label='Ensemble on Training data')\n",
    "    ax3.plot(t2, ens_pv[:,1], '-', color='black', label='Ensemble on Validation data')\n",
    "    # LSTM\n",
    "    ax3.plot(t1, lstm_pt[:,1], '--', color='plum', label='LSTM on Training data')\n",
    "    ax3.plot(t2, lstm_pv[:,1], '-', color='purple', label='LSTM on Validation data')\n",
    "    # SVM\n",
    "    ax3.plot(t1, svm_pt[:,1], '--', color='dodgerblue', label='SVM on Training data')\n",
    "    ax3.plot(t2, svm_pv[:,1], '-', color='blue', label='SVM on Validation data')\n",
    "    # CatBoost\n",
    "    ax3.plot(t1, cb_pt[:,1], '--', color='yellowgreen', label='CatBoost on Training data')\n",
    "    ax3.plot(t2, cb_pv[:,1], '-', color='green', label='CatBoost on Validation data')\n",
    "    ax3.set_xticks(np.arange(0, idxt+idxv, 11))\n",
    "    ax3.set_xticklabels(dates)\n",
    "    ax3.set_xlim(t1[0],)\n",
    "    ax3.set(xlabel='Year', ylabel='DIR')\n",
    "    ax3.grid(True, linewidth=0.5)\n",
    "\n",
    "    #################\n",
    "    ax4.plot(t2, gtv[:,1], '-', color='orange', label='Observed Cases')\n",
    "    ax4.set_title('Zoom on the Validation period', fontsize = 18)\n",
    "    ax4.plot(t2, ens_pv[:,1], '-', color='black', label='Ensemble')\n",
    "    ax4.plot(t2, lstm_pv[:,1], '-', color='purple', label='LSTM')\n",
    "    ax4.plot(t2, svm_pv[:,1], '-', color='blue', label='LSTM')\n",
    "    ax4.plot(t2, cb_pv[:,1], '-', color='green', label='CatBoost')\n",
    "    ax4.set_xticks(t2)\n",
    "    ax4.set_xticklabels(val_dates, rotation=45)\n",
    "    ax4.set_xlim(t2[0],)\n",
    "    ax4.set(xlabel='Month-Year', ylabel='DIR')\n",
    "    ax4.grid(True, linewidth=0.5)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    ax3.legend(fontsize=14, ncol=4, shadow=True, loc='center', bbox_to_anchor=(0.7,-0.5))\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    print('\\n##########################################################################################################')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XZ1ow2z-GAZx",
   "metadata": {
    "id": "XZ1ow2z-GAZx"
   },
   "source": [
    "## Normalized RMSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8iABrDo0s_-0",
   "metadata": {
    "id": "8iABrDo0s_-0"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9qk-84BmVGWb",
   "metadata": {
    "id": "9qk-84BmVGWb"
   },
   "outputs": [],
   "source": [
    "catD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'catboost*'))[0])\n",
    "svmD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'svm*'))[0])\n",
    "lstmD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'lstm*'))[0])\n",
    "rf_ensD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'rf*'))[0])\n",
    "baseD = pd.read_csv(glob(join(config['metrics'], \"Brazil\", 'baseline*'))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "TyT39eNgaGFd",
   "metadata": {
    "id": "TyT39eNgaGFd"
   },
   "source": [
    "Comparison between all models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R6GNhs9ljFdJ",
   "metadata": {
    "id": "R6GNhs9ljFdJ"
   },
   "outputs": [],
   "source": [
    "X = np.arange(0, 2*nb_deps, 2)\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(X, catD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[0].bar(X+0.3, svmD[\"NRMSE 0-19 Training\"], color='blue', label='SVM', width=0.3)\n",
    "axes[0].bar(X+0.6, lstmD[\"NRMSE 0-19 Training\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[0].bar(X+0.9, rf_ensD[\"NRMSE 0-19 Training\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[0].bar(X+1.2, baseD[\"NRMSE 0-19 Training\"], color='green', label='Baseline', width=0.3)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "\n",
    "axes[1].bar(X, catD[\"NRMSE All Training\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[1].bar(X+0.3, svmD[\"NRMSE All Training\"], color='blue', label='SVM', width=0.3)\n",
    "axes[1].bar(X+0.6, lstmD[\"NRMSE All Training\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[1].bar(X+0.9, rf_ensD[\"NRMSE All Training\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[1].bar(X+1.2, baseD[\"NRMSE All Training\"], color='green', label='Baseline', width=0.3)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(0.6+np.arange(0, 2*nb_deps, 2))\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.legend(shadow=True, ncol=5)\n",
    "    ax.set_xlim(X[0]-0.6, X[-1]+2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(X, catD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[0].bar(X+0.3, svmD[\"NRMSE 0-19 Validation\"], color='blue', label='SVM', width=0.3)\n",
    "axes[0].bar(X+0.6, lstmD[\"NRMSE 0-19 Validation\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[0].bar(X+0.9, rf_ensD[\"NRMSE 0-19 Validation\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[0].bar(X+1.2, baseD[\"NRMSE 0-19 Validation\"], color='green', label='Baseline', width=0.3)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "\n",
    "axes[1].bar(X, catD[\"NRMSE All Validation\"], color='red', label='CatBoost', width=0.3)\n",
    "axes[1].bar(X+0.3, svmD[\"NRMSE All Validation\"], color='blue', label='SVM', width=0.3)\n",
    "axes[1].bar(X+0.6, lstmD[\"NRMSE All Validation\"], color='orange', label='LSTM', width=0.3)\n",
    "axes[1].bar(X+0.9, rf_ensD[\"NRMSE All Validation\"], color='purple', label='Ensemble', width=0.3)\n",
    "axes[1].bar(X+1.2, baseD[\"NRMSE All Validation\"], color='green', label='Baseline', width=0.3)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_xticks(0.6+np.arange(0, 2*nb_deps, 2))\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.legend(shadow=True, ncol=5)\n",
    "    ax.set_xlim(X[0]-0.6, X[-1]+2)\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oq4kVB9WaRBH",
   "metadata": {
    "id": "oq4kVB9WaRBH"
   },
   "source": [
    "Comparison between 2 models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R-5bTep37ma5",
   "metadata": {
    "id": "R-5bTep37ma5"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(rf_ensD[\"Department\"], baseD[\"NRMSE 0-19 Training\"] - rf_ensD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[0].set_title('N-RMSE comparison for population 0-19 on Training set')\n",
    "axes[1].bar(rf_ensD[\"Department\"], baseD[\"NRMSE All Training\"]- rf_ensD[\"NRMSE All Training\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[1].set_title('N-RMSE comparison for All population on Training set')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$')\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(rf_cnn_ensD_withoutMaxDepth[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(rf_ensD[\"Department\"], baseD[\"NRMSE 0-19 Validation\"] - rf_ensD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[0].set_title('N-RMSE comparison for population 0-19 on Validation set')\n",
    "axes[1].bar(rf_ensD[\"Department\"], baseD[\"NRMSE All Validation\"]- rf_ensD[\"NRMSE All Validation\"], color='red', label='Ensemble vs. Baseline', width=0.25)\n",
    "axes[1].set_title('N-RMSE comparison for All population on Validation set')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$')\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(rf_ensD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zgdr5w9r_dYm",
   "metadata": {
    "id": "zgdr5w9r_dYm"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(17,8))\n",
    "\n",
    "X = np.arange(len(rf_ensD[\"Department\"].values))\n",
    "\n",
    "axes.bar(X, baseD[\"NRMSE 0-19 Validation\"] - rf_ensD[\"NRMSE 0-19 Validation\"], color='red', label='Population 0-19', width=0.25)\n",
    "axes.bar(X+0.25, baseD[\"NRMSE All Validation\"]- rf_ensD[\"NRMSE All Validation\"], color='blue', label='Population All', width=0.25)\n",
    "\n",
    "axes.set_title('N-RMSE comparison on Validation set', fontsize=18)\n",
    "axes.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "axes.set_ylabel('NRMSE$_{baseline}$ $-$ NRMSE$_{proposed}$', fontsize=16)\n",
    "axes.legend(shadow=True, fontsize=16)\n",
    "axes.grid(True, linewidth=0.5)\n",
    "axes.set_xticks(X+0.125)\n",
    "axes.set_xticklabels(rf_ensD[\"Department\"].values, rotation=90, fontsize=16)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v7BzdPF4oQN6",
   "metadata": {
    "id": "v7BzdPF4oQN6"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - catD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- catD[\"NRMSE All Training\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - catD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"]- catD[\"NRMSE All Validation\"], color='red', label='Ensemble vs. CatBoost', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oDW2ehUfIeFy",
   "metadata": {
    "id": "oDW2ehUfIeFy"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - svmD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- svmD[\"NRMSE All Training\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - svmD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"]- svmD[\"NRMSE All Validation\"], color='red', label='Ensemble RF vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jqAiaKO_JJ6B",
   "metadata": {
    "id": "jqAiaKO_JJ6B"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], rf_ensD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], rf_ensD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='Ensemble RF vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RWrb-sSaJTCX",
   "metadata": {
    "id": "RWrb-sSaJTCX"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], svmD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], svmD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], svmD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], svmD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='SVM vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "KUgi7d7eJkm4",
   "metadata": {
    "id": "KUgi7d7eJkm4"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Training\"] - lstmD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Training\"]- lstmD[\"NRMSE All Training\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Validation\"] - lstmD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Validation\"] - lstmD[\"NRMSE All Validation\"], color='red', label='CatBoost vs. LSTM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xGCFhDDZJu8W",
   "metadata": {
    "id": "xGCFhDDZJu8W"
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Training\"] - svmD[\"NRMSE 0-19 Training\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Training')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Training\"]- svmD[\"NRMSE All Training\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Training')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "\n",
    "fig, axes = plt.subplots(nrows = 1, ncols = 2, figsize = (30,10))\n",
    "axes[0].bar(catD[\"Department\"], catD[\"NRMSE 0-19 Validation\"] - svmD[\"NRMSE 0-19 Validation\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[0].set_title('N-RMSE 0-19 Validation')\n",
    "axes[1].bar(catD[\"Department\"], catD[\"NRMSE All Validation\"] - svmD[\"NRMSE All Validation\"], color='red', label='CatBoost vs. SVM', width=0.25)\n",
    "axes[1].set_title('N-RMSE All Validation')\n",
    "\n",
    "for ax in axes:\n",
    "    ax.axhline(y=0, color='black', linewidth=1, linestyle='--')\n",
    "    ax.legend(shadow=True)\n",
    "    ax.grid(True, linewidth=0.5)\n",
    "    ax.set_xticklabels(catD[\"Department\"].values, rotation=90)\n",
    "\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "WAMq89IFtBpL",
   "metadata": {
    "id": "WAMq89IFtBpL",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hbpqLUjvG85L",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hbpqLUjvG85L",
    "outputId": "f561524b-6dff-4b8a-a952-aa58a2a1643e"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/GAUTAMMANU/projminor.git\n",
    "%cd projminor\n",
    "%cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jU7sojnhtyLt",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jU7sojnhtyLt",
    "outputId": "80ae15c1-a5ca-4b2d-acbb-2ac872a82970"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "# os.chdir(join('..','data'))\n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))\n",
    "\n",
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "project_path\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "CU2MjhNw0H8k",
   "metadata": {
    "id": "CU2MjhNw0H8k"
   },
   "outputs": [],
   "source": [
    "class datasetHandler:\n",
    "    def __init__(self, training_dataframe, validation_dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the dataset handler with training and validation dataframes.\n",
    "        \"\"\"\n",
    "        self.training_dataframe = training_dataframe\n",
    "        self.validation_dataframe = validation_dataframe\n",
    "\n",
    "    def get_data(self, feature_type='learning', target_type='prediction'):\n",
    "        \"\"\"\n",
    "        Retrieve the features and target variables for learning and prediction.\n",
    "\n",
    "        Args:\n",
    "        - feature_type (str): Type of features to retrieve, e.g., 'learning' (X) or 'prediction' (y)\n",
    "        - target_type (str): Type of target to retrieve, e.g., 'prediction' (y)\n",
    "\n",
    "        Returns:\n",
    "        - Tuple: x_train, y_train, x_val, y_val\n",
    "        \"\"\"\n",
    "\n",
    "        # Extract features based on the feature_type\n",
    "        if feature_type == 'learning':\n",
    "            # Extracting features (PCA components in this case)\n",
    "            x_train = self.training_dataframe.drop(columns=['Year', 'DengRate_all', 'DengRate_019'])\n",
    "            x_val = self.validation_dataframe.drop(columns=['Year', 'DengRate_all', 'DengRate_019'])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported feature type\")\n",
    "\n",
    "        # Extracting targets (Dengue rates)\n",
    "        if target_type == 'prediction':\n",
    "            y_train = self.training_dataframe[['DengRate_all', 'DengRate_019']]\n",
    "            y_val = self.validation_dataframe[['DengRate_all', 'DengRate_019']]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported target type\")\n",
    "\n",
    "        # Convert the dataframes to numpy arrays\n",
    "        x_train = x_train.values\n",
    "        x_val = x_val.values\n",
    "        y_train = y_train.values\n",
    "        y_val = y_val.values\n",
    "\n",
    "        return x_train, y_train, x_val, y_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RnNs2OTxIDSv",
   "metadata": {
    "id": "RnNs2OTxIDSv"
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "# PCA Reducer\n",
    "def pca_reducer(X, n_components):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce the dimensionality of the input data.\n",
    "\n",
    "    Args:\n",
    "    - X (numpy.ndarray): The feature data to be reduced.\n",
    "    - n_components (int): The number of components to retain.\n",
    "\n",
    "    Returns:\n",
    "    - (numpy.ndarray): The transformed data after PCA reduction.\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_reduced = pca.fit_transform(X)\n",
    "    return X_reduced\n",
    "\n",
    "\n",
    "# PLS Reducer\n",
    "def pls_reducer(X, y, n_components):\n",
    "    \"\"\"\n",
    "    Apply PLS (Partial Least Squares) to reduce the dimensionality of the input data.\n",
    "\n",
    "    Args:\n",
    "    - X (numpy.ndarray): The feature data to be reduced.\n",
    "    - y (numpy.ndarray): The target variable to be used in fitting the model.\n",
    "    - n_components (int): The number of components to retain.\n",
    "\n",
    "    Returns:\n",
    "    - (numpy.ndarray): The transformed data after PLS reduction.\n",
    "    \"\"\"\n",
    "    pls = PLSRegression(n_components=n_components)\n",
    "    pls.fit(X, y)  # Fit PLS model\n",
    "    X_reduced = pls.transform(X)\n",
    "    return X_reduced\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "xK5Frrx6tDda",
   "metadata": {
    "id": "xK5Frrx6tDda"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "# from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# from data_reduction import pca_reducer, pls_reducer\n",
    "# from datasetHandler import datasetHandataler\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Load and inspect data\n",
    "dataframe = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "print(\"Initial DataFrame shape:\", dataframe.shape)\n",
    "print(dataframe.head())\n",
    "print(dataframe.info())\n",
    "\n",
    "# Convert Date column to datetime format if necessary\n",
    "dataframe['Date'] = pd.to_datetime(dataframe['Date'], errors='coerce')\n",
    "\n",
    "# Drop unnecessary columns and handle any missing values\n",
    "dataframe.dropna(inplace=True)\n",
    "print(\"DataFrame shape after cleaning:\", dataframe.shape)\n",
    "\n",
    "# Define GROUPED_VARS according to your dataset\n",
    "dataframe['rate_total'] = dataframe['cases_total'] / dataframe['PopTotal_UF']\n",
    "dataframe['rate_019'] = dataframe['cases0_19'] / dataframe['Pop0_19_UF']\n",
    "\n",
    "GROUPED_VARS = {\n",
    "    'EXCLUDED':[\n",
    "        't_fundc_ocup18m',\n",
    "        't_medioc_ocup18m',\n",
    "        'PopTotal_Urban_UF',\n",
    "        'PopTotal_Rural_UF',\n",
    "        'total_precipitation_d',\n",
    "        'surface_pressure_d',\n",
    "        'area_km2',\n",
    "        'humidity_d',\n",
    "        'temperature_2m_d',\n",
    "        'min_temperature_2m_d',\n",
    "    ],\n",
    "\n",
    "    'CLIMATIC VARIABLES': [\n",
    "        'dewpoint_temperature_2m_d',\n",
    "        'max_temperature_2m_d',\n",
    "        'u_component_of_wind_10m_d',\n",
    "        'v_component_of_wind_10m_d'\n",
    "    ],\n",
    "\n",
    "    'GEO VARIABLES': [\n",
    "        'NDVI_d',\n",
    "        'max_elevation_d',\n",
    "        'mean_elevation_d',\n",
    "        'min_elevation_d',\n",
    "        'stdDev_elevation_d',\n",
    "        'variance_elevation_d',\n",
    "        'Forest_Cover_Percent',\n",
    "        'Urban_Cover_Percent'\n",
    "    ],\n",
    "\n",
    "    'SOCIO VARIABLES':[\n",
    "        'Urban_Cover_Percent',\n",
    "        'ivs',\n",
    "        'ivs_infraestrutura_urbana',\n",
    "        'ivs_capital_humano',\n",
    "        'ivs_renda_e_trabalho',\n",
    "        't_sem_agua_esgoto',\n",
    "        't_sem_lixo',\n",
    "        't_vulner_mais1h',\n",
    "        't_analf_15m',\n",
    "        't_cdom_fundin',\n",
    "        't_p15a24_nada',\n",
    "        't_vulner',\n",
    "        't_desocup18m',\n",
    "        't_p18m_fundin_informal',\n",
    "        'idhm',\n",
    "        'idhm_long',\n",
    "        'idhm_educ',\n",
    "        'idhm_renda',\n",
    "        'idhm_educ_sub_esc',\n",
    "        't_pop18m_fundc',\n",
    "        'idhm_educ_sub_freq',\n",
    "        'renda_per_capita',\n",
    "        'pea10a14',\n",
    "        'pea15a17',\n",
    "        'pea18m',\n",
    "        't_eletrica',\n",
    "        't_densidadem2',\n",
    "        'rdpc_def_vulner',\n",
    "        't_analf_18m',\n",
    "        't_formal_18m'\n",
    "    ],\n",
    "\n",
    "    'AUXILIAR':[\n",
    "        'Month',\n",
    "        'cases20_99',\n",
    "        'cases0_19'\n",
    "    ],\n",
    "\n",
    "    'DENGUE':[\n",
    "        'rate_total',\n",
    "        'rate_019'\n",
    "    ]\n",
    "}\n",
    "\n",
    "DATA_REDUCER_SETTINGS = {\n",
    "    'TYPE': 'PCA',\n",
    "    'NUMBER OF COMPONENTS': {\n",
    "        'CLIMATIC VARIABLES': 4,\n",
    "        'GEO VARIABLES': 6,\n",
    "        'SOCIO VARIABLES':10\n",
    "    }\n",
    "}\n",
    "\n",
    "# Extract feature groups\n",
    "X_climatic = dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values\n",
    "X_geo = dataframe[GROUPED_VARS['GEO VARIABLES']].values\n",
    "X_socio = dataframe[GROUPED_VARS['SOCIO VARIABLES']].values\n",
    "y_dengue = dataframe[GROUPED_VARS['DENGUE']].values\n",
    "\n",
    "# Normalize the target variable\n",
    "scaler = MinMaxScaler()\n",
    "y_dengue = scaler.fit_transform(y_dengue)\n",
    "\n",
    "\n",
    "\n",
    "if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':\n",
    "    climatic_vars_reduced = pls_reducer(X_climatic, y_dengue, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    geo_vars_reduced = pls_reducer(X_geo, y_dengue, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    socio_vars_reduced = pls_reducer(X_socio, y_dengue, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "else:\n",
    "    climatic_vars_reduced = pca_reducer(X_climatic, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES'])\n",
    "    geo_vars_reduced = pca_reducer(X_geo, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES'])\n",
    "    socio_vars_reduced = pca_reducer(X_socio, DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES'])\n",
    "\n",
    "# Combine features into a single DataFrame\n",
    "columns = {\n",
    "    'Year': dataframe.Year.values,\n",
    "    'dep_id': dataframe.CD_UF.values,\n",
    "    **{f'PCA{i}-Climatic': climatic_vars_reduced[:, i] for i in range(climatic_vars_reduced.shape[1])},\n",
    "    **{f'PCA{i}-Geo': geo_vars_reduced[:, i] for i in range(geo_vars_reduced.shape[1])},\n",
    "    **{f'PCA{i}-Socio': socio_vars_reduced[:, i] for i in range(socio_vars_reduced.shape[1])},\n",
    "    'DengRate_all': y_dengue[:, 0],\n",
    "    'DengRate_019': y_dengue[:, 1]\n",
    "}\n",
    "\n",
    "# Create the reduced dataframe\n",
    "reduced_dataframe = pd.DataFrame(columns)\n",
    "print(reduced_dataframe.head())\n",
    "\n",
    "reduced_dataframe.to_csv(\"reduced_for_tcn.csv\",index=False)\n",
    "\n",
    "\n",
    "\n",
    "# Split the data into training and validation sets\n",
    "training_dataframe = reduced_dataframe[reduced_dataframe.Year <= 2016]\n",
    "validation_dataframe = reduced_dataframe[reduced_dataframe.Year > 2016]\n",
    "\n",
    "# Dataset handling\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)\n",
    "x_train, y_train, x_val, y_val = dataset_handler.get_data('learning', 'prediction')\n",
    "print('\\nTraining Shapes:', x_train.shape, y_train.shape)\n",
    "print('Validation Shapes:', x_val.shape, y_val.shape)\n",
    "\n",
    "# Data Augmentation\n",
    "def augment_data(X, y, scaling_factor=0.1, noise_factor=0.01):\n",
    "    noise = np.random.normal(loc=0.0, scale=noise_factor, size=X.shape)\n",
    "    augmented_X = X + noise\n",
    "    augmented_X *= (1 + scaling_factor * np.random.randn(*X.shape))\n",
    "    return augmented_X, y\n",
    "\n",
    "x_train_aug, y_train_aug = augment_data(x_train, y_train)\n",
    "print(\"Augmented Training Data Shape:\", x_train_aug.shape)\n",
    "\n",
    "# Reshape for TCN model\n",
    "def reshape_to_tcn_format(X, timesteps=12):\n",
    "    return X.reshape(X.shape[0] // timesteps, timesteps, X.shape[1])\n",
    "\n",
    "timesteps = 12\n",
    "x_train_tcn = reshape_to_tcn_format(x_train_aug, timesteps)\n",
    "x_val_tcn = reshape_to_tcn_format(x_val, timesteps)\n",
    "\n",
    "# Normalizing data\n",
    "scaler = MinMaxScaler()\n",
    "x_train_tcn = scaler.fit_transform(x_train_tcn.reshape(-1, x_train_tcn.shape[-1])).reshape(x_train_tcn.shape)\n",
    "x_val_tcn = scaler.transform(x_val_tcn.reshape(-1, x_val_tcn.shape[-1])).reshape(x_val_tcn.shape)\n",
    "\n",
    "# Reshape y_train_aug for TCN model based on timesteps\n",
    "y_train_tcn = y_train_aug[:len(x_train_tcn) * timesteps].reshape(-1, timesteps, y_train_aug.shape[1])[:len(x_train_tcn)]\n",
    "y_val_tcn = y_val[:len(x_val_tcn) * timesteps].reshape(-1, timesteps, y_val.shape[1])[:len(x_val_tcn)]\n",
    "\n",
    "print(f\"x_train_tcn shape: {x_train_tcn.shape}\")\n",
    "print(f\"y_train_tcn shape: {y_train_tcn.shape}\")\n",
    "print(f\"x_val_tcn shape: {x_val_tcn.shape}\")\n",
    "print(f\"y_val_tcn shape: {y_val_tcn.shape}\")\n",
    "\n",
    "# Modeling using TCN\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten\n",
    "\n",
    "def build_tcn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=2, activation='relu', input_shape=input_shape),\n",
    "        Conv1D(32, kernel_size=2, activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2) # Output for Dengue rates\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "tcn_model = build_tcn_model((timesteps, x_train_tcn.shape[-1]))\n",
    "tcn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "qFfoEwVf0bUv",
   "metadata": {
    "id": "qFfoEwVf0bUv"
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(config['output'], \"Brazil\")\n",
    "\n",
    "TRAINING = True  # Set to False to skip training and load the model if it exists\n",
    "\n",
    "# Check if model exists or train it\n",
    "if TRAINING:\n",
    "    # Train the model\n",
    "    history = tcn_model.fit(x_train_tcn, y_train_tcn, epochs=50, batch_size=32, validation_data=(x_val_tcn, y_val))\n",
    "\n",
    "    # Save the trained model\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    model_save_path = os.path.join(output_path, f'TCN_model_new1_{today}.h5')\n",
    "    model.save(model_save_path)\n",
    "\n",
    "    # Evaluate the model\n",
    "    results = tcn_model.evaluate(x_val_tcn, y_val)\n",
    "    print(f\"\\nEvaluation Results - Loss: {results[0]}, MAE: {results[1]}\")\n",
    "\n",
    "    # Plot the training history\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.show()\n",
    "else:\n",
    "    # Check if a saved model exists\n",
    "    saved_models = glob(os.path.join(output_path, \"TCN_model_new1_*.h5\"))\n",
    "    if saved_models:\n",
    "        # Load the most recent model\n",
    "        latest_model_path = max(saved_models, key=os.path.getctime)\n",
    "        model = load_model(latest_model_path)\n",
    "        # model.compile(optimizer='adam', loss='mse')\n",
    "    else:\n",
    "        print('No pre-trained model found. Run TRAINING = True to train and save the model.')\n",
    "        exit()\n",
    "    # Make predictions\n",
    "    y_val_indices=y_val.index.to_list()\n",
    "    y_val_indices_df = pd.DataFrame(y_val_indices,columns = ['actual_index'])\n",
    "\n",
    "    y_train_indices=y_train.index.to_list()\n",
    "    y_train_indices_df = pd.DataFrame(y_train_indices,columns = ['actual_index'])\n",
    "\n",
    "\n",
    "    y_train_pred = model.predict(X_train_tcn)\n",
    "    y_val_pred = model.predict(X_val_tcn)\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        # Filter rows corresponding to the current department for both training and validation\n",
    "        department_rows_val = val_data[val_data['dep_id'] - 1 == department_idx]\n",
    "        department_rows_train = train_data[train_data['dep_id'] - 1 == department_idx]  # Assuming train_data exists\n",
    "\n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "        # Matching indices for validation data\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        if matching_indices_val.empty:\n",
    "            # If no matching indices found for validation data, skip to the next department\n",
    "            continue\n",
    "\n",
    "        # Matching indices for training data (if applicable)\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "        if matching_indices_train.empty:\n",
    "            # If no matching indices found for training data, skip to the next department\n",
    "            continue\n",
    "\n",
    "        # Extract true and predicted values for validation data\n",
    "        true_dengrate_all_val = department_rows_val['DengRate_all'].values\n",
    "        true_dengrate_019_val = department_rows_val['DengRate_019'].values\n",
    "        predicted_dengrate_all_val = y_val_pred[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = y_val_pred[matching_indices_val, 1]\n",
    "\n",
    "        # Extract true and predicted values for training data\n",
    "        true_dengrate_all_train = department_rows_train['DengRate_all'].values\n",
    "        true_dengrate_019_train = department_rows_train['DengRate_019'].values\n",
    "        predicted_dengrate_all_train = y_train_pred[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = y_train_pred[matching_indices_train, 1]\n",
    "\n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)\n",
    "        nrmse_dengrate_all_val = np.sqrt(np.mean((true_dengrate_all_val - predicted_dengrate_all_val) ** 2)) / np.std(true_dengrate_all_val)\n",
    "        nrmse_dengrate_019_val = np.sqrt(np.mean((true_dengrate_019_val - predicted_dengrate_019_val) ** 2)) / np.std(true_dengrate_019_val)\n",
    "\n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)\n",
    "        nrmse_dengrate_all_train = np.sqrt(np.mean((true_dengrate_all_train - predicted_dengrate_all_train) ** 2)) / np.std(true_dengrate_all_train)\n",
    "        nrmse_dengrate_019_train = np.sqrt(np.mean((true_dengrate_019_train - predicted_dengrate_019_train) ** 2)) / np.std(true_dengrate_019_train)\n",
    "\n",
    "        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)\n",
    "        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "\n",
    "        # # Calculate MAE for both DengRate_all and DengRate_019 (training data)\n",
    "        # mae_dengrate_all_train = mean_absolute_error(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "        # mae_dengrate_019_train = mean_absolute_error(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Store the results for both training and validation data\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'NRMSE 0-19 Training': nrmse_dengrate_019_train,\n",
    "            'NRMSE All Training': nrmse_dengrate_all_train,\n",
    "            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,\n",
    "            'NRMSE All Validation': nrmse_dengrate_all_val,\n",
    "            # 'MAE (DengRate_all) Training': mae_dengrate_all_train,\n",
    "            # 'MAE (DengRate_019) Training': mae_dengrate_019_train,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val\n",
    "        })\n",
    "\n",
    "\n",
    "    # print(results)\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Save to CSV\n",
    "    results_df.to_csv('model_predictions_metrics.csv', index=False)\n",
    "\n",
    "    #Save results\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_path = os.path.join(config['metrics'], \"Brazil\", f'TCN-new1-{today}.csv')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf8967d-6ea6-443d-9a5a-4ce70d90a4b0",
   "metadata": {},
   "source": [
    "## TCn imple. from 0->no reduction or augmentation just scaling and appying=> max errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "if7dM36BuXMj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "if7dM36BuXMj",
    "outputId": "7b2e123b-98a5-463b-e37e-4bb8b9abd780"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/GAUTAMMANU/projminor.git\n",
    "%cd projminor\n",
    "%cd code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lG8dlGfwqBHX",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lG8dlGfwqBHX",
    "outputId": "7256bdf8-92dc-49a8-9b15-22c179b79c89"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "# os.chdir(join('..','data'))\n",
    "os.getcwd()\n",
    "\n",
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))\n",
    "\n",
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "project_path\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ulQrtzOxqC1W",
   "metadata": {
    "id": "ulQrtzOxqC1W"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, Dropout, BatchNormalization\n",
    "import matplotlib.pyplot as plt\n",
    "from glob import glob\n",
    "from config import DEP_NAMES\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gu74BbQQqS1p",
   "metadata": {
    "id": "gu74BbQQqS1p"
   },
   "outputs": [],
   "source": [
    "# Dataset Handler Class\n",
    "class datasetHandler:\n",
    "    def __init__(self, training_dataframe, validation_dataframe):\n",
    "        \"\"\"\n",
    "        Initialize the dataset handler with training and validation dataframes.\n",
    "        \"\"\"\n",
    "        self.training_dataframe = training_dataframe\n",
    "        self.validation_dataframe = validation_dataframe\n",
    "\n",
    "    def get_data(self, feature_type='learning', target_type='prediction'):\n",
    "        \"\"\"\n",
    "        Retrieve the features and target variables for learning and prediction.\n",
    "        \"\"\"\n",
    "        if feature_type == 'learning':\n",
    "            x_train = self.training_dataframe.drop(columns=['Year', 'DengRate_all', 'DengRate_019'])\n",
    "            x_val = self.validation_dataframe.drop(columns=['Year', 'DengRate_all', 'DengRate_019'])\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported feature type\")\n",
    "\n",
    "        if target_type == 'prediction':\n",
    "            y_train = self.training_dataframe[['DengRate_all', 'DengRate_019']]\n",
    "            y_val = self.validation_dataframe[['DengRate_all', 'DengRate_019']]\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported target type\")\n",
    "\n",
    "        return x_train.values, y_train.values, x_val.values, y_val.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j7KAvL6bqVVo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "j7KAvL6bqVVo",
    "outputId": "7d7a71e6-6652-4dcc-dc78-ae65ff4731db"
   },
   "outputs": [],
   "source": [
    "# Load and clean data\n",
    "dataframe = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "# dataframe['Date'] = pd.to_datetime(dataframe['Date'], errors='coerce')\n",
    "dataframe.dropna(inplace=True)\n",
    "dataframe['DengRate_all'] = dataframe['cases_total'] / dataframe['PopTotal_UF']\n",
    "dataframe['DengRate_019'] = dataframe['cases0_19'] / dataframe['Pop0_19_UF']\n",
    "\n",
    "\n",
    "# Data Splitting\n",
    "training_dataframe = dataframe[dataframe.Year <= 2016]\n",
    "validation_dataframe = dataframe[dataframe.Year > 2016]\n",
    "\n",
    "training_dataframe = training_dataframe.drop(columns=['Date'])\n",
    "validation_dataframe = validation_dataframe.drop(columns=['Date'])\n",
    "\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)\n",
    "x_train, y_train, x_val, y_val = dataset_handler.get_data('learning', 'prediction')\n",
    "\n",
    "\n",
    "# Scaling\n",
    "scaler_X = MinMaxScaler()\n",
    "scaler_y = MinMaxScaler()\n",
    "x_train = scaler_X.fit_transform(x_train)\n",
    "x_val = scaler_X.transform(x_val)\n",
    "y_train = scaler_y.fit_transform(y_train)\n",
    "y_val = scaler_y.transform(y_val)\n",
    "\n",
    "print(x_train.shape,\"\\n\",x_val.shape,\"\\n\",y_train.shape,\"\\n\",y_val.shape,\"\\n\",)\n",
    "# print(x_train,\"\\n\",x_val,\"\\n\",y_train,\"\\n\",y_val)\n",
    "\n",
    "# Reshape for TCN\n",
    "timesteps = 12\n",
    "\n",
    "\n",
    "# Function to reshape data into sequences for TCN\n",
    "def reshape_for_tcn(x_data, y_data, time_steps=12):\n",
    "    num_samples = x_data.shape[0] - time_steps + 1\n",
    "    features = x_data.shape[1]\n",
    "\n",
    "    # Generate sequences using a sliding window approach\n",
    "    x_tcn = np.array([x_data[i:i + time_steps] for i in range(num_samples)])\n",
    "    y_tcn = y_data[time_steps - 1:]  # Align targets to the end of each sequence\n",
    "\n",
    "    return x_tcn, y_tcn\n",
    "\n",
    "# Reshape training data\n",
    "x_train_tcn, y_train_tcn = reshape_for_tcn(x_train, y_train, time_steps=12)\n",
    "x_val_tcn, y_val_tcn = reshape_for_tcn(x_val, y_val, time_steps=12)\n",
    "\n",
    "# Check the shapes\n",
    "print(f\"x_train_tcn shape: {x_train_tcn.shape}\")\n",
    "print(f\"y_train_tcn shape: {y_train_tcn.shape}\")\n",
    "print(f\"x_val_tcn shape: {x_val_tcn},\")\n",
    "print(f\"y_val_tcn shape: {y_val_tcn}\")\n",
    "\n",
    "\n",
    "# Creating DataFrames of indices for tracking without reshaping\n",
    "y_train_indices_df = pd.DataFrame(y_train_tcn, columns=['DengRate_all', 'DengRate_019'])\n",
    "y_val_indices_df = pd.DataFrame(y_val_tcn, columns=['DengRate_all', 'DengRate_019'])\n",
    "\n",
    "# Check the created DataFrames\n",
    "print(\"y_train_indices_df (Head):\\n\", y_train_indices_df)\n",
    "print(\"y_val_indices_df (Head):\\n\", y_val_indices_df)\n",
    "\n",
    "# Build TCN Model\n",
    "def build_tcn_model(input_shape):\n",
    "    model = Sequential([\n",
    "        Conv1D(64, kernel_size=2, activation='relu', input_shape=input_shape),\n",
    "        BatchNormalization(),\n",
    "        Conv1D(32, kernel_size=2, activation='relu'),\n",
    "        BatchNormalization(),\n",
    "        Dropout(0.3),\n",
    "        Flatten(),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(2)  # Predicting two targets (DengRate_all, DengRate_019)\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Updated input shape for TCN\n",
    "input_shape = (timesteps, x_train_tcn.shape[2])  # (12, 60)\n",
    "tcn_model = build_tcn_model(input_shape)\n",
    "tcn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "QXN7Awggq5GO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QXN7Awggq5GO",
    "outputId": "3156f274-11c9-4bfd-82b0-51d81de948c4"
   },
   "outputs": [],
   "source": [
    "output_path = os.path.join(config['output'], \"Brazil\")\n",
    "\n",
    "TRAINING = False  # Set to False to skip training and load the model if it exists\n",
    "\n",
    "if TRAINING:\n",
    "    # Use reshaped TCN data for training and validation\n",
    "    history = tcn_model.fit(\n",
    "        x_train_tcn, y_train_tcn,\n",
    "        epochs=50,\n",
    "        batch_size=32,\n",
    "        validation_data=(x_val_tcn, y_val_tcn),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # # Save the model with a timestamped filename\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    model_save_path = os.path.join(output_path, f'TCN_model_new_{today}.h5')\n",
    "    tcn_model.save(model_save_path)\n",
    "    print(f\"Model saved to: {model_save_path}\")\n",
    "\n",
    "    # Evaluate the model\n",
    "    val_loss, val_mae = tcn_model.evaluate(x_val_tcn, y_val_tcn, verbose=1)\n",
    "    print(f\"Validation Loss: {val_loss:.4f}, Validation MAE: {val_mae:.4f}\")\n",
    "\n",
    "    # Plot the training history\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    plt.plot(history.history['loss'], label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Model Loss Curve')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "else:\n",
    "    saved_models = glob(os.path.join(output_path, \"TCN_model_new_*.h5\"))\n",
    "    if saved_models:\n",
    "        latest_model_path = max(saved_models, key=os.path.getctime)\n",
    "        tcn_model = load_model(latest_model_path, custom_objects={'mse': 'mean_squared_error'})\n",
    "        print(f\"Loaded model from: {latest_model_path}\")\n",
    "\n",
    "    else:\n",
    "        print(\"No pre-trained model found. Set TRAINING=True to train the model.\")\n",
    "        exit()\n",
    "\n",
    "    # Generate Predictions\n",
    "    y_train_pred = tcn_model.predict(x_train_tcn)\n",
    "    y_val_pred = tcn_model.predict(x_val_tcn)\n",
    "    \n",
    "    validation_dataframe_indices=validation_dataframe.index.to_list()\n",
    "    validation_dataframe_indices_df = pd.DataFrame(validation_dataframe_indices,columns = ['actual_index'])\n",
    "\n",
    "    training_dataframe_indices=training_dataframe.index.to_list()\n",
    "    training_dataframe_indices_df = pd.DataFrame(training_dataframe_indices,columns = ['actual_index'])\n",
    "\n",
    "    results = []\n",
    "    # c=0\n",
    "    # Loop over each department using DEP_NAMES dictionary\n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        # Filter department-specific data\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['CD_UF'] - 1 == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['CD_UF'] - 1 == department_idx]\n",
    "\n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "        # print(\"dep_indices_val\",department_indices_val,\"\\n\",\"dep_indices_train\",department_indices_train)\n",
    "        \n",
    "                # Matching indices for validation data\n",
    "        matching_indices_val = validation_dataframe_indices_df[validation_dataframe_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        matching_indices_train = training_dataframe_indices_df[training_dataframe_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "\n",
    "        if matching_indices_train.empty or matching_indices_val.empty:\n",
    "            # If no matching indices found for training data, skip to the next department\n",
    "            continue\n",
    "\n",
    "\n",
    "        # Extract true and predicted values for validation data\n",
    "        true_dengrate_all_val = department_rows_val['DengRate_all'].values\n",
    "        true_dengrate_019_val = department_rows_val['DengRate_019'].values\n",
    "        predicted_dengrate_all_val = y_val_pred[matching_indices_val, 0]  # Adjusted indexing\n",
    "        predicted_dengrate_019_val = y_val_pred[matching_indices_val, 1]  # Adjusted indexing\n",
    "\n",
    "        # Extract true and predicted values for training data\n",
    "        true_dengrate_all_train = department_rows_train.loc[matching_indices_train, 'DengRate_all'].values\n",
    "        true_dengrate_019_train = department_rows_train.loc[matching_indices_train, 'DengRate_019'].values\n",
    "\n",
    "        predicted_dengrate_all_train = y_train_pred[matching_indices_train, 0]  # Adjusted indexing\n",
    "        predicted_dengrate_019_train = y_train_pred[matching_indices_train, 1]  # Adjusted indexing\n",
    "\n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (training data)\n",
    "        nrmse_dengrate_all_train = np.sqrt(mean_squared_error(true_dengrate_all_train, predicted_dengrate_all_train)) / np.std(true_dengrate_all_train)\n",
    "        nrmse_dengrate_019_train = np.sqrt(mean_squared_error(true_dengrate_019_train, predicted_dengrate_019_train)) / np.std(true_dengrate_019_train)\n",
    "\n",
    "        # Calculate NRMSE for both DengRate_all and DengRate_019 (validation data)\n",
    "        nrmse_dengrate_all_val = np.sqrt(mean_squared_error(true_dengrate_all_val, predicted_dengrate_all_val)) / np.std(true_dengrate_all_val)\n",
    "        nrmse_dengrate_019_val = np.sqrt(mean_squared_error(true_dengrate_019_val, predicted_dengrate_019_val)) / np.std(true_dengrate_019_val)\n",
    "\n",
    "        # Calculate MAE for both DengRate_all and DengRate_019 (validation data)\n",
    "        mae_dengrate_all_val = mean_absolute_error(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_019_val = mean_absolute_error(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "\n",
    "        # Store the results for each department\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'NRMSE 0-19 Training': nrmse_dengrate_019_train,\n",
    "            'NRMSE All Training': nrmse_dengrate_all_train,\n",
    "            'NRMSE 0-19 Validation': nrmse_dengrate_019_val,\n",
    "            'NRMSE All Validation': nrmse_dengrate_all_val,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val\n",
    "        })\n",
    "        \n",
    "    \n",
    "\n",
    "    # Convert results to DataFrame and save to CSV\n",
    "    results_df = pd.DataFrame(results)\n",
    "    # Save with timestamp for record-keeping\n",
    "    today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "    output_path = os.path.join(config['metrics'], \"Brazil\", f'TCN-new-{today}.csv')\n",
    "\n",
    "    # Create the directory if it doesn't exist\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    results_df.to_csv(output_path, index=False)\n",
    "    print(f\"Results saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2Cgkzftmtx6",
   "metadata": {
    "id": "b2Cgkzftmtx6"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "7audVqxFBDWh",
    "717e2eae-97ff-475f-bfd5-78e8dcee981d",
    "52ae33ce-0228-4303-a796-ce8a7c50869a",
    "462125c4-fbb2-4fe7-8cfd-3e023fd5356c",
    "c80-UoybcjRp",
    "6a956d2f-f784-4d7e-953d-dfac8c36b3ba",
    "E3iGL37YB9FU",
    "nhxwEojupvjg",
    "0y2iVphNi2Bo",
    "9o0QM6JIGJaC",
    "XZ1ow2z-GAZx"
   ],
   "gpuType": "T4",
   "include_colab_link": true,
   "name": "AI.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
