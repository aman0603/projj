{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b52326e0-83dc-49ba-a1de-5788c7318306",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\amanp\\\\Desktop\\\\MINOR\\\\projj\\\\code'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %cd code\n",
    "# insert your desired path to work on\n",
    "import os\n",
    "from os.path import join\n",
    "project_path = os.path.dirname(os.getcwd())\n",
    "# os.chdir(join('..','data'))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6293dcbe-a629-4e2a-a4e6-12081ec665c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(join(project_path, 'code'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76c817f3-abaf-494c-893b-c66841c8ff1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac8a7f6-480b-4469-997a-14196073b392",
   "metadata": {},
   "source": [
    "**Plots settings.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52add8ec-bb87-4f7e-911f-6a86be107862",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "font = {'family':'Arial', 'size':'15', 'weight':'normal'}\n",
    "\n",
    "matplotlib.rc('font', **font)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4661ff7-f369-46ff-9abb-089dbe726433",
   "metadata": {},
   "source": [
    "**Set folder structure.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fb1561-f821-4ed2-bc32-4c271c6c5034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {\n",
    "    'main_brazil': 'Brazil',\n",
    "    'main_peru': 'Peru',\n",
    "    'baseline': join(project_path, \"baseline_models\"),\n",
    "    'output': join(project_path, \"code\", \"saved_models\"),\n",
    "    'metrics': join(project_path, \"code\", \"metrics\")\n",
    "}\n",
    "project_path\n",
    "\n",
    "# List comprehension for the folder structure code\n",
    "[os.makedirs(val, exist_ok=True) for key, val in config.items()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb5362-08e9-4fa4-8cca-b54e5519f8ee",
   "metadata": {},
   "source": [
    "# **AI4Dengue forecasting**\n",
    "![](https://drive.google.com/uc?export=view&id=1J5Bt5Cks-e2IV-dEJLHJkuwXFJNFAZgr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "24c1fd94-7db9-42c8-a93e-bd6f6432093c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as st\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from config import DEP_NAMES, GROUPED_VARS, DATA_REDUCER_SETTINGS, DATA_PROCESSING_SETTINGS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e175f6aa-b1a5-48d5-ad65-a25f07e816c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'clean',\n",
       " 'cx',\n",
       " 'geopandas',\n",
       " 'pd',\n",
       " 'plist',\n",
       " 'plotShape']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(utils)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ddab25-7d16-40c9-b934-faa1976dae05",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09e878c4-9686-4dba-902b-6a183318240d",
   "metadata": {},
   "source": [
    "## Load the dataframe\n",
    "**This dataframe comprises all the variables (climatic, epidemiological etc.) acquired for each Department during a defined number of years.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83fe6ace-f003-4f68-bb1f-d5d52aef1277",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Date                2004-02-01\n",
       "Year                      2004\n",
       "Month                        2\n",
       "CD_UF                       12\n",
       "area_km2            164173.431\n",
       "                       ...    \n",
       "rdpc_def_vulner         119.68\n",
       "t_analf_18m              17.79\n",
       "t_formal_18m             46.45\n",
       "t_fundc_ocup18m           55.2\n",
       "t_medioc_ocup18m         39.61\n",
       "Name: 1000, Length: 62, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.read_csv(join('dataset', \"Brazil_UF_dengue_monthly.csv\"))\n",
    "dataframe.head()\n",
    "dataframe.iloc[1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd677311-bae6-4034-bea4-2069fd2ccd2e",
   "metadata": {},
   "source": [
    "**Load CNN results as columns to dataframe.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b10adcfc-20a5-41fd-9c54-886756156c82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>53</td>\n",
       "      <td>29.022461</td>\n",
       "      <td>16.795465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>53</td>\n",
       "      <td>20.277210</td>\n",
       "      <td>5.658783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>53</td>\n",
       "      <td>7.219064</td>\n",
       "      <td>16.862005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>53</td>\n",
       "      <td>17.866333</td>\n",
       "      <td>28.619926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>53</td>\n",
       "      <td>13.846931</td>\n",
       "      <td>8.825871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      CD_UF    CNN_all   CNN_0-19\n",
       "0        11   1.000000   1.000000\n",
       "1        11   1.000000   1.000000\n",
       "2        11   1.000000   1.000000\n",
       "3        11   1.000000   1.000000\n",
       "4        11  32.159859  17.186546\n",
       "...     ...        ...        ...\n",
       "6151     53  29.022461  16.795465\n",
       "6152     53  20.277210   5.658783\n",
       "6153     53   7.219064  16.862005\n",
       "6154     53  17.866333  28.619926\n",
       "6155     53  13.846931   8.825871\n",
       "\n",
       "[6156 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn = pd.read_csv(join('saved_models', \"cnn_dataframe.csv\")).drop('Unnamed: 0', axis=1)\n",
    "cnn['CD_UF'] = cnn['CD_UF'].astype(np.int64)\n",
    "\n",
    "assert dataframe.shape[0] == cnn.shape[0]\n",
    "assert all(dataframe['CD_UF'].unique() == cnn['CD_UF'].unique())\n",
    "cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "676d3a20-91fa-4dcb-8327-38546b4d3a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>pea10a14</th>\n",
       "      <th>pea15a17</th>\n",
       "      <th>pea18m</th>\n",
       "      <th>t_eletrica</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>18698</td>\n",
       "      <td>34904</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.362744</td>\n",
       "      <td>282.150351</td>\n",
       "      <td>42.202163</td>\n",
       "      <td>304.210083</td>\n",
       "      <td>287.271135</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.317748</td>\n",
       "      <td>281.820936</td>\n",
       "      <td>34.023500</td>\n",
       "      <td>307.566780</td>\n",
       "      <td>290.719267</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>286.196146</td>\n",
       "      <td>45.486547</td>\n",
       "      <td>307.716003</td>\n",
       "      <td>291.720099</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.235493</td>\n",
       "      <td>290.445969</td>\n",
       "      <td>64.916154</td>\n",
       "      <td>306.706715</td>\n",
       "      <td>291.496597</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.314533</td>\n",
       "      <td>291.019027</td>\n",
       "      <td>69.799133</td>\n",
       "      <td>303.716225</td>\n",
       "      <td>291.647991</td>\n",
       "      <td>...</td>\n",
       "      <td>10706</td>\n",
       "      <td>36652</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 62 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0     2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1     2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2     2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3     2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4     2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "...          ...   ...    ...    ...         ...       ...   \n",
       "6151  2019-08-01  2019      8     53    5760.784  0.362744   \n",
       "6152  2019-09-01  2019      9     53    5760.784  0.317748   \n",
       "6153  2019-10-01  2019     10     53    5760.784  0.271795   \n",
       "6154  2019-11-01  2019     11     53    5760.784  0.235493   \n",
       "6155  2019-12-01  2019     12     53    5760.784  0.314533   \n",
       "\n",
       "      dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                    295.674980   88.460308            303.987216   \n",
       "1                    295.944060   88.856948            304.738755   \n",
       "2                    296.092747   89.305463            304.620829   \n",
       "3                    296.186143   88.590375            304.168669   \n",
       "4                    295.562972   86.939606            303.903043   \n",
       "...                         ...         ...                   ...   \n",
       "6151                 282.150351   42.202163            304.210083   \n",
       "6152                 281.820936   34.023500            307.566780   \n",
       "6153                 286.196146   45.486547            307.716003   \n",
       "6154                 290.445969   64.916154            306.706715   \n",
       "6155                 291.019027   69.799133            303.716225   \n",
       "\n",
       "      min_temperature_2m_d  ...  pea10a14  pea15a17   pea18m  t_eletrica  \\\n",
       "0               294.155015  ...     18698     34904   723839       97.26   \n",
       "1               294.332566  ...     18698     34904   723839       97.26   \n",
       "2               294.304126  ...     18698     34904   723839       97.26   \n",
       "3               293.921815  ...     18698     34904   723839       97.26   \n",
       "4               293.395959  ...     18698     34904   723839       97.26   \n",
       "...                    ...  ...       ...       ...      ...         ...   \n",
       "6151            287.271135  ...     10706     36652  1361053       99.91   \n",
       "6152            290.719267  ...     10706     36652  1361053       99.91   \n",
       "6153            291.720099  ...     10706     36652  1361053       99.91   \n",
       "6154            291.496597  ...     10706     36652  1361053       99.91   \n",
       "6155            291.647991  ...     10706     36652  1361053       99.91   \n",
       "\n",
       "      t_densidadem2  rdpc_def_vulner  t_analf_18m  t_formal_18m  \\\n",
       "0             27.15           144.93         9.42         51.72   \n",
       "1             27.15           144.93         9.42         51.72   \n",
       "2             27.15           144.93         9.42         51.72   \n",
       "3             27.15           144.93         9.42         51.72   \n",
       "4             27.15           144.93         9.42         51.72   \n",
       "...             ...              ...          ...           ...   \n",
       "6151          23.48           171.62         3.66         71.62   \n",
       "6152          23.48           171.62         3.66         71.62   \n",
       "6153          23.48           171.62         3.66         71.62   \n",
       "6154          23.48           171.62         3.66         71.62   \n",
       "6155          23.48           171.62         3.66         71.62   \n",
       "\n",
       "      t_fundc_ocup18m  t_medioc_ocup18m  \n",
       "0               53.83             36.93  \n",
       "1               53.83             36.93  \n",
       "2               53.83             36.93  \n",
       "3               53.83             36.93  \n",
       "4               53.83             36.93  \n",
       "...               ...               ...  \n",
       "6151            76.39             61.00  \n",
       "6152            76.39             61.00  \n",
       "6153            76.39             61.00  \n",
       "6154            76.39             61.00  \n",
       "6155            76.39             61.00  \n",
       "\n",
       "[6156 rows x 62 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe.sort_values(['CD_UF', 'Date'], inplace=True, ignore_index=True)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6c99628a-3178-41db-8212-afcf4a8d43fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>pea18m</th>\n",
       "      <th>t_eletrica</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>723839</td>\n",
       "      <td>97.26</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6151</th>\n",
       "      <td>2019-08-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>8</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.362744</td>\n",
       "      <td>282.150351</td>\n",
       "      <td>42.202163</td>\n",
       "      <td>304.210083</td>\n",
       "      <td>287.271135</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>29.022461</td>\n",
       "      <td>16.795465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6152</th>\n",
       "      <td>2019-09-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>9</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.317748</td>\n",
       "      <td>281.820936</td>\n",
       "      <td>34.023500</td>\n",
       "      <td>307.566780</td>\n",
       "      <td>290.719267</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>20.277210</td>\n",
       "      <td>5.658783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6153</th>\n",
       "      <td>2019-10-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>10</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.271795</td>\n",
       "      <td>286.196146</td>\n",
       "      <td>45.486547</td>\n",
       "      <td>307.716003</td>\n",
       "      <td>291.720099</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>7.219064</td>\n",
       "      <td>16.862005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6154</th>\n",
       "      <td>2019-11-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.235493</td>\n",
       "      <td>290.445969</td>\n",
       "      <td>64.916154</td>\n",
       "      <td>306.706715</td>\n",
       "      <td>291.496597</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>17.866333</td>\n",
       "      <td>28.619926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6155</th>\n",
       "      <td>2019-12-01</td>\n",
       "      <td>2019</td>\n",
       "      <td>12</td>\n",
       "      <td>53</td>\n",
       "      <td>5760.784</td>\n",
       "      <td>0.314533</td>\n",
       "      <td>291.019027</td>\n",
       "      <td>69.799133</td>\n",
       "      <td>303.716225</td>\n",
       "      <td>291.647991</td>\n",
       "      <td>...</td>\n",
       "      <td>1361053</td>\n",
       "      <td>99.91</td>\n",
       "      <td>23.48</td>\n",
       "      <td>171.62</td>\n",
       "      <td>3.66</td>\n",
       "      <td>71.62</td>\n",
       "      <td>76.39</td>\n",
       "      <td>61.00</td>\n",
       "      <td>13.846931</td>\n",
       "      <td>8.825871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6156 rows × 64 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0     2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1     2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2     2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3     2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4     2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "...          ...   ...    ...    ...         ...       ...   \n",
       "6151  2019-08-01  2019      8     53    5760.784  0.362744   \n",
       "6152  2019-09-01  2019      9     53    5760.784  0.317748   \n",
       "6153  2019-10-01  2019     10     53    5760.784  0.271795   \n",
       "6154  2019-11-01  2019     11     53    5760.784  0.235493   \n",
       "6155  2019-12-01  2019     12     53    5760.784  0.314533   \n",
       "\n",
       "      dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                    295.674980   88.460308            303.987216   \n",
       "1                    295.944060   88.856948            304.738755   \n",
       "2                    296.092747   89.305463            304.620829   \n",
       "3                    296.186143   88.590375            304.168669   \n",
       "4                    295.562972   86.939606            303.903043   \n",
       "...                         ...         ...                   ...   \n",
       "6151                 282.150351   42.202163            304.210083   \n",
       "6152                 281.820936   34.023500            307.566780   \n",
       "6153                 286.196146   45.486547            307.716003   \n",
       "6154                 290.445969   64.916154            306.706715   \n",
       "6155                 291.019027   69.799133            303.716225   \n",
       "\n",
       "      min_temperature_2m_d  ...   pea18m  t_eletrica  t_densidadem2  \\\n",
       "0               294.155015  ...   723839       97.26          27.15   \n",
       "1               294.332566  ...   723839       97.26          27.15   \n",
       "2               294.304126  ...   723839       97.26          27.15   \n",
       "3               293.921815  ...   723839       97.26          27.15   \n",
       "4               293.395959  ...   723839       97.26          27.15   \n",
       "...                    ...  ...      ...         ...            ...   \n",
       "6151            287.271135  ...  1361053       99.91          23.48   \n",
       "6152            290.719267  ...  1361053       99.91          23.48   \n",
       "6153            291.720099  ...  1361053       99.91          23.48   \n",
       "6154            291.496597  ...  1361053       99.91          23.48   \n",
       "6155            291.647991  ...  1361053       99.91          23.48   \n",
       "\n",
       "      rdpc_def_vulner  t_analf_18m  t_formal_18m  t_fundc_ocup18m  \\\n",
       "0              144.93         9.42         51.72            53.83   \n",
       "1              144.93         9.42         51.72            53.83   \n",
       "2              144.93         9.42         51.72            53.83   \n",
       "3              144.93         9.42         51.72            53.83   \n",
       "4              144.93         9.42         51.72            53.83   \n",
       "...               ...          ...           ...              ...   \n",
       "6151           171.62         3.66         71.62            76.39   \n",
       "6152           171.62         3.66         71.62            76.39   \n",
       "6153           171.62         3.66         71.62            76.39   \n",
       "6154           171.62         3.66         71.62            76.39   \n",
       "6155           171.62         3.66         71.62            76.39   \n",
       "\n",
       "      t_medioc_ocup18m    CNN_all   CNN_0-19  \n",
       "0                36.93   1.000000   1.000000  \n",
       "1                36.93   1.000000   1.000000  \n",
       "2                36.93   1.000000   1.000000  \n",
       "3                36.93   1.000000   1.000000  \n",
       "4                36.93  32.159859  17.186546  \n",
       "...                ...        ...        ...  \n",
       "6151             61.00  29.022461  16.795465  \n",
       "6152             61.00  20.277210   5.658783  \n",
       "6153             61.00   7.219064  16.862005  \n",
       "6154             61.00  17.866333  28.619926  \n",
       "6155             61.00  13.846931   8.825871  \n",
       "\n",
       "[6156 rows x 64 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = pd.concat([dataframe, cnn[['CNN_all', 'CNN_0-19']]], axis=1)\n",
    "dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f020c1-86d2-486e-ad0e-ad907a4a44f0",
   "metadata": {},
   "source": [
    "**'Clean' the dataset (e.g. remove NaN values)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fe4cdf83-78de-45d8-b733-dd93e6c483a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning dataframe...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Year</th>\n",
       "      <th>Month</th>\n",
       "      <th>CD_UF</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>NDVI_d</th>\n",
       "      <th>dewpoint_temperature_2m_d</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>max_temperature_2m_d</th>\n",
       "      <th>min_temperature_2m_d</th>\n",
       "      <th>...</th>\n",
       "      <th>t_densidadem2</th>\n",
       "      <th>rdpc_def_vulner</th>\n",
       "      <th>t_analf_18m</th>\n",
       "      <th>t_formal_18m</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "      <th>CNN_all</th>\n",
       "      <th>CNN_0-19</th>\n",
       "      <th>rate_total</th>\n",
       "      <th>rate_019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2001-01-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.154301</td>\n",
       "      <td>295.674980</td>\n",
       "      <td>88.460308</td>\n",
       "      <td>303.987216</td>\n",
       "      <td>294.155015</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>42.754490</td>\n",
       "      <td>29.124122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2001-02-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.216873</td>\n",
       "      <td>295.944060</td>\n",
       "      <td>88.856948</td>\n",
       "      <td>304.738755</td>\n",
       "      <td>294.332566</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>17.601025</td>\n",
       "      <td>11.718582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2001-03-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.239112</td>\n",
       "      <td>296.092747</td>\n",
       "      <td>89.305463</td>\n",
       "      <td>304.620829</td>\n",
       "      <td>294.304126</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>11.072645</td>\n",
       "      <td>6.376287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2001-04-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.334660</td>\n",
       "      <td>296.186143</td>\n",
       "      <td>88.590375</td>\n",
       "      <td>304.168669</td>\n",
       "      <td>293.921815</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>5.120298</td>\n",
       "      <td>3.791306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2001-05-01</td>\n",
       "      <td>2001</td>\n",
       "      <td>5</td>\n",
       "      <td>11</td>\n",
       "      <td>237765.347</td>\n",
       "      <td>0.378931</td>\n",
       "      <td>295.562972</td>\n",
       "      <td>86.939606</td>\n",
       "      <td>303.903043</td>\n",
       "      <td>293.395959</td>\n",
       "      <td>...</td>\n",
       "      <td>27.15</td>\n",
       "      <td>144.93</td>\n",
       "      <td>9.42</td>\n",
       "      <td>51.72</td>\n",
       "      <td>53.83</td>\n",
       "      <td>36.93</td>\n",
       "      <td>32.159859</td>\n",
       "      <td>17.186546</td>\n",
       "      <td>6.976406</td>\n",
       "      <td>4.652966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 61 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Year  Month  CD_UF    area_km2    NDVI_d  \\\n",
       "0  2001-01-01  2001      1     11  237765.347  0.154301   \n",
       "1  2001-02-01  2001      2     11  237765.347  0.216873   \n",
       "2  2001-03-01  2001      3     11  237765.347  0.239112   \n",
       "3  2001-04-01  2001      4     11  237765.347  0.334660   \n",
       "4  2001-05-01  2001      5     11  237765.347  0.378931   \n",
       "\n",
       "   dewpoint_temperature_2m_d  humidity_d  max_temperature_2m_d  \\\n",
       "0                 295.674980   88.460308            303.987216   \n",
       "1                 295.944060   88.856948            304.738755   \n",
       "2                 296.092747   89.305463            304.620829   \n",
       "3                 296.186143   88.590375            304.168669   \n",
       "4                 295.562972   86.939606            303.903043   \n",
       "\n",
       "   min_temperature_2m_d  ...  t_densidadem2  rdpc_def_vulner  t_analf_18m  \\\n",
       "0            294.155015  ...          27.15           144.93         9.42   \n",
       "1            294.332566  ...          27.15           144.93         9.42   \n",
       "2            294.304126  ...          27.15           144.93         9.42   \n",
       "3            293.921815  ...          27.15           144.93         9.42   \n",
       "4            293.395959  ...          27.15           144.93         9.42   \n",
       "\n",
       "   t_formal_18m  t_fundc_ocup18m  t_medioc_ocup18m    CNN_all   CNN_0-19  \\\n",
       "0         51.72            53.83             36.93   1.000000   1.000000   \n",
       "1         51.72            53.83             36.93   1.000000   1.000000   \n",
       "2         51.72            53.83             36.93   1.000000   1.000000   \n",
       "3         51.72            53.83             36.93   1.000000   1.000000   \n",
       "4         51.72            53.83             36.93  32.159859  17.186546   \n",
       "\n",
       "   rate_total   rate_019  \n",
       "0   42.754490  29.124122  \n",
       "1   17.601025  11.718582  \n",
       "2   11.072645   6.376287  \n",
       "3    5.120298   3.791306  \n",
       "4    6.976406   4.652966  \n",
       "\n",
       "[5 rows x 61 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataframe = utils.clean(dataframe)\n",
    "dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c239e6b3-250f-4a08-b48c-c6a3f4e53621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6156 entries, 0 to 6155\n",
      "Data columns (total 61 columns):\n",
      " #   Column                     Non-Null Count  Dtype  \n",
      "---  ------                     --------------  -----  \n",
      " 0   Date                       6156 non-null   object \n",
      " 1   Year                       6156 non-null   int64  \n",
      " 2   Month                      6156 non-null   int64  \n",
      " 3   CD_UF                      6156 non-null   int64  \n",
      " 4   area_km2                   6156 non-null   float64\n",
      " 5   NDVI_d                     6156 non-null   float64\n",
      " 6   dewpoint_temperature_2m_d  6156 non-null   float64\n",
      " 7   humidity_d                 6156 non-null   float64\n",
      " 8   max_temperature_2m_d       6156 non-null   float64\n",
      " 9   min_temperature_2m_d       6156 non-null   float64\n",
      " 10  surface_pressure_d         6156 non-null   float64\n",
      " 11  temperature_2m_d           6156 non-null   float64\n",
      " 12  total_precipitation_d      6156 non-null   float64\n",
      " 13  u_component_of_wind_10m_d  6156 non-null   float64\n",
      " 14  v_component_of_wind_10m_d  6156 non-null   float64\n",
      " 15  max_elevation_d            6156 non-null   float64\n",
      " 16  mean_elevation_d           6156 non-null   float64\n",
      " 17  min_elevation_d            6156 non-null   float64\n",
      " 18  stdDev_elevation_d         6156 non-null   float64\n",
      " 19  variance_elevation_d       6156 non-null   float64\n",
      " 20  PopTotal_Urban_UF          6156 non-null   int64  \n",
      " 21  PopTotal_Rural_UF          6156 non-null   int64  \n",
      " 22  cases0_19                  6156 non-null   int64  \n",
      " 23  cases20_99                 6156 non-null   int64  \n",
      " 24  Forest_Cover_Percent       6156 non-null   float64\n",
      " 25  Urban_Cover_Percent        6156 non-null   float64\n",
      " 26  ivs                        6156 non-null   float64\n",
      " 27  ivs_infraestrutura_urbana  6156 non-null   float64\n",
      " 28  ivs_capital_humano         6156 non-null   float64\n",
      " 29  ivs_renda_e_trabalho       6156 non-null   float64\n",
      " 30  t_sem_agua_esgoto          6156 non-null   float64\n",
      " 31  t_sem_lixo                 6156 non-null   float64\n",
      " 32  t_vulner_mais1h            6156 non-null   float64\n",
      " 33  t_analf_15m                6156 non-null   float64\n",
      " 34  t_cdom_fundin              6156 non-null   float64\n",
      " 35  t_p15a24_nada              6156 non-null   float64\n",
      " 36  t_vulner                   6156 non-null   float64\n",
      " 37  t_desocup18m               6156 non-null   float64\n",
      " 38  t_p18m_fundin_informal     6156 non-null   float64\n",
      " 39  idhm                       6156 non-null   float64\n",
      " 40  idhm_long                  6156 non-null   float64\n",
      " 41  idhm_educ                  6156 non-null   float64\n",
      " 42  idhm_renda                 6156 non-null   float64\n",
      " 43  idhm_educ_sub_esc          6156 non-null   float64\n",
      " 44  t_pop18m_fundc             6156 non-null   float64\n",
      " 45  idhm_educ_sub_freq         6156 non-null   float64\n",
      " 46  renda_per_capita           6156 non-null   float64\n",
      " 47  pea10a14                   6156 non-null   int64  \n",
      " 48  pea15a17                   6156 non-null   int64  \n",
      " 49  pea18m                     6156 non-null   int64  \n",
      " 50  t_eletrica                 6156 non-null   float64\n",
      " 51  t_densidadem2              6156 non-null   float64\n",
      " 52  rdpc_def_vulner            6156 non-null   float64\n",
      " 53  t_analf_18m                6156 non-null   float64\n",
      " 54  t_formal_18m               6156 non-null   float64\n",
      " 55  t_fundc_ocup18m            6156 non-null   float64\n",
      " 56  t_medioc_ocup18m           6156 non-null   float64\n",
      " 57  CNN_all                    6156 non-null   float64\n",
      " 58  CNN_0-19                   6156 non-null   float64\n",
      " 59  rate_total                 6156 non-null   float64\n",
      " 60  rate_019                   6156 non-null   float64\n",
      "dtypes: float64(50), int64(10), object(1)\n",
      "memory usage: 2.9+ MB\n"
     ]
    }
   ],
   "source": [
    "dataframe.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "805f5f4c-e959-4ca8-8aad-8699873536ec",
   "metadata": {},
   "source": [
    "## Apply Data Reduction\n",
    "**Data reduction is applied to three macro groups in order to reduce the number of variables on which the AI framework will be trained. The variables belonging to each group are set with the *PCAgroups* dictionary. The groups are:**\n",
    "1. ***CLIMATIC VARIABLES***,\n",
    "2. ***GEO VARIABLES***,\n",
    "3. ***SOCIO VARIABLES***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1f68cd77-57d0-43c3-9944-aca5e3879bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m PCA Excluded Variables \u001b[0m\n",
      "----- 1 t_fundc_ocup18m\n",
      "----- 2 t_medioc_ocup18m\n",
      "----- 3 PopTotal_Urban_UF\n",
      "----- 4 PopTotal_Rural_UF\n",
      "----- 5 total_precipitation_d\n",
      "----- 6 surface_pressure_d\n",
      "----- 7 area_km2\n",
      "----- 8 humidity_d\n",
      "----- 9 temperature_2m_d\n",
      "----- 10 min_temperature_2m_d\n",
      "----- 11 CNN_all\n",
      "----- 12 CNN_0-19\n",
      "\u001b[1m Climatic variables \u001b[0m\n",
      "----- 1 dewpoint_temperature_2m_d\n",
      "----- 2 max_temperature_2m_d\n",
      "----- 3 u_component_of_wind_10m_d\n",
      "----- 4 v_component_of_wind_10m_d\n",
      "\u001b[1m Geo variables \u001b[0m\n",
      "----- 1 NDVI_d\n",
      "----- 2 max_elevation_d\n",
      "----- 3 mean_elevation_d\n",
      "----- 4 min_elevation_d\n",
      "----- 5 stdDev_elevation_d\n",
      "----- 6 variance_elevation_d\n",
      "----- 7 Forest_Cover_Percent\n",
      "----- 8 Urban_Cover_Percent\n",
      "\u001b[1m Socio variables \u001b[0m\n",
      "----- 1 Urban_Cover_Percent\n",
      "----- 2 ivs\n",
      "----- 3 ivs_infraestrutura_urbana\n",
      "----- 4 ivs_capital_humano\n",
      "----- 5 ivs_renda_e_trabalho\n",
      "----- 6 t_sem_agua_esgoto\n",
      "----- 7 t_sem_lixo\n",
      "----- 8 t_vulner_mais1h\n",
      "----- 9 t_analf_15m\n",
      "----- 10 t_cdom_fundin\n",
      "----- 11 t_p15a24_nada\n",
      "----- 12 t_vulner\n",
      "----- 13 t_desocup18m\n",
      "----- 14 t_p18m_fundin_informal\n",
      "----- 15 idhm\n",
      "----- 16 idhm_long\n",
      "----- 17 idhm_educ\n",
      "----- 18 idhm_renda\n",
      "----- 19 idhm_educ_sub_esc\n",
      "----- 20 t_pop18m_fundc\n",
      "----- 21 idhm_educ_sub_freq\n",
      "----- 22 renda_per_capita\n",
      "----- 23 pea10a14\n",
      "----- 24 pea15a17\n",
      "----- 25 pea18m\n",
      "----- 26 t_eletrica\n",
      "----- 27 t_densidadem2\n",
      "----- 28 rdpc_def_vulner\n",
      "----- 29 t_analf_18m\n",
      "----- 30 t_formal_18m\n",
      "\u001b[1m Additional variables \u001b[0m\n",
      "----- 1 Month\n",
      "----- 2 cases20_99\n",
      "----- 3 cases0_19\n",
      "\u001b[1m Dengue variables \u001b[0m\n",
      "----- 1 rate_total\n",
      "----- 2 rate_019\n"
     ]
    }
   ],
   "source": [
    "print('\\033[1m PCA Excluded Variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['EXCLUDED'])\n",
    "\n",
    "print('\\033[1m Climatic variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['CLIMATIC VARIABLES'])\n",
    "\n",
    "print('\\033[1m Geo variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['GEO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Socio variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['SOCIO VARIABLES'])\n",
    "\n",
    "print('\\033[1m Additional variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['AUXILIAR'])\n",
    "\n",
    "print('\\033[1m Dengue variables \\033[0m')\n",
    "utils.plist(GROUPED_VARS['DENGUE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1815557e-4b40-4c03-b91f-b60d6a58f108",
   "metadata": {},
   "source": [
    "**We selected two types of data reduction methods: PCA (Principal Component Analysis) and PLS (Principal Least Square). The second one is the default solution because it reduces the input data by considering also a second variable that in our case is the Dengue Incidence Rates.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3c62d04b-4da5-44de-92fc-c70790b287a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_reduction import pca_reducer, pls_reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c4c0e59-ee4e-4be5-8074-794f8dc34b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6806ca81-c5c6-438c-9444-8462223831d0",
   "metadata": {},
   "source": [
    "**Extract climatic, geophysical and socio-economic variables from the dataframe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0bb4327b-6fc2-4cd7-b6b2-78ee4d7472a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_climatic = scaler.fit_transform(dataframe[GROUPED_VARS['CLIMATIC VARIABLES']].values)\n",
    "X_geo = scaler.fit_transform(dataframe[GROUPED_VARS['GEO VARIABLES']].values)\n",
    "X_socio = scaler.fit_transform(dataframe[GROUPED_VARS['SOCIO VARIABLES']].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed5247c-ff82-4f81-91d4-2e345aaabb3b",
   "metadata": {},
   "source": [
    "**Extract Dengue variables from the dataframe, apply a root scaling and normalization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9dacba83-0be0-4471-9305-eda1fad9b7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_dengue = dataframe[GROUPED_VARS['DENGUE']].values\n",
    "scaler = MinMaxScaler()\n",
    "y_dengue = scaler.fit_transform(y_dengue)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b84bee-ab9a-4e31-a10c-4400b0d7c8d2",
   "metadata": {},
   "source": [
    "**Apply data reduction technique**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "750a535b-cdeb-4369-be94-9c868017b6e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6156, 10)\n"
     ]
    }
   ],
   "source": [
    "if DATA_REDUCER_SETTINGS['TYPE'] == 'PLS':\n",
    "    climatic_vars_reduced = pls_reducer(\n",
    "        X_climatic, y_dengue, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES']\n",
    "    )\n",
    "    geo_vars_reduced = pls_reducer(\n",
    "        X_geo, y_dengue, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES']\n",
    "    )\n",
    "    socio_vars_reduced = pls_reducer(\n",
    "        X_socio, y_dengue, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES']\n",
    "    )\n",
    "    print(socio_vars_reduced.shape)\n",
    "elif DATA_REDUCER_SETTINGS['TYPE'] == 'PCA':\n",
    "    climatic_vars_reduced = pca_reducer(\n",
    "        X_climatic, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['CLIMATIC VARIABLES']\n",
    "    )\n",
    "    geo_vars_reduced = pca_reducer(\n",
    "        X_geo, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['GEO VARIABLES']\n",
    "    )\n",
    "    socio_vars_reduced = pca_reducer(\n",
    "        X_socio, \n",
    "        DATA_REDUCER_SETTINGS['NUMBER OF COMPONENTS']['SOCIO VARIABLES']\n",
    "    )\n",
    "else:\n",
    "    print('No data reduction.')\n",
    "    climatic_vars_reduced, geo_vars_reduced, socio_vars_reduced = X_climatic, X_geo, X_socio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2e0a54-baaa-4b51-9413-919b647a7f84",
   "metadata": {},
   "source": [
    "## Order reduced data in a new dataframe\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54e0386-aad7-48f5-9863-26ddbc54c045",
   "metadata": {},
   "source": [
    "**Normalize remaining variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "79010428-adf4-42f6-88a2-bf8544f33d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_excluded = dataframe[GROUPED_VARS['EXCLUDED']].values\n",
    "x_excluded = MinMaxScaler().fit_transform(x_excluded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d2fd3d6-4417-49fc-97ed-67b62f433b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_auxiliar = dataframe[GROUPED_VARS['AUXILIAR']].values\n",
    "X_auxiliar = MinMaxScaler().fit_transform(X_auxiliar)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112ea409-fcb0-42a7-993b-6a101bb3f20b",
   "metadata": {},
   "source": [
    "**Create a new database with the reduced, the auxiliar and Dengue variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9fc5d75c-d5b7-416e-998e-18681602ffa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Year', 'dep_id', 't_fundc_ocup18m', 't_medioc_ocup18m',\n",
       "       'PopTotal_Urban_UF', 'PopTotal_Rural_UF', 'total_precipitation_d',\n",
       "       'surface_pressure_d', 'area_km2', 'humidity_d', 'temperature_2m_d',\n",
       "       'min_temperature_2m_d', 'CNN_all', 'CNN_0-19', 'Month', 'cases20_99',\n",
       "       'cases0_19', 'RandEffects1', 'RandEffects2', 'RandEffects3',\n",
       "       'PCA0-Climatic', 'PCA1-Climatic', 'PCA2-Climatic', 'PCA3-Climatic',\n",
       "       'PCA0-Geo', 'PCA1-Geo', 'PCA2-Geo', 'PCA3-Geo', 'PCA4-Geo', 'PCA5-Geo',\n",
       "       'PCA0-Socio', 'PCA1-Socio', 'PCA2-Socio', 'PCA3-Socio', 'PCA4-Socio',\n",
       "       'PCA5-Socio', 'DengRate_all', 'DengRate_019'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "independent = {'Year':dataframe.Year.values, 'dep_id':dataframe.CD_UF.values, 't_fundc_ocup18m':x_excluded[:, 0], 't_medioc_ocup18m':x_excluded[:, 1],\n",
    "               'PopTotal_Urban_UF':x_excluded[:, 2], 'PopTotal_Rural_UF':x_excluded[:, 3], 'total_precipitation_d':x_excluded[:, 4],\n",
    "               'surface_pressure_d':x_excluded[:, 5], 'area_km2':x_excluded[:, 6], 'humidity_d':x_excluded[:, 7], 'temperature_2m_d':x_excluded[:, 8],\n",
    "               'min_temperature_2m_d':x_excluded[:, 9], 'CNN_all':x_excluded[:, 10], 'CNN_0-19':x_excluded[:, 11]}\n",
    "\n",
    "auxiliar    = {'Month': X_auxiliar[:, 0],\n",
    "               'cases20_99': X_auxiliar[:, 1], 'cases0_19': X_auxiliar[:, 2],\n",
    "               'RandEffects1':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],\n",
    "               'RandEffects2':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0],\n",
    "               'RandEffects3':  MinMaxScaler().fit_transform(np.reshape(dataframe.CD_UF.values*dataframe.Month.values*dataframe.Year.values, (dataframe.CD_UF.values.shape[0], 1)))[:,0]}\n",
    "\n",
    "climatic    = {'PCA0-Climatic':climatic_vars_reduced[:,0], 'PCA1-Climatic':climatic_vars_reduced[:,1], 'PCA2-Climatic':climatic_vars_reduced[:,2],\n",
    "               'PCA3-Climatic':climatic_vars_reduced[:,3]}\n",
    "\n",
    "geo         = {'PCA0-Geo':geo_vars_reduced[:,0], 'PCA1-Geo':geo_vars_reduced[:,1], 'PCA2-Geo':geo_vars_reduced[:,2],\n",
    "               'PCA3-Geo':geo_vars_reduced[:,3], 'PCA4-Geo':geo_vars_reduced[:,4], 'PCA5-Geo':geo_vars_reduced[:,5]}\n",
    "\n",
    "socio       = {'PCA0-Socio':socio_vars_reduced[:,0], 'PCA1-Socio':socio_vars_reduced[:,1], 'PCA2-Socio':socio_vars_reduced[:,2],\n",
    "               'PCA3-Socio':socio_vars_reduced[:,3], 'PCA4-Socio':socio_vars_reduced[:,4], 'PCA5-Socio':socio_vars_reduced[:,5]}\n",
    "\n",
    "dengue      = {'DengRate_all': y_dengue[:,0], 'DengRate_019': y_dengue[:,1]}\n",
    "\n",
    "columns     = {**independent, **auxiliar, **climatic, **geo, **socio, **dengue}\n",
    "\n",
    "reduced_dataframe = pd.DataFrame(columns)\n",
    "reduced_dataframe.head()\n",
    "reduced_dataframe.columns\n",
    "\n",
    "# reduced_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "728cf773-3168-4a4f-81bc-0e0fec99f9fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5184, 38)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reduced_dataframe=reduced_dataframe[reduced_dataframe['Year']>=2004]\n",
    "reduced_dataframe.to_csv('reduced_2004_2019.csv', index=False)\n",
    "reduced_dataframe.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04aca77-87a0-44f3-bfdd-70689cb5085e",
   "metadata": {},
   "source": [
    "## Create training and validation data\n",
    "**First of all, the dataframe is divided in two sub-dataframes (training and validation) by using the variable *Year***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7ced3105-fb39-4b28-9999-ff54caf7c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Shape: (5184, 41)\n",
      "Columns: Index(['Year', 'dep_id', 't_fundc_ocup18m', 't_medioc_ocup18m',\n",
      "       'PopTotal_Urban_UF', 'PopTotal_Rural_UF', 'total_precipitation_d',\n",
      "       'surface_pressure_d', 'area_km2', 'humidity_d', 'temperature_2m_d',\n",
      "       'min_temperature_2m_d', 'CNN_all', 'CNN_0-19', 'Month', 'cases20_99',\n",
      "       'cases0_19', 'RandEffects1', 'RandEffects2', 'RandEffects3',\n",
      "       'PCA0-Climatic', 'PCA1-Climatic', 'PCA2-Climatic', 'PCA3-Climatic',\n",
      "       'PCA0-Geo', 'PCA1-Geo', 'PCA2-Geo', 'PCA3-Geo', 'PCA4-Geo', 'PCA5-Geo',\n",
      "       'PCA0-Socio', 'PCA1-Socio', 'PCA2-Socio', 'PCA3-Socio', 'PCA4-Socio',\n",
      "       'PCA5-Socio', 'mosquito_interest', 'sintomas_dengue_interest',\n",
      "       'dengue_interest', 'DengRate_all', 'DengRate_019'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "dataset_path = os.path.join(\"..\", \"google trends\", \"merged_dataset.csv\")\n",
    "reduced_dataframe = pd.read_csv(dataset_path)\n",
    "# Print shape and column names\n",
    "print(\"Dataset Shape:\", reduced_dataframe.shape)\n",
    "print(\"Columns:\", reduced_dataframe.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "438dcf70-8288-4671-b9a2-3178b5816112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Size: 4212, Validation Size: 972\n",
      "Missing values in training set: 0\n",
      "Missing values in validation set: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Year</th>\n",
       "      <th>dep_id</th>\n",
       "      <th>t_fundc_ocup18m</th>\n",
       "      <th>t_medioc_ocup18m</th>\n",
       "      <th>PopTotal_Urban_UF</th>\n",
       "      <th>PopTotal_Rural_UF</th>\n",
       "      <th>total_precipitation_d</th>\n",
       "      <th>surface_pressure_d</th>\n",
       "      <th>area_km2</th>\n",
       "      <th>humidity_d</th>\n",
       "      <th>...</th>\n",
       "      <th>PCA1-Socio</th>\n",
       "      <th>PCA2-Socio</th>\n",
       "      <th>PCA3-Socio</th>\n",
       "      <th>PCA4-Socio</th>\n",
       "      <th>PCA5-Socio</th>\n",
       "      <th>mosquito_interest</th>\n",
       "      <th>sintomas_dengue_interest</th>\n",
       "      <th>dengue_interest</th>\n",
       "      <th>DengRate_all</th>\n",
       "      <th>DengRate_019</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.470017</td>\n",
       "      <td>0.816201</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.933080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096634</td>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.076292</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.032628</td>\n",
       "      <td>0.031062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.455807</td>\n",
       "      <td>0.813839</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.957511</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096634</td>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.076292</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.35</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.028203</td>\n",
       "      <td>0.036438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.409449</td>\n",
       "      <td>0.815870</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.945000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096634</td>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.076292</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.22</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.020461</td>\n",
       "      <td>0.020310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.239287</td>\n",
       "      <td>0.822995</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.947705</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096634</td>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.076292</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.15</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.008185</td>\n",
       "      <td>0.007766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>2017</td>\n",
       "      <td>11</td>\n",
       "      <td>0.206751</td>\n",
       "      <td>0.12853</td>\n",
       "      <td>0.020497</td>\n",
       "      <td>0.089637</td>\n",
       "      <td>0.123361</td>\n",
       "      <td>0.827288</td>\n",
       "      <td>0.149352</td>\n",
       "      <td>0.935646</td>\n",
       "      <td>...</td>\n",
       "      <td>0.096634</td>\n",
       "      <td>0.179967</td>\n",
       "      <td>0.325246</td>\n",
       "      <td>0.604686</td>\n",
       "      <td>0.076292</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.005032</td>\n",
       "      <td>0.006372</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 41 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Year  dep_id  t_fundc_ocup18m  t_medioc_ocup18m  PopTotal_Urban_UF  \\\n",
       "156  2017      11         0.206751           0.12853           0.020497   \n",
       "157  2017      11         0.206751           0.12853           0.020497   \n",
       "158  2017      11         0.206751           0.12853           0.020497   \n",
       "159  2017      11         0.206751           0.12853           0.020497   \n",
       "160  2017      11         0.206751           0.12853           0.020497   \n",
       "\n",
       "     PopTotal_Rural_UF  total_precipitation_d  surface_pressure_d  area_km2  \\\n",
       "156           0.089637               0.470017            0.816201  0.149352   \n",
       "157           0.089637               0.455807            0.813839  0.149352   \n",
       "158           0.089637               0.409449            0.815870  0.149352   \n",
       "159           0.089637               0.239287            0.822995  0.149352   \n",
       "160           0.089637               0.123361            0.827288  0.149352   \n",
       "\n",
       "     humidity_d  ...  PCA1-Socio  PCA2-Socio  PCA3-Socio  PCA4-Socio  \\\n",
       "156    0.933080  ...    0.096634    0.179967    0.325246    0.604686   \n",
       "157    0.957511  ...    0.096634    0.179967    0.325246    0.604686   \n",
       "158    0.945000  ...    0.096634    0.179967    0.325246    0.604686   \n",
       "159    0.947705  ...    0.096634    0.179967    0.325246    0.604686   \n",
       "160    0.935646  ...    0.096634    0.179967    0.325246    0.604686   \n",
       "\n",
       "     PCA5-Socio  mosquito_interest  sintomas_dengue_interest  dengue_interest  \\\n",
       "156    0.076292               0.11                      0.24             0.14   \n",
       "157    0.076292               0.14                      0.35             0.14   \n",
       "158    0.076292               0.26                      0.22             0.19   \n",
       "159    0.076292               0.15                      0.15             0.18   \n",
       "160    0.076292               0.12                      0.10             0.10   \n",
       "\n",
       "     DengRate_all  DengRate_019  \n",
       "156      0.032628      0.031062  \n",
       "157      0.028203      0.036438  \n",
       "158      0.020461      0.020310  \n",
       "159      0.008185      0.007766  \n",
       "160      0.005032      0.006372  \n",
       "\n",
       "[5 rows x 41 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_dataframe = reduced_dataframe[reduced_dataframe.Year < 2017].copy()\n",
    "validation_dataframe = reduced_dataframe[reduced_dataframe.Year >= 2017].copy()\n",
    "print(f\"Training Size: {len(training_dataframe)}, Validation Size: {len(validation_dataframe)}\")\n",
    "\n",
    "# Check for missing values\n",
    "print(\"Missing values in training set:\", training_dataframe.isnull().sum().sum())\n",
    "print(\"Missing values in validation set:\", validation_dataframe.isnull().sum().sum())\n",
    "training_dataframe.head()\n",
    "validation_dataframe.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "bf1008a7-e230-4319-91ba-139798af9a75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# training_dataframe[\"DengRate_all_log\"] = np.log1p(training_dataframe[\"DengRate_all\"])\n",
    "# validation_dataframe[\"DengRate_all_log\"] = np.log1p(validation_dataframe[\"DengRate_all\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c7324f60-a313-4abd-bdcb-fa52638e778d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Replace 'rate_total' with 'DengRate_all' (or 'DengRate_019' if needed)\n",
    "# plt.hist(training_dataframe[\"DengRate_all\"], bins=30, alpha=0.5, label=\"Train\")\n",
    "# plt.hist(validation_dataframe[\"DengRate_all\"], bins=30, alpha=0.5, label=\"Validation\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9816e4-2288-48dd-ba26-3fdbc0c12a67",
   "metadata": {},
   "source": [
    "**Then the dataset handler is initialized. This object will handle all the operations needed to create, reshape and augment the training and validation dataset to fit the requirements of each Deep Learning or Machine Learning model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e9fa0b1f-5096-4ed9-ab0e-e48a48e836a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasetHandler import datasetHandler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95b22769-2021-412b-b9b3-6be9a4973048",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Drop excluded columns before feeding into DatasetHandler\n",
    "# excluded_columns = GROUPED_VARS[\"EXCLUDED\"]\n",
    "# training_dataframe_filtered = training_dataframe.drop(columns=excluded_columns, errors=\"ignore\")\n",
    "# validation_dataframe_filtered = validation_dataframe.drop(columns=excluded_columns, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8284bdad-6dbc-4ac5-b13d-c39d03674383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize dataset handler with filtered data\n",
    "dataset_handler = datasetHandler(training_dataframe, validation_dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e6c34a-a4b0-4b37-9a75-b99a2d643b11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "75c0f745-aa80-4b64-95c7-a595a6558564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training shape: (3888, 12, 41)\n",
      "Y Training shape: (3888, 2)\n",
      "X Validation shape: (648, 12, 41)\n",
      "Y Validation shape: (648, 2)\n",
      "\n",
      "X Training shape: (3888, 12, 41)\n",
      "Y Training shape: (3888, 2)\n",
      "X Validation shape: (648, 12, 41)\n",
      "Y Validation shape: (648, 2)\n"
     ]
    }
   ],
   "source": [
    "# Fetch training/validation data & indices\n",
    "x_train, y_train, x_val, y_val, train_indices, val_indices = dataset_handler.get_data(\n",
    "    DATA_PROCESSING_SETTINGS[\"T LEARNING\"], DATA_PROCESSING_SETTINGS[\"T PREDICTION\"]\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(\"\\nX Training shape:\", x_train.shape)\n",
    "print(\"Y Training shape:\", y_train.shape)\n",
    "print(\"X Validation shape:\", x_val.shape)\n",
    "print(\"Y Validation shape:\", y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cf48d4c-2b99-4305-adb5-8776b1081f01",
   "metadata": {},
   "source": [
    "**Apply data augmention**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "3df8036e-45d9-4145-9e5d-78dae5dbb566",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X Training Augmented shape: (11664, 12, 41)\n",
      "Y Training Augmented shape: (11664, 2)\n",
      "X Validation Augmented shape: (1944, 12, 41)\n",
      "Y Validation Augmented shape: (1944, 2)\n"
     ]
    }
   ],
   "source": [
    "# Apply augmentation with dynamic noise scaling\n",
    "x_train_a, y_train_a, x_val_a, y_val_a = dataset_handler.augment(\n",
    "    x_train, y_train, x_val, y_val, DATA_PROCESSING_SETTINGS[\"AUGMENTATION\"]\n",
    ")\n",
    "\n",
    "# Print shapes\n",
    "print(\"X Training Augmented shape:\", x_train_a.shape)\n",
    "print(\"Y Training Augmented shape:\", y_train_a.shape)\n",
    "print(\"X Validation Augmented shape:\", x_val_a.shape)\n",
    "print(\"Y Validation Augmented shape:\", y_val_a.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6689f47-70df-4707-9510-a6adf4c7c50a",
   "metadata": {},
   "source": [
    "# TCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b6b0922-76b2-4050-8d41-2ac07125e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv1D, Dense, Flatten, BatchNormalization, Dropout, Input, Activation\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import root_mean_squared_error\n",
    "from keras.metrics import MeanSquaredError, MeanAbsoluteError\n",
    "from keras_tuner import HyperModel, RandomSearch\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from glob import glob\n",
    "from tensorflow.keras.models import load_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, roc_curve, auc, average_precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48879a87-b990-4308-9848-adeeb5f4bcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, regularizers, Input\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# Even though ExponentialDecay is imported, it is not actually used in this code.\n",
    "from tensorflow.keras.optimizers.schedules import ExponentialDecay  \n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "\n",
    "LSTM_SETTINGS = {\n",
    "    'EPOCHS': 200,\n",
    "    'LEARNING RATE': 0.0001,\n",
    "    'BATCH SIZE': 16,\n",
    "    'EARLY STOPPING': 24,\n",
    "    'DROPOUT_RATE': 0.3,\n",
    "    'L2_REGULARIZATION': 1e-4,\n",
    "    'NUM_RESIDUAL_BLOCKS': 4,\n",
    "    'NUM_FILTERS': 64\n",
    "}\n",
    "\n",
    "def build_tcn_model_v2(input_shape, output_units, num_filters, dropout_rate, l2_reg, num_residual_blocks):\n",
    "    def residual_block(x, filters, dilation_rate):\n",
    "        shortcut = x\n",
    "        x = layers.Conv1D(filters, 3, padding='causal', activation='relu',\n",
    "                         dilation_rate=dilation_rate, kernel_regularizer=regularizers.l2(l2_reg))(x)\n",
    "        x = layers.BatchNormalization()(x)\n",
    "        x = layers.Dropout(dropout_rate)(x)\n",
    "        x = layers.add([x, shortcut])\n",
    "        return layers.ReLU()(x)\n",
    "\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = layers.Conv1D(num_filters, 3, padding='causal', activation='relu')(inputs)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Dropout(dropout_rate * 0.5)(x)\n",
    "    \n",
    "    for dilation in [1, 2, 4, 8][:num_residual_blocks]:\n",
    "        x = residual_block(x, num_filters, dilation)\n",
    "        \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    outputs = layers.Dense(output_units, activation='linear')(x)\n",
    "    return models.Model(inputs, outputs)\n",
    "\n",
    "    \n",
    "class ImprovedTCNNet:\n",
    "    def __init__(self, shape, output_units=2, num_filters=LSTM_SETTINGS['NUM_FILTERS'],\n",
    "                 dropout_rate=LSTM_SETTINGS['DROPOUT_RATE'], l2_regularization=LSTM_SETTINGS['L2_REGULARIZATION'],\n",
    "                 num_residual_blocks=LSTM_SETTINGS['NUM_RESIDUAL_BLOCKS']):\n",
    "        self.shape = shape\n",
    "        self.epochs = LSTM_SETTINGS['EPOCHS']\n",
    "        self.batch_size = LSTM_SETTINGS['BATCH SIZE']\n",
    "        self.lr = LSTM_SETTINGS['LEARNING RATE']\n",
    "        self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.l2_regularization = l2_regularization\n",
    "        self.num_filters = num_filters\n",
    "        self.num_residual_blocks = num_residual_blocks\n",
    "        \n",
    "        # Build the uncompiled model\n",
    "        self.model = build_tcn_model_v2(\n",
    "            self.shape, output_units, self.num_filters,\n",
    "            self.dropout_rate, self.l2_regularization, self.num_residual_blocks\n",
    "        )\n",
    "        \n",
    "        # Compile the model\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(learning_rate=self.lr),\n",
    "            loss='Huber',\n",
    "            metrics=['mae']\n",
    "        )\n",
    "    \n",
    "    def load(self, model_path):\n",
    "        \"\"\"\n",
    "        Load a saved model from the specified path.\n",
    "        \"\"\"\n",
    "        self.model = tf.keras.models.load_model(model_path)\n",
    "        print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def train(self, training, validation, output_path):\n",
    "        \"\"\"Train the TCN model with early stopping and learning rate reduction.\"\"\"\n",
    "        # Early Stopping\n",
    "        es = EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=self.early_stopping_rounds,\n",
    "            restore_best_weights=True\n",
    "        )\n",
    "        \n",
    "        # Reduce LR on Plateau\n",
    "        lr_scheduler = ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            min_lr=1e-6,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Training\n",
    "        history = self.model.fit(\n",
    "            x=training[0],\n",
    "            y=training[1],\n",
    "            validation_data=(validation[0], validation[1]),\n",
    "            epochs=self.epochs,\n",
    "            batch_size=self.batch_size,\n",
    "            callbacks=[es, lr_scheduler],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        # Plot the training history\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.plot(history.history['loss'], label='Train Loss')\n",
    "        plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "        plt.legend()\n",
    "        plt.title('Improved TCN Model Loss Curve')\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.show()\n",
    "\n",
    "        # Save the model\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        model_filename = f\"TCN-new-search-{today}.keras\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        save_path = os.path.join(output_path, model_filename)\n",
    "        self.model.save(save_path)\n",
    "        print(f\"Model saved to {save_path}\")\n",
    "\n",
    "        return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "651eb1f2-25da-4aaa-ab04-e0963e55d4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_percentage_error, mean_squared_error, r2_score, mean_absolute_error, mean_squared_log_error\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "212cf739-c23b-4a19-97b5-06a977215364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking for saved models...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"conv1d_5\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, None)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 108\u001b[0m\n\u001b[0;32m    105\u001b[0m     exit()\n\u001b[0;32m    107\u001b[0m \u001b[38;5;66;03m# Load the most recent TCN model\u001b[39;00m\n\u001b[1;32m--> 108\u001b[0m tcn \u001b[38;5;241m=\u001b[39m \u001b[43mImprovedTCNNet\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Dummy shape, replaced by loaded model\u001b[39;00m\n\u001b[0;32m    109\u001b[0m tcn\u001b[38;5;241m.\u001b[39mload(tcn_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading model from: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtcn_models[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[39], line 66\u001b[0m, in \u001b[0;36mImprovedTCNNet.__init__\u001b[1;34m(self, shape, output_units, num_filters, dropout_rate, l2_regularization, num_residual_blocks)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_residual_blocks \u001b[38;5;241m=\u001b[39m num_residual_blocks\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# Build the uncompiled model\u001b[39;00m\n\u001b[1;32m---> 66\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[43mbuild_tcn_model_v2\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_units\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43ml2_regularization\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_residual_blocks\u001b[49m\n\u001b[0;32m     69\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Compile the model\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     73\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr),\n\u001b[0;32m     74\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHuber\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     75\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmae\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     76\u001b[0m )\n",
      "Cell \u001b[1;32mIn[39], line 39\u001b[0m, in \u001b[0;36mbuild_tcn_model_v2\u001b[1;34m(input_shape, output_units, num_filters, dropout_rate, l2_reg, num_residual_blocks)\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layers\u001b[38;5;241m.\u001b[39mReLU()(x)\n\u001b[0;32m     38\u001b[0m inputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mInput(shape\u001b[38;5;241m=\u001b[39minput_shape)\n\u001b[1;32m---> 39\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mConv1D\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_filters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcausal\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mBatchNormalization()(x)\n\u001b[0;32m     41\u001b[0m x \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDropout(dropout_rate \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.5\u001b[39m)(x)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python312\\site-packages\\keras\\src\\layers\\input_spec.py:202\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmin_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    201\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[1;32m--> 202\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    203\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    204\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    205\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected min_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmin_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    206\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    207\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    208\u001b[0m         )\n\u001b[0;32m    209\u001b[0m \u001b[38;5;66;03m# Check dtype.\u001b[39;00m\n\u001b[0;32m    210\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mValueError\u001b[0m: Input 0 of layer \"conv1d_5\" is incompatible with the layer: expected min_ndim=3, found ndim=2. Full shape received: (None, None)"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from glob import glob  # Needed if you use glob(...) below\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
    "                             mean_absolute_percentage_error, r2_score,\n",
    "                             roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "output_path = os.path.join(config['output'], \"Brazil\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define training flag\n",
    "TRAINING = True  # Set to False to skip training and load an existing model\n",
    "\n",
    "# Utility functions\n",
    "def calculate_nrmse(true_values, predicted_values):\n",
    "    # Replaces the undefined root_mean_squared_error\n",
    "    # with a direct sqrt(mean_squared_error(...))\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    return rmse / (true_values.max() - true_values.min())\n",
    "\n",
    "def calculate_mae(true_values, predicted_values):\n",
    "    return mean_absolute_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_mse(true_values, predicted_values):\n",
    "    return mean_squared_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_rmse(mse):\n",
    "    return mse ** 0.5\n",
    "\n",
    "def calculate_mape(true_values, predicted_values):\n",
    "    return mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_r2(true_values, predicted_values):\n",
    "    return r2_score(true_values, predicted_values)\n",
    "\n",
    "# Discretize continuous values into binary for confusion matrix\n",
    "def discretize_to_binary(values, threshold=0.5):\n",
    "    return np.where(values > threshold, 1, 0)\n",
    "\n",
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(true_values, predicted_values, department_name, metric_type=\"All\"):\n",
    "    cm = confusion_matrix(true_values, predicted_values)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'Confusion Matrix: {department_name} - {metric_type}')\n",
    "    plt.show()\n",
    "\n",
    "if TRAINING:\n",
    "    print(\"Training the model...\")\n",
    "    # Make sure you specify output_path with os.path.join\n",
    "    print(\"Preparing and Training the model...\")\n",
    "    trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train, y_train, x_val, y_val)\n",
    "    # **Dynamically determine input shape from the prepared data:**\n",
    "    input_shape_dynamic = trainingT[0].shape[1:]\n",
    "    print(f\"Dynamically determined input shape from data: {input_shape_dynamic}\")\n",
    "\n",
    "    hyperparameter_settings = {\n",
    "        'DROPOUT_RATE': 0.4,       # Example: Increased dropout\n",
    "        'L2_REGULARIZATION': 1e-4,  # Example: Increased L2 regularization\n",
    "        'NUM_RESIDUAL_BLOCKS': 3,   # Example: Reduced residual blocks\n",
    "        'NUM_FILTERS': 64          # Example: Reduced filters\n",
    "    }\n",
    "    \n",
    "    tcn = ImprovedTCNNet(\n",
    "        shape=input_shape_dynamic,\n",
    "        dropout_rate=hyperparameter_settings.get('DROPOUT_RATE', LSTM_SETTINGS['DROPOUT_RATE']),\n",
    "        l2_regularization=hyperparameter_settings.get('L2_REGULARIZATION', LSTM_SETTINGS['L2_REGULARIZATION']),\n",
    "        num_filters=hyperparameter_settings.get('NUM_FILTERS', LSTM_SETTINGS['NUM_FILTERS']),\n",
    "        num_residual_blocks=hyperparameter_settings.get('NUM_RESIDUAL_BLOCKS', LSTM_SETTINGS['NUM_RESIDUAL_BLOCKS'])\n",
    "    )\n",
    "    history = tcn.train(training=trainingT,\n",
    "                        validation=validationT,\n",
    "                        output_path=os.path.join(config['output'], \"Brazil\"))\n",
    "else:\n",
    "    print(\"Checking for saved models...\")\n",
    "    tcn_models = glob(os.path.join(output_path, \"TCN-new-search-*.keras\"))\n",
    "    if not tcn_models:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "        exit()\n",
    "    else:\n",
    "        # Load the most recent TCN model\n",
    "        tcn = ImprovedTCNNet((None,))  # Dummy shape, will be replaced by loaded model\n",
    "        tcn.load(tcn_models[-1])\n",
    "        \n",
    "        print(f\"Loading model from: {tcn_models[-1]}\")\n",
    "\n",
    "    # Re-prepare the data for inference\n",
    "    (trainingT_X, trainingT_Y), (validationT_X, validationT_Y) = dataset_handler.prepare_data_LSTM(x_train, y_train, x_val, y_val)\n",
    "\n",
    "    trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train,\n",
    "                                                     x_val[:,:,2:], y_val)\n",
    "\n",
    "    # Suppose val_indices, train_indices, scaler, etc. are also defined\n",
    "    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "\n",
    "    # Make predictions\n",
    "    preds_tra = tcn.model.predict(trainT[0])\n",
    "    preds_tra[preds_tra < 0] = 0\n",
    "\n",
    "    preds_val = tcn.model.predict(valT[0])\n",
    "    preds_val[preds_val < 0] = 0\n",
    "\n",
    "    # Inverse transform predictions and ground truth if using scaling\n",
    "    preds_val_original = scaler.inverse_transform(preds_val)\n",
    "    y_val_original = scaler.inverse_transform(valT[1])\n",
    "    preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "    y_train_original = scaler.inverse_transform(trainT[1])\n",
    "\n",
    "    # Collect results by department\n",
    "    results = []\n",
    "\n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "        if department_rows_val.empty or department_rows_train.empty:\n",
    "            continue\n",
    "\n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "        if matching_indices_val.empty or matching_indices_train.empty:\n",
    "            continue\n",
    "\n",
    "        # Split into DengRate_all (col 0) and DengRate_019 (col 1)\n",
    "        true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "        true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "\n",
    "        true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "        true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "\n",
    "        # Calculate metrics for DengRate_all\n",
    "        mse_dengrate_all_val = calculate_mse(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mse_dengrate_all_train = calculate_mse(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "        rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "\n",
    "        mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        # Calculate metrics for DengRate_019\n",
    "        mse_dengrate_019_val = calculate_mse(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mse_dengrate_019_train = calculate_mse(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "        rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "\n",
    "        mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Discretize predictions for confusion matrix (if you want binary classification)\n",
    "        true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "        predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "\n",
    "        true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "        predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "        # Append results for this department\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "            'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "            'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "            'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "\n",
    "            'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "            'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "            'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "            'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "            'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "            'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "            'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "            'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "            'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "\n",
    "            'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "            'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "            'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "            'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "            'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "        })\n",
    "\n",
    "        # Create or update DataFrame for metrics\n",
    "        results_df = pd.DataFrame(results)\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        out_csv = os.path.join(config['metrics'], \"Brazil\",\n",
    "                               f'TCN_new_model_search_{today}.csv')\n",
    "        os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "        results_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Results saved to {out_csv}\")\n",
    "\n",
    "      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c31262c-f2ef-41e5-bf5c-af1b6bbf6272",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from glob import glob  # Needed if you use glob(...) below\n",
    "\n",
    "# Sklearn metrics\n",
    "from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
    "                             mean_absolute_percentage_error, r2_score,\n",
    "                             roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "# Example config usage\n",
    "# config = {\n",
    "#     'output': './some_output_dir',\n",
    "#     'metrics': './some_metrics_dir'\n",
    "# }\n",
    "# DEP_NAMES, scaler, training_dataframe, validation_dataframe, etc. assumed defined\n",
    "\n",
    "# Define output path\n",
    "output_path = os.path.join(config['output'], \"Brazil\")\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# Define training flag\n",
    "TRAINING = True  # Set to False to skip training and load an existing model\n",
    "\n",
    "# Utility functions\n",
    "def calculate_nrmse(true_values, predicted_values):\n",
    "    # Replaces the undefined root_mean_squared_error\n",
    "    # with a direct sqrt(mean_squared_error(...))\n",
    "    rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "    return rmse / (true_values.max() - true_values.min())\n",
    "\n",
    "def calculate_mae(true_values, predicted_values):\n",
    "    return mean_absolute_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_mse(true_values, predicted_values):\n",
    "    return mean_squared_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_rmse(mse):\n",
    "    return mse ** 0.5\n",
    "\n",
    "def calculate_mape(true_values, predicted_values):\n",
    "    return mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "def calculate_r2(true_values, predicted_values):\n",
    "    return r2_score(true_values, predicted_values)\n",
    "\n",
    "# Discretize continuous values into binary for confusion matrix\n",
    "def discretize_to_binary(values, threshold=0.5):\n",
    "    return np.where(values > threshold, 1, 0)\n",
    "\n",
    "# Function to plot a confusion matrix\n",
    "def plot_confusion_matrix(true_values, predicted_values, department_name, metric_type=\"All\"):\n",
    "    cm = confusion_matrix(true_values, predicted_values)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    ax.set_title(f'Confusion Matrix: {department_name} - {metric_type}')\n",
    "    plt.show()\n",
    "\n",
    "# ------------------------------------------------------------------------------\n",
    "# TCN object creation, data prep, etc. assumed done above (for example):\n",
    "#    trainingT, validationT = dataset_handler.prepare_data_LSTM(...)\n",
    "#    tcn = ImprovedTCNNet(trainingT[0].shape[1:])\n",
    "# ------------------------------------------------------------------------------\n",
    "# print(f\"Value of train_model: {train_model}\") # Add this line\n",
    "if TRAINING:\n",
    "    print(\"Training the model...\")\n",
    "    # Make sure you specify output_path with os.path.join\n",
    "    print(\"Preparing and Training the model...\")\n",
    "    trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train, y_train, x_val, y_val)\n",
    "    # **Dynamically determine input shape from the prepared data:**\n",
    "    input_shape_dynamic = trainingT[0].shape[1:]\n",
    "    print(f\"Dynamically determined input shape from data: {input_shape_dynamic}\")\n",
    "\n",
    "    hyperparameter_settings = {\n",
    "        'DROPOUT_RATE': 0.4,       # Example: Increased dropout\n",
    "        'L2_REGULARIZATION': 1e-4,  # Example: Increased L2 regularization\n",
    "        'NUM_RESIDUAL_BLOCKS': 3,   # Example: Reduced residual blocks\n",
    "        'NUM_FILTERS': 64          # Example: Reduced filters\n",
    "    }\n",
    "    \n",
    "    tcn = ImprovedTCNNet(\n",
    "        shape=input_shape_dynamic,\n",
    "        dropout_rate=hyperparameter_settings.get('DROPOUT_RATE', LSTM_SETTINGS['DROPOUT_RATE']),\n",
    "        l2_regularization=hyperparameter_settings.get('L2_REGULARIZATION', LSTM_SETTINGS['L2_REGULARIZATION']),\n",
    "        num_filters=hyperparameter_settings.get('NUM_FILTERS', LSTM_SETTINGS['NUM_FILTERS']),\n",
    "        num_residual_blocks=hyperparameter_settings.get('NUM_RESIDUAL_BLOCKS', LSTM_SETTINGS['NUM_RESIDUAL_BLOCKS'])\n",
    "    )\n",
    "    history = tcn.train(training=trainingT,\n",
    "                        validation=validationT,\n",
    "                        output_path=os.path.join(config['output'], \"Brazil\"))\n",
    "else:\n",
    "    print(\"Checking for saved models...\")\n",
    "    tcn_models = glob(os.path.join(output_path, \"TCN-new-search-*.keras\"))\n",
    "    if not tcn_models:\n",
    "        print('No file with such pattern was found in the directory. Run TRAINING = True first.')\n",
    "        exit()\n",
    "    else:\n",
    "        # Load the most recent TCN model\n",
    "        tcn = ImprovedTCNNet((None,))  # Dummy shape, will be replaced by loaded model\n",
    "        tcn.load(tcn_models[-1])\n",
    "        \n",
    "        print(f\"Loading model from: {tcn_models[-1]}\")\n",
    "\n",
    "    # Re-prepare the data for inference\n",
    "    (trainingT_X, trainingT_Y), (validationT_X, validationT_Y) = dataset_handler.prepare_data_LSTM(x_train, y_train, x_val, y_val)\n",
    "\n",
    "    trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train,\n",
    "                                                     x_val[:,:,2:], y_val)\n",
    "\n",
    "    # Suppose val_indices, train_indices, scaler, etc. are also defined\n",
    "    y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "    y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "\n",
    "    # Make predictions\n",
    "    preds_tra = tcn.model.predict(trainT[0])\n",
    "    preds_tra[preds_tra < 0] = 0\n",
    "\n",
    "    preds_val = tcn.model.predict(valT[0])\n",
    "    preds_val[preds_val < 0] = 0\n",
    "\n",
    "    # Inverse transform predictions and ground truth if using scaling\n",
    "    preds_val_original = scaler.inverse_transform(preds_val)\n",
    "    y_val_original = scaler.inverse_transform(valT[1])\n",
    "    preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "    y_train_original = scaler.inverse_transform(trainT[1])\n",
    "\n",
    "    # Collect results by department\n",
    "    results = []\n",
    "\n",
    "    for department_idx, department_name in DEP_NAMES.items():\n",
    "        department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "        department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "        if department_rows_val.empty or department_rows_train.empty:\n",
    "            continue\n",
    "\n",
    "        department_indices_val = department_rows_val.index.tolist()\n",
    "        department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "        matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "        matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "        if matching_indices_val.empty or matching_indices_train.empty:\n",
    "            continue\n",
    "\n",
    "        # Split into DengRate_all (col 0) and DengRate_019 (col 1)\n",
    "        true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "        true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "        predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "        predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "\n",
    "        true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "        true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "        predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "        predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "\n",
    "        # Calculate metrics for DengRate_all\n",
    "        mse_dengrate_all_val = calculate_mse(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mse_dengrate_all_train = calculate_mse(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "        rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "\n",
    "        mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "        r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "        # Calculate metrics for DengRate_019\n",
    "        mse_dengrate_019_val = calculate_mse(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mse_dengrate_019_train = calculate_mse(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "        rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "\n",
    "        mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "        r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "        # Discretize predictions for confusion matrix (if you want binary classification)\n",
    "        true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "        predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "\n",
    "        true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "        predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "        # Append results for this department\n",
    "        results.append({\n",
    "            'Department': department_name,\n",
    "            'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "            'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "            'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "            'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "            'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "\n",
    "            'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "            'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "            'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "            'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "            'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "\n",
    "            'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "            'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "            'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "            'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "            'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "\n",
    "            'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "            'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "            'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "            'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "            'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "        })\n",
    "\n",
    "        # Create or update DataFrame for metrics\n",
    "        results_df = pd.DataFrame(results)\n",
    "        today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "        out_csv = os.path.join(config['metrics'], \"Brazil\",\n",
    "                               f'TCN_new_model_search_{today}.csv')\n",
    "        os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "        results_df.to_csv(out_csv, index=False)\n",
    "        print(f\"Results saved to {out_csv}\")\n",
    "\n",
    "        # Optional: Plot confusion matrices or ROC curves\n",
    "        # plot_confusion_matrix(true_dengrate_all_val_bin, predicted_dengrate_all_val_bin, department_name, \"Validation\")\n",
    "        # plot_confusion_matrix(true_dengrate_all_train_bin, predicted_dengrate_all_train_bin, department_name, \"Training\")\n",
    "        #\n",
    "        # fpr, tpr, _ = roc_curve(true_dengrate_all_val_bin, predicted_dengrate_all_val_bin)\n",
    "        # roc_auc = auc(fpr, tpr)\n",
    "        # plt.figure(figsize=(8, 6))\n",
    "        # plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "        # plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "        # plt.xlim([0.0, 1.0])\n",
    "        # plt.ylim([0.0, 1.05])\n",
    "        # plt.xlabel('False Positive Rate')\n",
    "        # plt.ylabel('True Positive Rate')\n",
    "        # plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "        # plt.legend(loc='lower right')\n",
    "        # plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafa74ce-2e36-4885-9e16-eab35348389c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"y_val_original shape: {y_val_original.shape}\")\n",
    "print(f\"matching_indices_val: {matching_indices_val}\")\n",
    "print(f\"y_train_original shape: {y_train_original.shape}\")\n",
    "print(f\"matching_indices_train: {matching_indices_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6200392f-6b82-42db-bc9c-e313d198adfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_dengrate_all = mean_squared_error(y_val_original[:, 0], preds_val_original[:, 0])\n",
    "rmse_dengrate_all = np.sqrt(mse_dengrate_all)\n",
    "mae_dengrate_all = mean_absolute_error(y_val_original[:, 0], preds_val_original[:, 0])\n",
    "mape_dengrate_all = mean_absolute_percentage_error(y_val_original[:, 0], preds_val_original[:, 0])\n",
    "r2_dengrate_all = r2_score(y_val_original[:, 0], preds_val_original[:, 0])\n",
    "\n",
    "# For the second output (DengRate_019):\n",
    "mse_dengrate_019 = mean_squared_error(y_val_original[:, 1], preds_val_original[:, 1])\n",
    "rmse_dengrate_019 = np.sqrt(mse_dengrate_019)\n",
    "mae_dengrate_019 = mean_absolute_error(y_val_original[:, 1], preds_val_original[:, 1])\n",
    "mape_dengrate_019 = mean_absolute_percentage_error(y_val_original[:, 1], preds_val_original[:, 1])\n",
    "r2_dengrate_019 = r2_score(y_val_original[:, 1], preds_val_original[:, 1])\n",
    "\n",
    "# Print the results\n",
    "print(f\"MSE DengRate_all: {mse_dengrate_all}, RMSE DengRate_all: {rmse_dengrate_all}, MAE DengRate_all: {mae_dengrate_all}, MAPE DengRate_all: {mape_dengrate_all}, R2 DengRate_all: {r2_dengrate_all}\")\n",
    "print(f\"MSE DengRate_019: {mse_dengrate_019}, RMSE DengRate_019: {rmse_dengrate_019}, MAE DengRate_019: {mae_dengrate_019}, MAPE DengRate_019: {mape_dengrate_019}, R2 DengRate_019: {r2_dengrate_019}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0c57e4-6a8e-4631-89ac-8fc215477321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import glob\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics import (\n",
    "#     mean_absolute_percentage_error, \n",
    "#     mean_squared_error, \n",
    "#     r2_score, \n",
    "#     mean_absolute_error, \n",
    "#     confusion_matrix\n",
    "# )\n",
    "\n",
    "\n",
    "# def calculate_metrics(y_true, y_pred):\n",
    "#     \"\"\"Calculate regression metrics.\"\"\"\n",
    "#     return {\n",
    "#         'MAE': mean_absolute_error(y_true, y_pred),\n",
    "#         'MSE': mean_squared_error(y_true, y_pred),\n",
    "#         'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "#         'MAPE': mean_absolute_percentage_error(y_true, y_pred),\n",
    "#         'R2': r2_score(y_true, y_pred)\n",
    "#     }\n",
    "\n",
    "\n",
    "    \n",
    "# # Define Paths\n",
    "# output_path = os.path.join(config['output'], \"Brazil\")\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# #  Load Latest Model\n",
    "# print(\"🔄 Checking for saved models...\")\n",
    "# tcn_models = sorted(glob.glob(os.path.join(output_path, \"TCN-*.keras\")), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# if tcn_models:\n",
    "#     latest_model = tcn_models[0]  # Get the most recent model\n",
    "#     print(f\"✅ Latest model found: {latest_model}\")\n",
    "    \n",
    "#     # Initialize TCN model with correct input shape before loading weights\n",
    "#     tcn = ImprovedTCNNet(trainingT[0].shape[1:])\n",
    "    \n",
    "#     try:\n",
    "#         tcn.load(latest_model)\n",
    "#         print(f\"✅ Model successfully loaded from: {latest_model}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading model: {e}\")\n",
    "#         exit()\n",
    "# else:\n",
    "#     print(\"❌ No saved model found in path:\", output_path)\n",
    "#     exit()\n",
    "\n",
    "# # Prepare Data\n",
    "# trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "# # Make Predictions\n",
    "# print(\"🔄 Making Predictions...\")\n",
    "# preds_train = np.maximum(tcn.model.predict(trainT[0]), 0)\n",
    "# preds_val = np.maximum(tcn.model.predict(valT[0]), 0)\n",
    "\n",
    "# # Reverse Scaling\n",
    "# preds_train_original = scaler.inverse_transform(preds_train)\n",
    "# preds_val_original = scaler.inverse_transform(preds_val)\n",
    "# y_train_original = scaler.inverse_transform(trainT[1])\n",
    "# y_val_original = scaler.inverse_transform(valT[1])\n",
    "\n",
    "# # Store Results\n",
    "# results = []\n",
    "\n",
    "# for department_idx, department_name in DEP_NAMES.items():\n",
    "#     print(f\"🔍 Processing Department: {department_name}\")\n",
    "\n",
    "#     department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "#     department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "#     if department_rows_val.empty or department_rows_train.empty:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No data found in validation or training sets\")\n",
    "#         continue\n",
    "\n",
    "#     department_indices_val = department_rows_val.index.tolist()\n",
    "#     department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "#     matching_indices_val = np.intersect1d(val_indices, department_indices_val)\n",
    "#     matching_indices_train = np.intersect1d(train_indices, department_indices_train)\n",
    "\n",
    "#     if matching_indices_val.size == 0 or matching_indices_train.size == 0:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No valid indices found after filtering\")\n",
    "#         continue\n",
    "\n",
    "    \n",
    "    \n",
    "#     # Filter out invalid indices for validation set\n",
    "#     valid_indices_val = [idx for idx in matching_indices_val if idx < y_val_original.shape[0]]\n",
    "#     print(f\"Filtered validation indices: {valid_indices_val}\")\n",
    "#     if not valid_indices_val:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No valid indices found for validation set\")\n",
    "#         continue\n",
    "    \n",
    "#     # Extract true and predicted values for validation set\n",
    "#     true_dengrate_all_val = y_val_original[valid_indices_val, 0]\n",
    "#     predicted_dengrate_all_val = preds_val_original[valid_indices_val, 0]\n",
    "    \n",
    "#     # Filter out invalid indices for training set\n",
    "#     valid_indices_train = [idx for idx in matching_indices_train if idx < y_train_original.shape[0]]\n",
    "#     print(f\"Filtered training indices: {valid_indices_train}\")\n",
    "#     if not valid_indices_train:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No valid indices found for training set\")\n",
    "#         continue\n",
    "    \n",
    "#     # Extract true and predicted values for training set\n",
    "#     true_dengrate_all_train = y_train_original[valid_indices_train, 0]\n",
    "#     predicted_dengrate_all_train = preds_train_original[valid_indices_train, 0]\n",
    "    \n",
    "#     # Calculate and store metrics\n",
    "#     metrics_all_val = calculate_metrics(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#     metrics_all_train = calculate_metrics(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "    \n",
    "#     results.append({\n",
    "#         'Department': department_name,\n",
    "#         **{f'{key} (DengRate_all) Val': value for key, value in metrics_all_val.items()},\n",
    "#         **{f'{key} (DengRate_all) Train': value for key, value in metrics_all_train.items()},\n",
    "#     })\n",
    "\n",
    "# # Save Results\n",
    "# results_df = pd.DataFrame(results)\n",
    "# metrics_path = os.path.join(config['metrics'], \"Brazil\", f'TCN_metrics_{datetime.now().strftime('%d-%m-%Y-%H-%M-%S')}.csv')\n",
    "# os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "# results_df.to_csv(metrics_path, index=False)\n",
    "# print(f\"✅ Results saved to {metrics_path}\")\n",
    "\n",
    "# # Apply SHAP Explanation\n",
    "# feature_names = list(training_dataframe.columns[2:])\n",
    "# plot_shap_explanation(tcn.model, trainT[0], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297b4818-0431-4117-adbd-ee58117bb74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import shap\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "# import os\n",
    "# import glob\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics import (\n",
    "#     mean_absolute_percentage_error, \n",
    "#     mean_squared_error, \n",
    "#     r2_score, \n",
    "#     mean_absolute_error, \n",
    "#     confusion_matrix\n",
    "# )\n",
    "\n",
    "\n",
    "# def calculate_metrics(y_true, y_pred):\n",
    "#     \"\"\"Calculate regression metrics.\"\"\"\n",
    "#     return {\n",
    "#         'MAE': mean_absolute_error(y_true, y_pred),\n",
    "#         'MSE': mean_squared_error(y_true, y_pred),\n",
    "#         'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "#         'MAPE': mean_absolute_percentage_error(y_true, y_pred),\n",
    "#         'R2': r2_score(y_true, y_pred)\n",
    "#     }\n",
    "\n",
    "\n",
    "    \n",
    "# # Define Paths\n",
    "# output_path = os.path.join(config['output'], \"Brazil\")\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# #  Load Latest Model\n",
    "# print(\"🔄 Checking for saved models...\")\n",
    "# tcn_models = sorted(glob.glob(os.path.join(output_path, \"TCN-*.keras\")), key=os.path.getmtime, reverse=True)\n",
    "\n",
    "# if tcn_models:\n",
    "#     latest_model = tcn_models[0]  # Get the most recent model\n",
    "#     print(f\"✅ Latest model found: {latest_model}\")\n",
    "    \n",
    "#     # Initialize TCN model with correct input shape before loading weights\n",
    "#     tcn = ImprovedTCNNet(trainingT[0].shape[1:])\n",
    "    \n",
    "#     try:\n",
    "#         tcn.load(latest_model)\n",
    "#         print(f\"✅ Model successfully loaded from: {latest_model}\")\n",
    "#     except Exception as e:\n",
    "#         print(f\"❌ Error loading model: {e}\")\n",
    "#         exit()\n",
    "# else:\n",
    "#     print(\"❌ No saved model found in path:\", output_path)\n",
    "#     exit()\n",
    "\n",
    "# # Prepare Data\n",
    "# trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train, x_val[:,:,2:], y_val)\n",
    "\n",
    "# # Make Predictions\n",
    "# print(\"🔄 Making Predictions...\")\n",
    "# preds_train = np.maximum(tcn.model.predict(trainT[0]), 0)\n",
    "# preds_val = np.maximum(tcn.model.predict(valT[0]), 0)\n",
    "\n",
    "# # Reverse Scaling\n",
    "# preds_train_original = scaler.inverse_transform(preds_train)\n",
    "# preds_val_original = scaler.inverse_transform(preds_val)\n",
    "# y_train_original = scaler.inverse_transform(trainT[1])\n",
    "# y_val_original = scaler.inverse_transform(valT[1])\n",
    "\n",
    "# # Store Results\n",
    "# results = []\n",
    "\n",
    "# for department_idx, department_name in DEP_NAMES.items():\n",
    "#     print(f\"🔍 Processing Department: {department_name}\")\n",
    "\n",
    "#     department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "#     department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "#     if department_rows_val.empty or department_rows_train.empty:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No data found in validation or training sets\")\n",
    "#         continue\n",
    "\n",
    "#     department_indices_val = department_rows_val.index.tolist()\n",
    "#     department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "#     matching_indices_val = np.intersect1d(val_indices, department_indices_val)\n",
    "#     matching_indices_train = np.intersect1d(train_indices, department_indices_train)\n",
    "\n",
    "#     if matching_indices_val.size == 0 or matching_indices_train.size == 0:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No valid indices found after filtering\")\n",
    "#         continue\n",
    "\n",
    "\n",
    "    \n",
    "#     # Extract true and predicted values\n",
    "#     true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "#     predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "\n",
    "#     true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "#     predicted_dengrate_all_train = preds_train_original[matching_indices_train, 0]\n",
    "\n",
    "#     # Calculate and store metrics\n",
    "#     metrics_all_val = calculate_metrics(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#     metrics_all_train = calculate_metrics(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "#     results.append({\n",
    "#         'Department': department_name,\n",
    "#         **{f'{key} (DengRate_all) Val': value for key, value in metrics_all_val.items()},\n",
    "#         **{f'{key} (DengRate_all) Train': value for key, value in metrics_all_train.items()},\n",
    "#     })\n",
    "\n",
    "# # Save Results\n",
    "# results_df = pd.DataFrame(results)\n",
    "# metrics_path = os.path.join(config['metrics'], \"Brazil\", f'TCN_metrics_{datetime.now().strftime('%d-%m-%Y-%H-%M-%S')}.csv')\n",
    "# os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "# results_df.to_csv(metrics_path, index=False)\n",
    "# print(f\"✅ Results saved to {metrics_path}\")\n",
    "\n",
    "# # Apply SHAP Explanation\n",
    "# feature_names = list(training_dataframe.columns[2:])\n",
    "# plot_shap_explanation(tcn.model, trainT[0], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df8e2869-dc26-4d3b-91ff-d3da65a277b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(f\"y_val_original shape: {y_val_original.shape}\")\n",
    "# print(f\"matching_indices_val: {matching_indices_val}\")\n",
    "# print(f\"y_train_original shape: {y_train_original.shape}\")\n",
    "# print(f\"matching_indices_train: {matching_indices_train}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c3b7fb-4686-4997-b883-c73741405557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "\n",
    "# # Get all saved model files\n",
    "# tcn_models = glob.glob(os.path.join(output_path, \"TCN-new-search-*.keras\"))\n",
    "\n",
    "# if not tcn_models:\n",
    "#     print('❌ No saved model found. Run TRAINING = True first.')\n",
    "#     exit()\n",
    "# # else:\n",
    "#     # Sort models by modification time (newest first)\n",
    "#     tcn_models.sort(key=os.path.getmtime, reverse=True)\n",
    "    \n",
    "#     latest_model = tcn_models[0]  # Pick the latest model\n",
    "#     tcn.load(latest_model)\n",
    "#     print(f\"✅ Model loaded from: {latest_model}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb40a89-5b7d-4c44-90fa-cb8c35e8ebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers, Input\n",
    "# from tensorflow.keras.callbacks import EarlyStopping\n",
    "# from tensorflow.keras.optimizers.schedules import ExponentialDecay\n",
    "# import matplotlib.pyplot as plt\n",
    "# import os\n",
    "# import glob\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# from datetime import datetime\n",
    "# from sklearn.metrics import (\n",
    "#     mean_absolute_percentage_error, \n",
    "#     mean_squared_error, \n",
    "#     r2_score, \n",
    "#     mean_absolute_error, \n",
    "#     confusion_matrix\n",
    "# )\n",
    "\n",
    "# # Training Configuration\n",
    "# LSTM_SETTINGS = {\n",
    "#     'EPOCHS': 150,\n",
    "#     'LEARNING RATE': 0.0001,\n",
    "#     'BATCH SIZE': 32,\n",
    "#     'EARLY STOPPING': 20\n",
    "# }\n",
    "\n",
    "# # Define paths\n",
    "# output_path = os.path.join(config['output'], \"Brazil\")\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# # Training Flag\n",
    "# TRAINING = True  # Set to False for evaluation\n",
    "\n",
    "# # Function to build TCN model\n",
    "# def build_tcn_model_v2(input_shape, output_units):\n",
    "#     def residual_block(x, filters, dilation_rate):\n",
    "#         shortcut = layers.Conv1D(filters, kernel_size=1, padding='same')(x)\n",
    "#         x = layers.Conv1D(filters, kernel_size=3, padding='causal', activation='relu', dilation_rate=dilation_rate,\n",
    "#                           kernel_regularizer=regularizers.l2(1e-4))(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Dropout(0.3)(x)\n",
    "#         x = layers.add([x, shortcut])\n",
    "#         x = layers.ReLU()(x)\n",
    "#         return x\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = layers.Conv1D(64, kernel_size=3, padding='causal', activation='relu')(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "\n",
    "#     for dilation_rate in [1, 2, 4, 8]:\n",
    "#         x = residual_block(x, 64, dilation_rate)\n",
    "\n",
    "#     x = layers.GlobalAveragePooling1D()(x)\n",
    "#     outputs = layers.Dense(output_units, activation='linear')(x)\n",
    "\n",
    "#     optimizer = tf.keras.optimizers.Adam(learning_rate=ExponentialDecay(\n",
    "#         LSTM_SETTINGS['LEARNING RATE'], decay_steps=1000, decay_rate=0.96, staircase=True))\n",
    "    \n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     model.compile(optimizer=optimizer, loss=tf.keras.losses.Huber(), metrics=['mae'])\n",
    "#     return model\n",
    "\n",
    "# # Improved TCN class\n",
    "# class ImprovedTCNNet:\n",
    "#     def __init__(self, shape, output_units=2):\n",
    "#         self.shape = shape\n",
    "#         self.model = build_tcn_model_v2(self.shape, output_units)\n",
    "#         self.epochs = LSTM_SETTINGS['EPOCHS']\n",
    "#         self.batch_size = LSTM_SETTINGS['BATCH SIZE']\n",
    "#         self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']\n",
    "    \n",
    "#     def load(self, model_path):\n",
    "#         self.model = tf.keras.models.load_model(model_path)\n",
    "#         print(f\"✅ Model loaded from {model_path}\")\n",
    "\n",
    "#     def train(self, training, validation, output_path):\n",
    "#         \"\"\"Train the TCN model with early stopping.\"\"\"\n",
    "#         es = EarlyStopping(monitor='val_loss', patience=self.early_stopping_rounds, restore_best_weights=True)\n",
    "#         history = self.model.fit(\n",
    "#             x=training[0], y=training[1],\n",
    "#             validation_data=(validation[0], validation[1]),\n",
    "#             epochs=self.epochs, batch_size=self.batch_size,\n",
    "#             callbacks=[es], shuffle=True)\n",
    "\n",
    "#         plt.plot(history.history['loss'], label='Train Loss')\n",
    "#         plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#         plt.legend()\n",
    "#         plt.title(\"Training Loss Curve\")\n",
    "#         plt.show()\n",
    "\n",
    "#         model_filename = f\"TCN-{datetime.now().strftime('%d-%m-%Y-%H-%M-%S')}.keras\"\n",
    "#         save_path = os.path.join(output_path, model_filename)\n",
    "#         self.model.save(save_path)\n",
    "#         print(f\"✅ Model saved at {save_path}\")\n",
    "#         return history\n",
    "\n",
    "# # Prepare data for training\n",
    "# trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train_a[:,:,2:], y_train_a, x_val_a[:,:,2:], y_val_a)\n",
    "\n",
    "# # Initialize Model\n",
    "# tcn = ImprovedTCNNet(trainingT[0].shape[1:])\n",
    "\n",
    "# # **TRAIN THE MODEL BEFORE MAKING PREDICTIONS**\n",
    "# if TRAINING:\n",
    "#     print(\"🔄 Training the model...\")\n",
    "#     tcn.train(trainingT, validationT, output_path)\n",
    "# else:\n",
    "#     # **Load Latest Saved Model for Evaluation**\n",
    "#     print(\"🔄 Checking for saved models...\")\n",
    "#     tcn_models = sorted(glob.glob(os.path.join(output_path, \"TCN-*.keras\")), key=os.path.getmtime, reverse=True)\n",
    "#     if tcn_models:\n",
    "#         latest_model = tcn_models[0]\n",
    "#         tcn.load(latest_model)\n",
    "#         print(f\"✅ Model loaded from: {latest_model}\")\n",
    "#     else:\n",
    "#         print(\"❌ No saved model found.\")\n",
    "#         exit()\n",
    "\n",
    "# # **MAKE PREDICTIONS AFTER TRAINING OR LOADING**\n",
    "# print(\"🔄 Making Predictions...\")\n",
    "# preds_train = np.maximum(tcn.model.predict(trainingT[0]), 0)\n",
    "# preds_val = np.maximum(tcn.model.predict(validationT[0]), 0)\n",
    "\n",
    "# # **Reverse Scaling**\n",
    "# preds_train_original = scaler.inverse_transform(preds_train)\n",
    "# preds_val_original = scaler.inverse_transform(preds_val)\n",
    "# y_train_original = scaler.inverse_transform(trainingT[1])\n",
    "# y_val_original = scaler.inverse_transform(validationT[1])\n",
    "\n",
    "# # **Save Model Predictions**\n",
    "# results = []\n",
    "\n",
    "# for department_idx, department_name in DEP_NAMES.items():\n",
    "#     print(f\"🔍 Processing Department: {department_name}\")\n",
    "\n",
    "#     department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "#     department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "#     if department_rows_val.empty or department_rows_train.empty:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No data found in validation or training sets\")\n",
    "#         continue\n",
    "\n",
    "#     department_indices_val = department_rows_val.index.tolist()\n",
    "#     department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "#     matching_indices_val = np.intersect1d(val_indices, department_indices_val)\n",
    "#     matching_indices_train = np.intersect1d(train_indices, department_indices_train)\n",
    "\n",
    "#     if matching_indices_val.size == 0 or matching_indices_train.size == 0:\n",
    "#         print(f\"⚠️ Skipping {department_name}: No valid indices found after filtering\")\n",
    "#         continue\n",
    "\n",
    "#     # Extract true and predicted values\n",
    "#     true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "#     predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "\n",
    "#     true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "#     predicted_dengrate_all_train = preds_train_original[matching_indices_train, 0]\n",
    "\n",
    "#     # Calculate and store metrics\n",
    "#     metrics_all_val = calculate_metrics(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#     metrics_all_train = calculate_metrics(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "#     results.append({\n",
    "#         'Department': department_name,\n",
    "#         **{f'{key} (DengRate_all) Val': value for key, value in metrics_all_val.items()},\n",
    "#         **{f'{key} (DengRate_all) Train': value for key, value in metrics_all_train.items()},\n",
    "#     })\n",
    "\n",
    "# # Save Results\n",
    "# results_df = pd.DataFrame(results)\n",
    "# metrics_path = os.path.join(config['metrics'], \"Brazil\", f'TCN_metrics_{datetime.now().strftime('%d-%m-%Y-%H-%M-%S')}.csv\")\n",
    "# os.makedirs(os.path.dirname(metrics_path), exist_ok=True)\n",
    "# results_df.to_csv(metrics_path, index=False)\n",
    "# print(f\"✅ Results saved to {metrics_path}\")\n",
    "\n",
    "# # Apply SHAP after training\n",
    "# feature_names = list(training_dataframe.columns[2:])\n",
    "# plot_shap_explanation(tcn.model, trainingT[0], feature_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0bbb64-8b9e-4dd1-a058-816426d01816",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# from datetime import datetime\n",
    "# from glob import glob  # Needed if you use glob(...) below\n",
    "\n",
    "# # Sklearn metrics\n",
    "# from sklearn.metrics import (mean_squared_error, mean_absolute_error,\n",
    "#                             mean_absolute_percentage_error, r2_score,\n",
    "#                             roc_curve, auc, confusion_matrix, ConfusionMatrixDisplay)\n",
    "\n",
    "# # Tensorflow imports (ensure these are present if not already imported)\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import layers, models, regularizers, Input\n",
    "# from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "# import tensorflow.keras.backend as K\n",
    "\n",
    "# # Define output path - assuming config is defined elsewhere as per previous corrections\n",
    "# output_path = os.path.join(config['output'], \"Brazil\")\n",
    "# os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "# # Define training flag\n",
    "# TRAINING = True  # Set to True to train the model\n",
    "\n",
    "# # Utility functions (same as before)\n",
    "# def calculate_nrmse(true_values, predicted_values):\n",
    "#     rmse = np.sqrt(mean_squared_error(true_values, predicted_values))\n",
    "#     return rmse / (true_values.max() - true_values.min())\n",
    "\n",
    "# def calculate_mae(true_values, predicted_values):\n",
    "#     return mean_absolute_error(true_values, predicted_values)\n",
    "\n",
    "# def calculate_mse(true_values, predicted_values):\n",
    "#     return mean_squared_error(true_values, predicted_values)\n",
    "\n",
    "# def calculate_rmse(mse):\n",
    "#     return mse ** 0.5\n",
    "\n",
    "# def calculate_mape(true_values, predicted_values):\n",
    "#     return mean_absolute_percentage_error(true_values, predicted_values)\n",
    "\n",
    "# def calculate_r2(true_values, predicted_values):\n",
    "#     return r2_score(true_values, predicted_values)\n",
    "\n",
    "# def discretize_to_binary(values, threshold=0.5):\n",
    "#     return np.where(values > threshold, 1, 0)\n",
    "\n",
    "# def plot_confusion_matrix(true_values, predicted_values, department_name, metric_type=\"All\"):\n",
    "#     cm = confusion_matrix(true_values, predicted_values)\n",
    "#     fig, ax = plt.subplots(figsize=(6, 6))\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix=cm)\n",
    "#     disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "#     ax.set_title(f'Confusion Matrix: {department_name} - {metric_type}')\n",
    "#     plt.show()\n",
    "\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # Modified TCN Model Definition to accept num_filters\n",
    "# # ------------------------------------------------------------------------------\n",
    "# LSTM_SETTINGS = { # Keep LSTM_SETTINGS for training parameters\n",
    "#     'EPOCHS': 200,\n",
    "#     'LEARNING RATE': 0.0001,\n",
    "#     'BATCH SIZE': 16,\n",
    "#     'OPTIMZER': 'rmsprop', # Not used directly in this code, kept for reference\n",
    "#     'LOSS': 'mae',\n",
    "#     'EVALUATION METRIC': ['mse'],\n",
    "#     'EARLY STOPPING': 12\n",
    "# }\n",
    "\n",
    "# def build_tcn_model_v2(input_shape, output_units, num_filters=64): # ADDED num_filters with default\n",
    "#     \"\"\"Builds the TCN model structure with adjustable number of filters.\"\"\"\n",
    "#     def residual_block(x, filters, dilation_rate): # filters now comes from outer function\n",
    "#         shortcut = x  # Residual connection\n",
    "\n",
    "#         x = layers.Conv1D(\n",
    "#             filters=filters, # Use 'filters' argument\n",
    "#             kernel_size=3,\n",
    "#             padding='causal',\n",
    "#             activation='relu',\n",
    "#             dilation_rate=dilation_rate,\n",
    "#             kernel_regularizer=regularizers.l2(1e-4)\n",
    "#         )(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Dropout(0.3)(x)\n",
    "\n",
    "#         x = layers.Conv1D(\n",
    "#             filters=filters, # Use 'filters' argument\n",
    "#             kernel_size=3,\n",
    "#             padding='causal',\n",
    "#             activation='relu',\n",
    "#             dilation_rate=dilation_rate,\n",
    "#             kernel_regularizer=regularizers.l2(1e-4)\n",
    "#         )(x)\n",
    "#         x = layers.BatchNormalization()(x)\n",
    "#         x = layers.Dropout(0.3)(x)\n",
    "\n",
    "#         x = layers.add([x, shortcut])\n",
    "#         x = layers.ReLU()(x)\n",
    "#         return x\n",
    "\n",
    "#     inputs = Input(shape=input_shape)\n",
    "#     x = layers.Conv1D(\n",
    "#         filters=num_filters, # Use 'num_filters' for initial layer\n",
    "#         kernel_size=3,\n",
    "#         padding='causal',\n",
    "#         activation='relu'\n",
    "#     )(inputs)\n",
    "#     x = layers.BatchNormalization()(x)\n",
    "#     x = layers.Dropout(0.2)(x)\n",
    "\n",
    "#     for dilation_rate in [1, 2, 4, 8]:\n",
    "#         x = residual_block(x, filters=num_filters, dilation_rate=dilation_rate) # Use 'num_filters' for residual blocks\n",
    "\n",
    "#     x = layers.Flatten()(x)\n",
    "#     outputs = layers.Dense(units=output_units, activation='linear')(x)\n",
    "\n",
    "#     model = models.Model(inputs, outputs)\n",
    "#     return model\n",
    "\n",
    "# class ImprovedTCNNet:\n",
    "#     \"\"\"Wrapper class for TCN model, now with num_filters as parameter.\"\"\"\n",
    "#     def __init__(self, shape, output_units=2, num_filters=64): # ADDED num_filters with default\n",
    "#         self.shape = shape\n",
    "#         self.epochs = LSTM_SETTINGS['EPOCHS']\n",
    "#         self.batch_size = LSTM_SETTINGS['BATCH SIZE']\n",
    "#         self.lr = LSTM_SETTINGS['LEARNING RATE']\n",
    "#         self.early_stopping_rounds = LSTM_SETTINGS['EARLY STOPPING']\n",
    "#         self.num_filters = num_filters # Store num_filters\n",
    "\n",
    "#         # Build the uncompiled model, passing num_filters\n",
    "#         self.model = build_tcn_model_v2(self.shape, output_units, num_filters=self.num_filters)\n",
    "\n",
    "#         # Compile the model (same as before)\n",
    "#         self.model.compile(\n",
    "#             optimizer=Adam(learning_rate=self.lr),\n",
    "#             loss='Huber',\n",
    "#             metrics=['mae']\n",
    "#         )\n",
    "\n",
    "#     def load(self, model_path):\n",
    "#         \"\"\"Loads a saved model.\"\"\"\n",
    "#         self.model = tf.keras.models.load_model(model_path)\n",
    "#         print(f\"Model loaded successfully from {model_path}\")\n",
    "\n",
    "#     def train(self, training, validation, output_path):\n",
    "#         \"\"\"Trains the TCN model.\"\"\"\n",
    "#         es = EarlyStopping(monitor='val_loss', patience=self.early_stopping_rounds, restore_best_weights=True)\n",
    "#         lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6, verbose=1)\n",
    "#         print(\">>> Actual LR before fit:\", K.get_value(self.model.optimizer.lr))\n",
    "#         history = self.model.fit(\n",
    "#             x=training[0], y=training[1],\n",
    "#             validation_data=(validation[0], validation[1]),\n",
    "#             epochs=self.epochs, batch_size=self.batch_size,\n",
    "#             callbacks=[es, lr_scheduler], shuffle=True\n",
    "#         )\n",
    "\n",
    "#         plt.figure(figsize=(8, 6))\n",
    "#         plt.plot(history.history['loss'], label='Train Loss')\n",
    "#         plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "#         plt.legend()\n",
    "#         plt.title(f'Improved TCN Model Loss Curve (Filters={self.num_filters})') # Added num_filters to title\n",
    "#         plt.xlabel('Epochs')\n",
    "#         plt.ylabel('Loss')\n",
    "#         plt.show()\n",
    "\n",
    "#         today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "#         model_filename = f\"TCN-new-search-filters{self.num_filters}-{today}.keras\" # Filename includes num_filters\n",
    "#         os.makedirs(output_path, exist_ok=True)\n",
    "#         save_path = os.path.join(output_path, model_filename)\n",
    "#         self.model.save(save_path)\n",
    "#         print(f\"Model saved to {save_path}\")\n",
    "#         return history\n",
    "\n",
    "# # ------------------------------------------------------------------------------\n",
    "# # Main execution block (TRAINING and EVALUATION)\n",
    "# # ------------------------------------------------------------------------------\n",
    "\n",
    "# if TRAINING:\n",
    "#     print(\"Training the model...\")\n",
    "#     # --- Instantiate ImprovedTCNNet here, you can now optionally set num_filters ---\n",
    "#     trainingT, validationT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train,\n",
    "#                                                                  x_val[:,:,2:], y_val) # Assuming these are prepared\n",
    "#     tcn = ImprovedTCNNet(trainingT[0].shape[1:], num_filters=128) # Example: Using 128 filters\n",
    "\n",
    "#     history = tcn.train(training=trainingT,\n",
    "#                           validation=validationT,\n",
    "#                           output_path=os.path.join(config['output'], \"Brazil\"))\n",
    "# else:\n",
    "#     print(\"Checking for saved models...\")\n",
    "#     tcn_models = glob(os.path.join(output_path, \"TCN-new-search-*.keras\")) # Might need to adjust glob pattern to match new filenames\n",
    "#     if not tcn_models:\n",
    "#         print('No file with such pattern was found. Run TRAINING = True first.')\n",
    "#         exit()\n",
    "#     else:\n",
    "#         tcn_models.sort(key=os.path.getmtime)\n",
    "#         tcn.load(tcn_models[-1]) # Load latest model\n",
    "#         print(f\"Loading model from: {tcn_models[-1]}\")\n",
    "\n",
    "#     # --- Data preparation for evaluation (same as in the `else` block previously) ---\n",
    "#     trainT, valT = dataset_handler.prepare_data_LSTM(x_train[:,:,2:], y_train,\n",
    "#                                                         x_val[:,:,2:], y_val)\n",
    "\n",
    "#     y_val_indices_df = pd.DataFrame(val_indices, columns=['actual_index'])\n",
    "#     y_train_indices_df = pd.DataFrame(train_indices, columns=['actual_index'])\n",
    "\n",
    "#     preds_tra = tcn.model.predict(trainT[0])\n",
    "#     preds_tra[preds_tra < 0] = 0\n",
    "#     preds_val = tcn.model.predict(valT[0])\n",
    "#     preds_val[preds_val < 0] = 0\n",
    "\n",
    "#     preds_val_original = scaler.inverse_transform(preds_val)\n",
    "#     y_val_original = scaler.inverse_transform(valT[1])\n",
    "#     preds_tra_original = scaler.inverse_transform(preds_tra)\n",
    "#     y_train_original = scaler.inverse_transform(trainT[1])\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     for department_idx, department_name in DEP_NAMES.items():\n",
    "#         department_rows_val = validation_dataframe[validation_dataframe['dep_id'] == department_idx]\n",
    "#         department_rows_train = training_dataframe[training_dataframe['dep_id'] == department_idx]\n",
    "\n",
    "#         if department_rows_val.empty or department_rows_train.empty:\n",
    "#             continue\n",
    "\n",
    "#         department_indices_val = department_rows_val.index.tolist()\n",
    "#         department_indices_train = department_rows_train.index.tolist()\n",
    "\n",
    "#         matching_indices_val = y_val_indices_df[y_val_indices_df['actual_index'].isin(department_indices_val)].index\n",
    "#         matching_indices_train = y_train_indices_df[y_train_indices_df['actual_index'].isin(department_indices_train)].index\n",
    "\n",
    "#         if matching_indices_val.empty or matching_indices_train.empty:\n",
    "#             continue\n",
    "\n",
    "#         true_dengrate_all_val = y_val_original[matching_indices_val, 0]\n",
    "#         true_dengrate_019_val = y_val_original[matching_indices_val, 1]\n",
    "#         predicted_dengrate_all_val = preds_val_original[matching_indices_val, 0]\n",
    "#         predicted_dengrate_019_val = preds_val_original[matching_indices_val, 1]\n",
    "\n",
    "#         true_dengrate_all_train = y_train_original[matching_indices_train, 0]\n",
    "#         true_dengrate_019_train = y_train_original[matching_indices_train, 1]\n",
    "#         predicted_dengrate_all_train = preds_tra_original[matching_indices_train, 0]\n",
    "#         predicted_dengrate_019_train = preds_tra_original[matching_indices_train, 1]\n",
    "\n",
    "#         mse_dengrate_all_val = calculate_mse(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#         mse_dengrate_all_train = calculate_mse(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "#         rmse_dengrate_all_val = calculate_rmse(mse_dengrate_all_val)\n",
    "#         rmse_dengrate_all_train = calculate_rmse(mse_dengrate_all_train)\n",
    "#         mae_dengrate_all_val = calculate_mae(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#         mae_dengrate_all_train = calculate_mae(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "#         mape_dengrate_all_val = calculate_mape(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#         mape_dengrate_all_train = calculate_mape(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "#         r2_dengrate_all_val = calculate_r2(true_dengrate_all_val, predicted_dengrate_all_val)\n",
    "#         r2_dengrate_all_train = calculate_r2(true_dengrate_all_train, predicted_dengrate_all_train)\n",
    "\n",
    "#         mse_dengrate_019_val = calculate_mse(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "#         mse_dengrate_019_train = calculate_mse(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "#         rmse_dengrate_019_val = calculate_rmse(mse_dengrate_019_val)\n",
    "#         rmse_dengrate_019_train = calculate_rmse(mse_dengrate_019_train)\n",
    "#         mae_dengrate_019_val = calculate_mae(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "#         mae_dengrate_019_train = calculate_mae(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "#         mape_dengrate_019_val = calculate_mape(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "#         mape_dengrate_019_train = calculate_mape(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "#         r2_dengrate_019_val = calculate_r2(true_dengrate_019_val, predicted_dengrate_019_val)\n",
    "#         r2_dengrate_019_train = calculate_r2(true_dengrate_019_train, predicted_dengrate_019_train)\n",
    "\n",
    "#         true_dengrate_all_val_bin = discretize_to_binary(true_dengrate_all_val)\n",
    "#         predicted_dengrate_all_val_bin = discretize_to_binary(predicted_dengrate_all_val)\n",
    "#         true_dengrate_all_train_bin = discretize_to_binary(true_dengrate_all_train)\n",
    "#         predicted_dengrate_all_train_bin = discretize_to_binary(predicted_dengrate_all_train)\n",
    "\n",
    "#         results.append({\n",
    "#             'Department': department_name,\n",
    "#             'MAE (DengRate_all) Val': mae_dengrate_all_val,\n",
    "#             'RMSE (DengRate_all) Val': rmse_dengrate_all_val,\n",
    "#             'MAPE (DengRate_all) Val': mape_dengrate_all_val,\n",
    "#             'R2 (DengRate_all) Val': r2_dengrate_all_val,\n",
    "#             'MSE (DengRate_all) Val': mse_dengrate_all_val,\n",
    "#             'MAE (DengRate_all) Train': mae_dengrate_all_train,\n",
    "#             'RMSE (DengRate_all) Train': rmse_dengrate_all_train,\n",
    "#             'MAPE (DengRate_all) Train': mape_dengrate_all_train,\n",
    "#             'R2 (DengRate_all) Train': r2_dengrate_all_train,\n",
    "#             'MSE (DengRate_all) Train': mse_dengrate_all_train,\n",
    "#             'MAE (DengRate_019) Val': mae_dengrate_019_val,\n",
    "#             'RMSE (DengRate_019) Val': rmse_dengrate_019_val,\n",
    "#             'MAPE (DengRate_019) Val': mape_dengrate_019_val,\n",
    "#             'R2 (DengRate_019) Val': r2_dengrate_019_val,\n",
    "#             'MSE (DengRate_019) Val': mse_dengrate_019_val,\n",
    "#             'MAE (DengRate_019) Train': mae_dengrate_019_train,\n",
    "#             'RMSE (DengRate_019) Train': rmse_dengrate_019_train,\n",
    "#             'MAPE (DengRate_019) Train': mape_dengrate_019_train,\n",
    "#             'R2 (DengRate_019) Train': r2_dengrate_019_train,\n",
    "#             'MSE (DengRate_019) Train': mse_dengrate_019_train,\n",
    "#         })\n",
    "\n",
    "#         results_df = pd.DataFrame(results)\n",
    "#         today = datetime.now().strftime(\"%d-%m-%Y-%H-%M-%S\")\n",
    "#         out_csv = os.path.join(config['metrics'], \"Brazil\",\n",
    "#                                  f'TCN_new_model_search_filters{tcn.num_filters}_{today}.csv') # Filename now includes filter count\n",
    "#         os.makedirs(os.path.dirname(out_csv), exist_ok=True)\n",
    "#         results_df.to_csv(out_csv, index=False)\n",
    "#         print(f\"Results saved to {out_csv}\")\n",
    "\n",
    "#         # Optional plotting - confusion matrices or ROC curves can be added here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
